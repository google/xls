{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#xls-accelerated-hw-synthesis","title":"XLS: Accelerated HW Synthesis","text":""},{"location":"#what-is-xls","title":"What is XLS?","text":"<p>XLS implements a High Level Synthesis toolchain that produces synthesizable designs (Verilog and SystemVerilog) from flexible, high-level descriptions of functionality. It is Apache 2 licensed.</p> <p>XLS (Accelerated HW Synthesis) aims to be the Software Development Kit (SDK) for the End of Moore's Law (EoML) era. In this \"age of specialization\", software and hardware engineers must do more co-design across their domain boundaries -- collaborate on shared artifacts, understand each other's cost models, and share tooling/methodology. XLS attempts to leverage automation, software engineers, and machine cycles to accelerate this overall process.</p> <p>XLS enables the rapid development of hardware IP that also runs as efficient host software via \"software style\" methodology. An XLS design runs at native speeds for use in host software or a simulator, but that design can also generate hardware block output -- the XLS tools' correctness ensures (and provides tools to help formally verify) that they are functionally identical.</p> <p>XLS also supports concurrent processes, in Communicating Sequential Processes (CSP) style, that allow pipelines to communicate with each other and induct over time. This feature is still under active development but today supports base use cases.</p>"},{"location":"#state-of-the-project","title":"State of the Project","text":"<p>XLS is experimental, undergoing rapid development, and not an officially supported Google product. Expect bugs and sharp edges. Please help by trying it out, running through some tutorials, reporting bugs.</p> <p>We are early stage and this has some practical effects:</p> <ul> <li>We welcome your issues and PRs.     * Please try to lead with an issue. Engage us in conversation if you wish to       upstream changes. Sending a PR without back and forth with us in an issue       may be a longer road to success. If you believe your PR is ready and has       not received a response within two business days, please ping the issue       with what you think are next steps.</li> <li>At the current point in its evolution, we regularly improve DSLX without       considering backward compatibility.     * If you are building a corpus of hardware with XLS, please be thoughtful       about your process for bringing in new versions of the compiler.</li> </ul>"},{"location":"#install-using-conda","title":"Install Using Conda","text":"<pre><code>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -p conda-env/ -b\nsource conda-env/bin/activate\nconda install --yes -c litex-hub xls\ninterpreter_main --version\nir_converter_main --version\nopt_main --version\ncodegen_main --version\n</code></pre>"},{"location":"#building-from-source","title":"Building From Source","text":"<p>Currently, XLS must be built from source using the Bazel build system.</p> <p>Note: Binary distributions of the XLS library are not currently available, but we hope to enable them via continuous integration, see this issue.</p> <p>The following instructions are for the Ubuntu 22.04 (Jammy Jellyfish) Linux distribution.</p> <p>We start by assuming Bazel has been installed. On an average 8-core VM, a full initial build (including the C++ frontend) may take up to 6 hours. A build without the C++ frontend may take about 2 hours. Please see the two corresponding command lines below:</p> <pre><code>~$ git clone https://github.com/google/xls.git\n~$ cd xls\n\n~/xls$ # Follow the bazel install instructions:\n~/xls$ # https://bazel.build/install/ubuntu\n~/xls$ # If you do use bazelisk, be aware that documentation here generally uses `bazel`,\n~/xls$ # so you may want to alias bazelisk to bazel.\n~/xls$ # Afterwards we observe:\n~/xls$ bazel --version\nbazel 6.4.0\n\n~/xls$ # Note we're going to tell Ubuntu that `/usr/bin/env python` is actually python3\n~/xls$ # here, since that has not been the case by default on past Ubuntus.\n~/xls$ # This is important. Without this step, you may experience cryptic error messages:\n~/xls$ sudo apt install python3-distutils python3-dev libtinfo5 python-is-python3\n\n~/xls$ # Now build/test in optimized build mode.\n~/xls$ # If you don't plan on using the C++ frontend, which is not needed to get started,\n~/xls$ # use this command line:\n~/xls$ bazel test -c opt -- //xls/... -//xls/contrib/xlscc/...\n\n~/xls$ # To build everything, including the C++ frontend:\n~/xls$ bazel test -c opt -- //xls/...\n</code></pre> <p>Reference build/test environment setups are also provided via <code>Dockerfile</code>s:</p> <pre><code>~$ git clone https://github.com/google/xls.git\n~$ cd xls\n\n~/xls$ # Several Dockerfiles are available to choose from:\n~/xls$ docker build . -f Dockerfile-ubuntu-20.04 # Performs optimized build and test.\n~/xls$ docker build . -f Dockerfile-ubuntu-20.10\n~/xls$ docker build . -f Dockerfile-ubuntu-22.04\n</code></pre>"},{"location":"#adding-additional-build-caching","title":"Adding Additional Build Caching","text":"<p>Many programmers are used to using programs like <code>ccache</code> to improve caching for a build, but Bazel actually ships with very-high quality caching layers. In particular, incremental builds are more safe.</p> <p>However, there are circumstances where Bazel might decide to recompile files where the results could have been cached locally - or where it might be safe to reuse certain intermediate results, even after a <code>bazel clean</code>. To improve this, you can tell Bazel to use a shared \"disk cache\", storing files persistently elsewhere on disk; just create a directory somewhere (e.g., <code>~/.bazel_disk_cache/</code>), and then run:</p> <pre><code>echo \"build --disk_cache=$(realpath ~/.bazel_disk_cache)\" &gt;&gt; ~/.bazelrc\necho \"test --disk_cache=$(realpath ~/.bazel_disk_cache)\" &gt;&gt; ~/.bazelrc\n</code></pre> <p>WARNING: Bazel does not automate garbage collection of this directory, so it will grow over time without bounds. You will need to clean it up periodically, either manually or with an automated script.</p> <p>Alternatively, you can add a remote cache that takes care of garbage collection for you. This can be hosted on a personal server or even on the local machine. We've personally had good results with localhost instances of bazel-remote.</p>"},{"location":"#stack-diagram-and-project-layout","title":"Stack Diagram and Project Layout","text":"<p>Navigating a new code base can be daunting; the following description provides a high-level view of the important directories and their intended organization / purpose, and correspond to the components in this XLS stack diagram:</p> <ul> <li><code>dependency_support</code>:   Configuration files that load, build, and expose Bazel targets for external   dependencies of XLS.</li> <li><code>docs</code>: Generated documentation   served via GitHub pages:   https://google.github.io/xls/</li> <li><code>docs_src</code>: Markdown file   sources, rendered to <code>docs</code> via   mkdocs.</li> <li> <p><code>xls</code>: Project-named   subdirectory within the repository, in common Bazel-project style.</p> <ul> <li><code>build</code>: Build macros   that create XLS artifacts; e.g. convert DSL to IR, create test targets for   DSL code, etc.</li> <li><code>codegen</code>: Verilog   AST (VAST) support to generate Verilog/SystemVerilog operations and FSMs.   VAST is built up by components we call generators (e.g.   PipelineGenerator, SequentialGenerator for FSMs) in the translation from XLS   IR.</li> <li><code>common</code>: \"base\"   functionality that layers on top of standard library usage. Generally we use   Abseil versions of base constructs wherever possible.</li> <li><code>contrib/xlscc</code>:   Experimental C++ syntax support that targets XLS IR (alternative path to   DSLX) developed by a sister team at Google, sharing the same open source /   testing flow as the rest of the XLS project. May be of particular interest   for teams with existing C++ HLS code bases.</li> <li><code>data_structures</code>:   Generic data structures used in XLS that augment standard libraries; e.g.   BDDs, union find, min cut, etc.</li> <li><code>delay_model</code>:   Functionality to characterize, describe, and interpolate data delay for   XLS IR operations on a target backend process. Already-characterized   descriptions are placed in <code>xls/delay_model/models</code> and can be referred to via   command line flags.</li> <li><code>dslx</code>: A DSL (called   \"DSLX\") that mimics Rust, while being an immutable expression-language   dataflow DSL with hardware-oriented features; e.g.  arbitrary bitwidths,   entirely fixed size objects, fully analyzeable call graph. XLS team has found   dataflow DSLs are a good fit to describe hardware as compared to languages   designed assume von Neumann style computation.</li> <li><code>fuzzer</code>: A   whole-stack multiprocess fuzzer that generates programs at the DSL level and   cross-compares different execution engines (DSL interpreter, IR interpreter,   IR JIT, code-generated-Verilog simulator). Designed so that it can easily be   run on different nodes in a cluster simultaneously and accumulate shared   findings.</li> <li><code>examples</code>: Example   computations that are tested and executable through the XLS stack.</li> <li><code>experimental</code>:   Artifacts captured from experimental explorations.</li> <li><code>interpreter</code>:   Interpreter for XLS IR - useful for debugging and exploration. For cases   needing throughput, consider using the JIT (below).</li> <li><code>ir</code>:   XLS IR definition, text parser/formatter, and facilities for abstract   evaluation.</li> <li><code>jit</code>:   LLVM-based JIT for XLS IR. Enables native-speed execution of DSLX and XLS IR   programs.</li> <li><code>modules</code>:   Hardware building block DSLX \"libraries\" (outside the DSLX standard library)   that may be easily reused or instantiated in a broader design.</li> <li><code>netlist</code>: Libraries   that parse/analyze/interpret netlist-level descriptions, as are   generally given in simple structural Verilog with an associated cell library.</li> <li><code>passes</code>: Passes that   run on the XLS IR as part of optimization, before scheduling / code   generation.</li> <li><code>scheduling</code>:   Scheduling algorithms, determine when operations execute (e.g. which   pipeline stage) in a clocked design.</li> <li><code>simulation</code>:   Code that wraps Verilog simulators and generates Verilog testbenches for XLS   computations. iverilog is   currently used to simulate as it supports non-synthesizable testbench   constructs.</li> <li><code>solvers</code>:   Converters from XLS IR into SMT solver input, such that formal proofs can be   run on XLS computations; e.g. Logical Equalence Checks between XLS IR and a   netlist description. Z3 is used as the   solver engine.</li> <li><code>synthesis</code>:   Interface that wraps backend synthesis flows, such that tools can be   retargeted e.g. between ASIC and FPGA flows.</li> <li><code>tests</code>:   Integration tests that span various top-level components of the XLS project.</li> <li><code>tools</code>:   Many tools that work with the XLS   system and its libraries in a decomposed way via command line interfaces.</li> <li><code>uncore_rtl</code>:   Helper RTL that interfaces XLS-generated blocks with device top-level for e.g.   FPGA experiments.</li> <li><code>visualization</code>:   Visualization tools to inspect the XLS compiler/system interactively. See   IR visualization.</li> </ul> </li> </ul>"},{"location":"#community","title":"Community","text":"<p>Discussions about XLS - development, debugging, usage, etc:</p> <ul> <li>Ideally happen in the XLS repo GitHub   discussions</li> <li>But, if you feel email is a better venue for the discussion, there is also an   xls-dev mailing list -- please prefer   GitHub discussions if possible as they are searchable and can be   easily cross-referenced and converted to the issue tracker</li> </ul>"},{"location":"#contributors","title":"Contributors","text":"<p>The following are contributors to the XLS project, see our contributing documentation and good first issues if you're interested in contributing, or reach out via GitHub discussions!</p> <ul> <li>Albert Magyar</li> <li>Alex Light</li> <li>Amin Kalantar</li> <li>Balint Christian</li> <li>Blaok</li> <li>Brandon Jiang</li> <li>Brian Searls</li> <li>Chen-hao Chang</li> <li>Chris Drake</li> <li>Chris Leary</li> <li>Conor McCullough</li> <li>Dan Killebrew</li> <li>Derek Lockhart</li> <li>Eric Astor</li> <li>Ethan Mahintorabi</li> <li>Felix Zhu</li> <li>Georges Rotival</li> <li>Hanchen Ye</li> <li>Hans Montero</li> <li>Henner Zeller</li> <li>Iliyan Malchev</li> <li>Johan Euphrosine</li> <li>Jonathan Bailey</li> <li>Josh Varga</li> <li>Julian Viera</li> <li>Kevin Harlley</li> <li>Leonardo Romor</li> <li>Manav Kohli</li> <li>Mark Heffernan</li> <li>Paul Rigge</li> <li>Per Gr\u00f6n</li> <li>Ravi Nanavati</li> <li>Rebecca Chen (Pytype)</li> <li>Remy Goldschmidt</li> <li>Robert Hundt</li> <li>Rob Springer</li> <li>Sameer Agarwal</li> <li>Sean Purser-Haskell</li> <li>Ted Hong</li> <li>Ted Xie</li> <li>Tim Callahan</li> <li>Vincent Mirian</li> </ul>"},{"location":"adding_ir_operation/","title":"Adding a new IR operation","text":"<p>XLS has about 60 different opcodes and periodically new ones are added to extend functionality or improve the expressiveness of the IR. XLS has many different components and adding a new opcode involves changes to numerous places in the code. These changes, some of which are optional, are described below:</p> <ol> <li> <p>Add operation to     op_specification.py</p> <p>Opcodes and IR node classes are defined in the file <code>op_specification.py</code>.   This Python code generates the C++ header and source files which define   opcodes (<code>op.h</code> and <code>op.cc</code>) and the IR node type hierarchy (<code>nodes.h</code> and   <code>nodes.cc</code>). Every opcode has an associated node subclass derived from the   <code>xls::Node</code> base class. Some opcodes such as <code>Op::kArray</code> have their own   class (<code>Array</code>) because of the unique structure of the operation. Other   opcodes such as the logical operations (<code>Op::kAnd</code>, <code>Op::kOr</code>, etc) share a   common base class (<code>BinOp</code>).</p> <p>The first step to adding a new operations is to add an opcode, and   potentially a new Node class, in <code>op_specification.py</code>. After adding the   opcode numerous files will fail to build because switch statements over the   set of opcodes will no longer be exhaustive. Add the necessary cases to each   switch statement. The exact code in each case will, of course, be   operation-specific. Initially the implementation might return an   <code>absl::UnimplementedError</code> status until later changes add proper support for   the new operation.</p> <p>As part of this change the new operations needs to be added to the DFS   visitor class <code>DfsVisitor</code> by adding a handler method. This class is used   throughout XLS to traverse the IR. This will also adding an implementation   of this new method to many of the subclasses derived from <code>DfsVisitor</code>.</p> <p>(Code example)</p> </li> <li> <p>IR Verifier</p> <p>The IR verifier checks numerous invariants about the IR including   operation-specific properties such as the number and type of operands. Add   an additional handler method for the new operand and add appropriate   operation-specific checks.</p> <p>(Code example)</p> </li> <li> <p>IR Semantics document</p> <p>Describe the semantics and syntax of the new operation in the IR semantics   document.</p> <p>(Code example)</p> </li> <li> <p>Function builder</p> <p>The function builder is the primary API for constructing IR. If appropriate,   add a method to the <code>BuilderBase</code> class which adds an IR node of the new   type to a function.</p> <p>(Code example)</p> </li> <li> <p>IR Parser</p> <p>Add support for parsing of the new operation. The parser tests typically   send a snippet of IR with the operation through the parser and text   serialization and verifies that the output matches the original. Supporting   the new operation may require modifying the <code>xls::Node::ToString</code> method to   emit any special fields required by the operation.</p> <p>(Code example)</p> </li> <li> <p>IR Interpreter</p> <p>The IR interpreter has C++ implementations of all of the operations.   Implement the new operation and add tests.</p> <p>(Code example)</p> </li> <li> <p>IR Matcher</p> <p>The IR matcher is used in tests to enable easy matching of IR expressions.   For example, the following tests that the return value of a function is the   parameter <code>x</code> plus the parameter <code>y</code>:</p> <pre><code>EXPECT_THAT(f-&gt;return_value(), m::Add(m::Param(\"x\", m::Param(\"y\")));\n</code></pre> <p>If the new operation has no named attributed, IR matcher support is   typically a single line using the macro <code>NODE_MATCHER</code>. Otherwise, a custom   matcher should be added to enable matching the attribute as well.</p> <p>(Code example)</p> </li> <li> <p>LLVM JIT</p> <p>The LLVM JIT enables fast simulation of the XLS IR. The JIT constructs LLVM   IR for each XLS operation which is then optimized by LLVM and runs natively   on the host. Implement the new operation in the <code>FunctionBuilderVisitor</code>   class.</p> <p>(Code example)</p> </li> <li> <p>Code generation</p> <p>In XLS \"code generation\" refers to the generation of (System)Verilog from   XLS IR. If the operation can be emitted as a single Verilog expression, then   likely support for the new operation can be added to <code>node_expressions.h</code>,   otherwise if the implementation requires multiple statements then support is   added to <code>module_builder.h</code>.</p> <p>(Code example)</p> </li> <li> <p>Abstract evaluator</p> <p>The abstract evaluator enables evaluation of the XLS IR using different   evaluation systems than Boolean algebra. Users define the semantics of   simple logical operations such as and, or, and not. Then, the abstract   evaluator interprets an IR function using these rules. One example use case   is ternary logic which uses three logic values (true, false, and unknown)   rather than two (true and false) Ternary evaluation is used by the optimizer   to discover statically known bits in the IR graph. The abstract evaluator   can also be used for translation of the IR to other representations. For   example, IR is translated to the Z3 solver representation for formal   verification using the abstract evaluator.</p> <p>If appropriate, the operation should be implemented in   <code>AbstractNodeEvaluator</code> by providing an implementation which decomposes the   operation into fundamental logical operations.</p> <p>(Code example)</p> </li> <li> <p>Z3 solver</p> <p>The Z3 solver is used for theorem proving and logical equivalence checking   between the IR in different stages of compilation and the netlist. To enable   this functionality for the new operation, add a lowering of the operation to   Z3's internal representation.</p> <p>(Code example)</p> </li> <li> <p>Delay model</p> <p>In order to generate efficient circuits which meet timing requirement, XLS   models the delay (in picoseconds) of each operation for different process   technology nodes. This model is constructed by characterizing the process   node using an EDA tool to synthesize the circuit and estimate delay.   Typically, a new operation will need to be characterized by running numerous   permutations of the operation (e.g., with different bit widths) through a   synthesis flow, extracting delay, and building a delay model.</p> <p>(Code example)</p> </li> <li> <p>DSLX frontend</p> <p>Most ops are used by the DSLX frontend in the lowering of DSLX to IR. The   operation may be exposed directly as a builtin (or other operation) or used   in the lowering of other AST nodes. In any case, some changes to the DSLX   frontend will likely be necessary.</p> <p>(Code example)</p> </li> <li> <p>Fuzzer</p> <p>The fuzzer generates random DSLX functions and random inputs to check and   compare different parts of XLS, for example checking that un-optimized and   optimized IR give the same outputs when interpreted. If there is an   operation in DSLX that maps nicely onto the newly added operation, the   fuzzer can be modified to generate functions with DSLX that exercise the   new operation. This is done by adding a handler to <code>AstGenerator</code>. See   here for   more details on how the fuzzer works and how to run it.</p> <p>(Code example)</p> </li> <li> <p>Operation-specific optimizations</p> <p>Typically, a new operation provides optimization opportunities unique to the   node. The details, of course, will be vary for different operations.   However, typically these are at least several easy optimizations which can   be implemented.</p> </li> </ol>"},{"location":"bazel_rules_macros/","title":"Bazel Rules And Macros","text":""},{"location":"bazel_rules_macros/#bazel-rules-and-macros","title":"Bazel Rules And Macros","text":""},{"location":"bazel_rules_macros/#check_sha256sum_frozen","title":"check_sha256sum_frozen","text":"<pre>\ncheck_sha256sum_frozen(name, src, frozen_file, sha256sum)\n</pre> <p>Produces a frozen file if the sha256sum checksum of a source file matches a user-defined checksum.</p> <p>As projects cut releases or freeze, it's important to know that generated (e.g. Verilog) code is never changing without having to actually check in the generated artifact. This rule performs a checksum of a generated file as an integrity check. Users might use this rule to help enable confidence that there is neither:</p> <ul> <li>non-determinism in the toolchain, nor</li> <li>an accidental dependence on a non-released toolchain (e.g. an     accidental dependence on top-of-tree, where the toolchain is     constantly changing)</li> </ul> <p>Say there was a codegen rule producing <code>my_output.v</code>, a user might instantiate something like:</p> <pre><code>check_sha256sum_frozen(\n    name = \"my_output_checksum\",\n    src = \":my_output.v\",\n    sha256sum = \"d1bc8d3ba4afc7e109612cb73acbdddac052c93025aa1f82942edabb7deb82a1\",\n    frozen_file = \"my_output.frozen.x\",\n)\n</code></pre> <p>... and then take a dependency on <code>my_output.frozen.v</code> in the surrounding project, knowing that it had been checksum-verified.</p> <p>Taking a dependence on <code>my_output.v</code> directly may also be ok if the <code>:my_output_checksum</code> target is also built (e.g. via the same wildcard build request), but taking a dependence on the output <code>.frozen.v</code> file ensures that the checking is an integral part of the downstream build-artifact-creation process.</p> <p>At its core, this rule ensure that the contents of a file does not change by verifying that it matches a given checksum. Typically, this rule is used to control the build process. The rule serves as a trigger on rules depending on its output (the frozen file). When the validation of the sha256sum succeed, rules depending on the frozen file are built/executed. When the validation of the sha256sum fails, rules depending on the frozen file are not built/executed.</p> <p>In the example below, when the validation of the sha256sum for target 'generated_file_sha256sum_frozen' succeeds, target 'generated_file_dslx' is built. However, when the validation of the sha256sum for target 'generated_file_sha256sum_frozen' fails, target 'generated_file_dslx' is not built.</p> <p>Examples:</p> <ol> <li>A simple example.<pre><code>check_sha256sum_frozen(\n    name = \"generated_file_sha256sum_frozen\",\n    src = \":generated_file.x\",\n    sha256sum = \"6522799f7b64dbbb2a31eb2862052b8988e78821d8b61fff7f508237a9d9f01d\",\n    frozen_file = \"generated_file.frozen.x\",\n)\n\ndslx_library(\n    name = \"generated_file_dslx\",\n    src = \":generated_file.frozen.x\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required src The source file. Label required frozen_file The frozen output file. Label required sha256sum The sha256sum of the source file. String required <p></p>"},{"location":"bazel_rules_macros/#check_sha256sum_test","title":"check_sha256sum_test","text":"<pre>\ncheck_sha256sum_test(name, src, sha256sum)\n</pre> <p>Validates the sha256sum checksum of a source file with a user-defined checksum.</p> <p>This rule is typically used to ensure that the contents of a file is unchanged.</p> <p>Examples:</p> <ol> <li>A simple example.<pre><code>check_sha256sum_test(\n    name = \"generated_file_sha256sum_test\",\n    src = \":generated_file.x\",\n    sha256sum = \"6522799f7b64dbbb2a31eb2862052b8988e78821d8b61fff7f508237a9d9f01d\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required src The source file. Label required sha256sum The sha256sum of the source file. String required <p></p>"},{"location":"bazel_rules_macros/#proto_data","title":"proto_data","text":"<pre>\nproto_data(name, src, proto_name, protobin_file)\n</pre> <p>Converts a proto text with a xlscc.HLSBlock message to a proto binary.</p> <p>This rules is used in conjunction with the (e.g. xls_cc_ir and xls_cc_verilog) rules and xls_cc_* (e.g. xls_cc_ir_macro and xls_cc_verilog_macro) macros.</p> <p>Examples:</p> <ol> <li>A simple example.<pre><code>proto_data(\n    name = \"packet_selector_block_pb\",\n    src = \"packet_selector.textproto\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required src The source file. Label required proto_name The name of the message type in the .proto files that 'src' file represents. String optional <code>\"xlscc.HLSBlock\"</code> protobin_file The name of the output file to write binary proto to. If not specified, the target name of the bazel rule followed by a .protobin extension is used. Label optional <code>None</code> <p></p>"},{"location":"bazel_rules_macros/#xls_benchmark_verilog","title":"xls_benchmark_verilog","text":"<pre>\nxls_benchmark_verilog(name, verilog_target)\n</pre> <p>Computes and prints various metrics about a Verilog target.</p> <p>Example:     <pre><code>xls_benchmark_verilog(\n    name = \"a_benchmark\",\n    verilog_target = \"a_verilog_target\",\n)\n</code></pre></p> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required verilog_target The verilog target to benchmark. Label optional <code>None</code> <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_library","title":"xls_dslx_library","text":"<pre>\nxls_dslx_library(name, deps, srcs, warnings_as_errors)\n</pre> <p>A build rule that parses and type checks DSLX source files.</p> <p>Examples:</p> <ol> <li> <p>A collection of DSLX source files.</p> <pre><code>xls_dslx_library(\n    name = \"files_123_dslx\",\n    srcs = [\n        \"file_1.x\",\n        \"file_2.x\",\n        \"file_3.x\",\n    ],\n)\n</code></pre> </li> <li> <p>Dependency on other xls_dslx_library targets.</p> <pre><code>xls_dslx_library(\n    name = \"a_dslx\",\n    srcs = [\"a.x\"],\n)\n\n# Depends on target a_dslx.\nxls_dslx_library(\n    name = \"b_dslx\",\n    srcs = [\"b.x\"],\n    deps = [\":a_dslx\"],\n)\n\n# Depends on target a_dslx.\nxls_dslx_library(\n    name = \"c_dslx\",\n    srcs = [\"c.x\"],\n    deps = [\":a_dslx\"],\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required deps Dependency targets for the rule. List of labels optional <code>[]</code> srcs Source files for the rule. Files must have a '.x' extension. List of labels optional <code>[]</code> warnings_as_errors Whether warnings are errors within this library definition. Boolean optional <code>False</code> <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_opt_ir_test","title":"xls_dslx_opt_ir_test","text":"<pre>\nxls_dslx_opt_ir_test(name, benchmark_ir_args, dep, dslx_test_args, input_validator,\n                     input_validator_expr, ir_equivalence_args, ir_eval_args,\n                     scheduling_options_proto, top)\n</pre> <p>A build rule that tests a xls_dslx_opt_ir target.</p> <p>Executes the test commands for the following rules in the order presented:</p> <ol> <li>xls_dslx_test</li> <li>xls_ir_equivalence_test</li> <li>xls_eval_ir_test</li> <li>xls_benchmark_ir</li> </ol> <p>Examples:</p> <ol> <li>A simple example.<pre><code>xls_dslx_opt_ir(\n    name = \"a_opt_ir\",\n    srcs = [\"a.x\"],\n    dslx_top = \"a\",\n)\n\nxls_dslx_opt_ir_test(\n    name = \"a_opt_ir_test\",\n    dep = \":a_opt_ir\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required benchmark_ir_args Arguments of the benchmark IR tool. For details on the arguments, refer to the benchmark_main application at //xls/tools/benchmark_main.cc. Dictionary: String -&gt; String optional <code>{}</code> dep The xls_dslx_opt_ir target to test. Label optional <code>None</code> dslx_test_args Arguments of the DSLX interpreter executable. For details on the arguments, refer to the interpreter_main application at //xls/dslx/interpreter_main.cc. Dictionary: String -&gt; String optional <code>{}</code> input_validator The DSLX library defining the input validator for this test. Mutually exclusive with \"input_validator_expr\". Label optional <code>None</code> input_validator_expr The expression to validate an input for the test function. Mutually exclusive with \"input_validator\". String optional <code>\"\"</code> ir_equivalence_args Arguments of the IR equivalence tool. For details on the arguments, refer to the check_ir_equivalence_main application at //xls/tools/check_ir_equivalence_main.cc. The 'function' argument is not assigned using this attribute. Dictionary: String -&gt; String optional <code>{}</code> ir_eval_args Arguments of the IR interpreter. For details on the arguments, refer to the eval_ir_main application at //xls/tools/eval_ir_main.cc.The 'top' argument is not assigned using this attribute. Dictionary: String -&gt; String optional <code>{\"random_inputs\": \"100\", \"optimize_ir\": \"true\"}</code> scheduling_options_proto Protobuf filename of scheduling arguments to the benchmark IR tool. For details on the arguments, refer to the benchmark_main application at //xls/tools/benchmark_main.cc. Label optional <code>None</code> top The (mangled) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional <code>\"\"</code> <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_test","title":"xls_dslx_test","text":"<pre>\nxls_dslx_test(name, deps, srcs, dslx_test_args, library)\n</pre> <p>A dslx test executes the tests and quick checks of a DSLX source file.</p> <p>Examples:</p> <ol> <li> <p>xls_dslx_test on DSLX source files.</p> <pre><code># Assume a xls_dslx_library target bc_dslx is present.\nxls_dslx_test(\n    name = \"e_dslx_test\",\n    srcs = [\n        \"d.x\",\n        \"e.x\",\n    ],\n    deps = [\":bc_dslx\"],\n)\n</code></pre> </li> <li> <p>xls_dslx_test on a xls_dslx_library.</p> <pre><code>xls_dslx_library(\n    name = \"b_dslx\",\n    srcs = [\"b.x\"],\n    deps = [\":a_dslx\"],\n)\n\nxls_dslx_test(\n    name = \"b_dslx_test\",\n    library = \"b_dslx\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required deps Dependency targets for the files in the 'srcs' attribute. This attribute is mutually exclusive with the 'library' attribute. List of labels optional <code>[]</code> srcs Source files for the rule. The files must have a '.x' extension. This attribute is mutually exclusive with the 'library' attribute. List of labels optional <code>[]</code> dslx_test_args Arguments of the DSLX interpreter executable. For details on the arguments, refer to the interpreter_main application at //xls/dslx/interpreter_main.cc. Dictionary: String -&gt; String optional <code>{}</code> library A DSLX library target where the direct (non-transitive) files of the target are tested. This attribute is mutually exclusive with the 'srcs' and 'deps' attribute. Label optional <code>None</code> <p></p>"},{"location":"bazel_rules_macros/#xls_eval_ir_test","title":"xls_eval_ir_test","text":"<pre>\nxls_eval_ir_test(name, src, input_validator, input_validator_expr, ir_eval_args, top)\n</pre> <p>Executes the IR interpreter on an IR file.</p> <p>Examples:</p> <ol> <li> <p>A file as the source.</p> <pre><code>xls_eval_ir_test(\n    name = \"a_eval_ir_test\",\n    src = \"a.ir\",\n)\n</code></pre> </li> <li> <p>An xls_ir_opt_ir target as the source.</p> <pre><code>xls_ir_opt_ir(\n    name = \"a_opt_ir\",\n    src = \"a.ir\",\n)\n\n\nxls_eval_ir_test(\n    name = \"a_eval_ir_test\",\n    src = \":a_opt_ir\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required src The IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required input_validator The DSLX library defining the input validator for this test. Mutually exclusive with \"input_validator_expr\". Label optional <code>None</code> input_validator_expr The expression to validate an input for the test function. Mutually exclusive with \"input_validator\". String optional <code>\"\"</code> ir_eval_args Arguments of the IR interpreter. For details on the arguments, refer to the eval_ir_main application at //xls/tools/eval_ir_main.cc.The 'top' argument is not assigned using this attribute. Dictionary: String -&gt; String optional <code>{\"random_inputs\": \"100\", \"optimize_ir\": \"true\"}</code> top The (mangled) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional <code>\"\"</code> <p></p>"},{"location":"bazel_rules_macros/#xls_ir_equivalence_test","title":"xls_ir_equivalence_test","text":"<pre>\nxls_ir_equivalence_test(name, ir_equivalence_args, src_0, src_1, top)\n</pre> <p>Executes the equivalence tool on two IR files.</p> <p>Examples:</p> <ol> <li> <p>A file as the source.</p> <pre><code>xls_ir_equivalence_test(\n    name = \"ab_ir_equivalence_test\",\n    src_0 = \"a.ir\",\n    src_1 = \"b.ir\",\n)\n</code></pre> </li> <li> <p>A target as the source.</p> <pre><code>xls_dslx_ir(\n    name = \"b_ir\",\n    srcs = [\"b.x\"],\n)\n\nxls_ir_equivalence_test(\n    name = \"ab_ir_equivalence_test\",\n    src_0 = \"a.ir\",\n    src_1 = \":b_ir\",\n)\n</code></pre> </li> </ol> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required ir_equivalence_args Arguments of the IR equivalence tool. For details on the arguments, refer to the check_ir_equivalence_main application at //xls/tools/check_ir_equivalence_main.cc. The 'function' argument is not assigned using this attribute. Dictionary: String -&gt; String optional <code>{}</code> src_0 An IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required src_1 An IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required top The (mangled) name of the entry point. See get_mangled_ir_symbol. Defines the 'top' argument of the IR tool/application. String optional <code>\"\"</code> <p></p>"},{"location":"bazel_rules_macros/#xls_ir_verilog_fdo","title":"xls_ir_verilog_fdo","text":"<pre>\nxls_ir_verilog_fdo(name, src, outs, block_ir_file, codegen_args, codegen_options_proto,\n                   module_sig_file, schedule_file, schedule_ir_file, scheduling_options_proto,\n                   sta_tool, standard_cells, verilog_file, verilog_line_map_file, yosys_tool)\n</pre> <p>A build rule that generates a Verilog file from an IR file using FDO (feedback-directed optimization).  Codegen args to activate FDO and provide required dependencies are automatically provided.  Default values for FDO parameters are provided but can be overridden in \"codegen_args {...}\".</p> <p>In FDO mode, the codegen_arg \"clock_period_ps\" MUST be provided.</p> <p>Example:</p> <pre><code>```\nxls_ir_verilog_fdo(\n    name = \"a_verilog\",\n    src = \"a.ir\",\n    codegen_args = {\n        \"clock_period_ps\": \"750\",\n        \"fdo_iteration_number\": \"5\",\n        ...\n    },\n)\n```\n</code></pre> <p>ATTRIBUTES</p> Name Description Type Mandatory Default name A unique name for this target. Name required src The IR source file for the rule. A single source file must be provided. The file must have a '.ir' extension. Label required outs The list of generated files. List of strings optional <code>[]</code> block_ir_file The filename of block-level IR file generated during codegen. If not specified, the basename of the Verilog file followed by a .block.ir extension is used. Label optional <code>None</code> codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. Dictionary: String -&gt; String optional <code>{}</code> codegen_options_proto Filename of a protobuf with arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. Label optional <code>None</code> module_sig_file The filename of module signature of the generated Verilog file. If not specified, the basename of the Verilog file followed by a .sig.textproto extension is used. Label optional <code>None</code> schedule_file The filename of schedule of the generated Verilog file.If not specified, the basename of the Verilog file followed by a .schedule.textproto extension is used. Label optional <code>None</code> schedule_ir_file The filename of scheduled IR file generated during scheduled. If not specified, the basename of the Verilog file followed by a .schedule.opt.ir extension is used. Label optional <code>None</code> scheduling_options_proto Filename of a protobuf with scheduling options arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. Label optional <code>None</code> sta_tool - Label optional <code>\"@org_theopenroadproject//:opensta\"</code> standard_cells - Label optional <code>\"@com_google_skywater_pdk_sky130_fd_sc_hd//:sky130_fd_sc_hd\"</code> verilog_file The filename of Verilog file generated. The filename must have a v extension. Label required verilog_line_map_file The filename of line map for the generated Verilog file.If not specified, the basename of the Verilog file followed by a .verilog_line_map.textproto extension is used. Label optional <code>None</code> yosys_tool - Label optional <code>\"//third_party/yosys\"</code> <p></p>"},{"location":"bazel_rules_macros/#cc_xls_ir_jit_wrapper","title":"cc_xls_ir_jit_wrapper","text":"<pre>\ncc_xls_ir_jit_wrapper(name, src, jit_wrapper_args, kwargs)\n</pre> <p>Invokes the JIT wrapper generator and compiles the result as a cc_library.</p> <p>The macro invokes the JIT wrapper generator on an IR source file. The generated source files are the inputs to a cc_library with its target name identical to this macro.</p> <p>PARAMETERS</p> Name Description Default Value name The name of the cc_library target. none src The path to the IR file. none jit_wrapper_args Arguments of the JIT wrapper tool. Note: argument 'output_name' cannot be defined. <code>{}</code> kwargs Keyword arguments. Named arguments. none <p></p>"},{"location":"bazel_rules_macros/#get_mangled_ir_symbol","title":"get_mangled_ir_symbol","text":"<pre>\nget_mangled_ir_symbol(module_name, function_name, parametric_values, is_implicit_token,\n                      is_proc_next)\n</pre> <p>Returns the mangled IR symbol for the module/function combination.</p> <p>\"Mangling\" is the process of turning nicely namedspaced symbols into \"grosser\" (mangled) flat (non hierarchical) symbol, e.g. that lives on a package after IR conversion. To retrieve/execute functions that have been IR converted, we use their mangled names to refer to them in the IR namespace.</p> <p>PARAMETERS</p> Name Description Default Value module_name The DSLX module name that the function is within. none function_name The DSLX function name within the module. none parametric_values Any parametric values used for instantiation (e.g. for a parametric entry point that is known to be instantiated in the IR converted module). This is generally for more advanced use cases like internals testing. The argument is mutually exclusive with argument 'is_proc_next'. <code>None</code> is_implicit_token A boolean flag denoting whether the symbol contains an implicit token. The argument is mutually exclusive with argument 'is_proc_next'. <code>False</code> is_proc_next A boolean flag denoting whether the symbol is a next proc function. The argument is mutually exclusive with arguments: 'parametric_values' and 'is_implicit_token'. <code>False</code> <p>RETURNS</p> <p>The \"mangled\" symbol string.</p> <p></p>"},{"location":"bazel_rules_macros/#xls_benchmark_ir","title":"xls_benchmark_ir","text":"<pre>\nxls_benchmark_ir(name, src, synthesize, codegen_args, benchmark_ir_args, standard_cells, tags,\n                 ir_tags, synth_tags, kwargs)\n</pre> <p>Executes the benchmark tool on an IR file.</p> <p>Examples:</p> <ol> <li> <p>A file as the source.</p> <pre><code>xls_benchmark_ir(\n    name = \"a_benchmark\",\n    src = \"a.ir\",\n)\n</code></pre> </li> <li> <p>An xls_ir_opt_ir target as the source.</p> <pre><code>xls_ir_opt_ir(\n    name = \"a_opt_ir\",\n    src = \"a.ir\",\n)\n\n\nxls_benchmark_ir(\n    name = \"a_benchmark\",\n    src = \":a_opt_ir\",\n)\n</code></pre> <p>Args:       name: A unique name for this target.       src: The IR source file for the rule. A single source file must be provided. The file must         have a '.ir' extension.       synthesize: Add a synthesis benchmark in addition to the IR benchmark.       codegen_args: Arguments of the codegen tool. For details on the arguments,         refer to the codegen_main application at         //xls/tools/codegen_main.cc.       benchmark_ir_args: Arguments of the benchmark IR tool. For details on the arguments, refer         to the benchmark_main application at //xls/tools/benchmark_main.cc.       standard_cells: Label for the PDK (possibly specifying a         non-default corner), with the assumption that $location will         return the timing (Liberty) library for the PDK corner. Unused if synthesize == False.       tags: Tags for IR and synthesis benchmark targets.       ir_tags: Tags for the IR benchmark target only.       synth_tags: Tags for the synthesis and synthesis benchmark targets. Unused if synthesize == False.       **kwargs: Keyword arguments for the IR benchmark target only.</p> </li> </ol> <p>PARAMETERS</p> Name Description Default Value name <p> - </p> none src <p> - </p> none synthesize <p> - </p> <code>True</code> codegen_args <p> - </p> <code>{}</code> benchmark_ir_args <p> - </p> <code>{}</code> standard_cells <p> - </p> <code>None</code> tags <p> - </p> <code>None</code> ir_tags <p> - </p> <code>None</code> synth_tags <p> - </p> <code>None</code> kwargs <p> - </p> none <p></p>"},{"location":"bazel_rules_macros/#xls_delay_model_generation","title":"xls_delay_model_generation","text":"<pre>\nxls_delay_model_generation(name, standard_cells, samples_file, kwargs)\n</pre> <p>Generate an XLS delay model for one PDK corner.</p> <p>This macro gathers the locations of the required dependencies (Yosys, OpenSTA, helper scripts, and cell libraries) and generates a wrapper script that invokes \"run_timing_characterization\" with the dependency locations provided as args.</p> <p>Any extra runtime args will get passed in to the \"run_timing_characterization\" script (e.g. \"--debug\" or \"--quick_run\").</p> <p>The script must be \"run\" from the root of the workspace to perform the timing characterization.  The output textproto will be produced in the current directory (which, as just stated, must be the root of the workspace).</p> <p>Currently, only a subset of XLS operators are characterized, including most arithmetic, logical, and shift operators. However, many common operators such as \"concat\", \"bit_slice\", and \"encode\" are missing, and so the delay model that is currently produced should be considered INCOMPLETE.</p> <p>PARAMETERS</p> Name Description Default Value name Used as basename for both the script and the output textproto. none standard_cells Label for the PDK (possibly specifying a non-default corner), with the assumption that $location will return the timing (Liberty) library for the PDK corner. none samples_file Path to proto providing sample points. none kwargs Accepts add'l keyword arguments. Passed to native.genrule(). none <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_cpp_type_library","title":"xls_dslx_cpp_type_library","text":"<pre>\nxls_dslx_cpp_type_library(name, src, namespace)\n</pre> <p>Creates a cc_library target for transpiled DSLX types.</p> <p>This macros invokes the DSLX-to-C++ transpiler and compiles the result as a cc_library with its target name identical to this macro.</p> <p>PARAMETERS</p> Name Description Default Value name The name of the eventual cc_library. none src The DSLX file whose types to compile as C++. none namespace The C++ namespace to generate the code in (e.g., <code>foo::bar</code>). <code>None</code> <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_fmt_test","title":"xls_dslx_fmt_test","text":"<pre>\nxls_dslx_fmt_test(name, src, opportunistic_postcondition)\n</pre> <p>Creates a test target that confirms <code>src</code> is auto-formatted.</p> <p>PARAMETERS</p> Name Description Default Value name Name of the (diff) test target this will emit. none src Source file to auto-format. none opportunistic_postcondition Flag that checks whether the output text is highly similar to the input text. Note that sometimes this /can/ flag an error for some set of valid auto-formattings, so is intended primarily for use as a development/debugging tool. <code>False</code> <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_ir","title":"xls_dslx_ir","text":"<pre>\nxls_dslx_ir(name, dslx_top, srcs, deps, library, ir_conv_args, enable_generated_file,\n            enable_presubmit_generated_file, kwargs)\n</pre> <p>A macro that instantiates a build rule converting a DSLX source file to an IR file.</p> <p>The macro instantiates a rule that converts a DSLX source file to an IR file. The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule.</p> <p>Example:</p> <p>An IR conversion with a top entity defined.</p> <pre><code>```\n# Assume a xls_dslx_library target bc_dslx is present.\nxls_dslx_ir(\n    name = \"d_ir\",\n    srcs = [\"d.x\"],\n    deps = [\":bc_dslx\"],\n    dslx_top = \"d\",\n)\n```\n</code></pre> <p>PARAMETERS</p> Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. <code>None</code> deps Dependency targets for the files in the 'srcs' argument. <code>None</code> library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. <code>None</code> ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_convert/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. <code>{}</code> enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. <code>True</code> enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. <code>False</code> kwargs Keyword arguments. Named arguments. none <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_opt_ir","title":"xls_dslx_opt_ir","text":"<pre>\nxls_dslx_opt_ir(name, dslx_top, srcs, deps, library, ir_conv_args, opt_ir_args,\n                enable_generated_file, enable_presubmit_generated_file, kwargs)\n</pre> <p>A macro that instantiates a build rule generating an optimized IR file from a DSLX source file.</p> <p>The macro instantiates a build rule that generates an optimized IR file from a DSLX source file. The build rule executes the core functionality of following macros:</p> <ol> <li>xls_dslx_ir (converts a DSLX file to an IR), and,</li> <li>xls_ir_opt_ir (optimizes the IR).</li> </ol> <p>The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule.</p> <p>Examples:</p> <ol> <li>A simple example.<pre><code># Assume a xls_dslx_library target bc_dslx is present.\nxls_dslx_opt_ir(\n    name = \"d_opt_ir\",\n    srcs = [\"d.x\"],\n    deps = [\":bc_dslx\"],\n    dslx_top = \"d\",\n)\n</code></pre> </li> </ol> <p>PARAMETERS</p> Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. <code>None</code> deps Dependency targets for the files in the 'srcs' argument. <code>None</code> library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. <code>None</code> ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_convert/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. <code>{}</code> opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. <code>{}</code> enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. <code>True</code> enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. <code>False</code> kwargs Keyword arguments. Named arguments. none <p></p>"},{"location":"bazel_rules_macros/#xls_dslx_verilog","title":"xls_dslx_verilog","text":"<pre>\nxls_dslx_verilog(name, dslx_top, verilog_file, srcs, deps, library, ir_conv_args, opt_ir_args,\n                 codegen_args, enable_generated_file, enable_presubmit_generated_file, kwargs)\n</pre> <p>A macro that instantiates a build rule generating a Verilog file from a DSLX source file and tests the build.</p> <p>The macro instantiates a build rule that generates a Verilog file from a DSLX source file. The build rule executes the core functionality of following macros:</p> <ol> <li>xls_dslx_ir (converts a DSLX file to an IR),</li> <li>xls_ir_opt_ir (optimizes the IR), and,</li> <li>xls_ir_verilog (generated a Verilog file).</li> </ol> <p>The macro also instantiates a 'build_test' testing that the build rule generating a Verilog file. If the build is not successful, an error is produced when executing a test command on the target.</p> <p>Examples:</p> <ol> <li>A simple example.<pre><code># Assume a xls_dslx_library target bc_dslx is present.\nxls_dslx_verilog(\n    name = \"d_verilog\",\n    srcs = [\"d.x\"],\n    deps = [\":bc_dslx\"],\n    codegen_args = {\n        \"pipeline_stages\": \"1\",\n    },\n    dslx_top = \"d\",\n)\n</code></pre> </li> </ol> <p>PARAMETERS</p> Name Description Default Value name The name of the rule. none dslx_top The top entity to perform the IR conversion. none verilog_file The filename of Verilog file generated. The filename must have a '.v' extension. none srcs Top level source files for the conversion. Files must have a '.x' extension. There must be single source file. <code>None</code> deps Dependency targets for the files in the 'srcs' argument. <code>None</code> library A DSLX library target where the direct (non-transitive) files of the target are tested. This argument is mutually exclusive with the 'srcs' and 'deps' arguments. <code>None</code> ir_conv_args Arguments of the IR conversion tool. For details on the arguments, refer to the ir_converter_main application at //xls/dslx/ir_convert/ir_converter_main.cc. Note: the 'top' argument is not assigned using this attribute. <code>{}</code> opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. <code>{}</code> codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. <code>{}</code> enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. <code>True</code> enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. <code>False</code> kwargs Keyword arguments. Named arguments. none <p></p>"},{"location":"bazel_rules_macros/#xls_ir_cc_library","title":"xls_ir_cc_library","text":"<pre>\nxls_ir_cc_library(name, src, top, namespaces)\n</pre> <p>Invokes the AOT compiles the input IR into a cc_library.</p> <p>Example:</p> <pre><code>xls_ir_opt_ir(\n    name \"foo\",\n    ...\n)\n\nxls_ir_cc_library_macro(\n    name = \"foo_cc\",\n    src = \":foo.opt.ir\",\n    top = \"bar\",\n    namespaces = \"a,b,c\",\n)\n</code></pre> <p>This will produce a cc_library that will execute the fn <code>bar</code> from the <code>foo</code> IR file. The call itself will be inside the namespace <code>a::b::c</code>.</p> <p>PARAMETERS</p> Name Description Default Value name The name of the resulting library. none src The path to the IR file to compile. none top The entry point in the IR file of interest. <code>None</code> namespaces A comma-separated list of namespaces into which the generated code should go. <code>\"\"</code> <p></p>"},{"location":"bazel_rules_macros/#xls_ir_opt_ir","title":"xls_ir_opt_ir","text":"<pre>\nxls_ir_opt_ir(name, src, opt_ir_args, enable_generated_file, enable_presubmit_generated_file,\n              debug_srcs, kwargs)\n</pre> <p>A macro that instantiates a build rule optimizing an IR file.</p> <p>The macro instantiates a build rule that optimizes an IR file. The macro also instantiates the 'enable_generated_file_wrapper' function. The generated files are listed in the outs attribute of the rule.</p> <p>Examples:</p> <ol> <li> <p>A simple example.</p> <pre><code>xls_ir_opt_ir(\n    name = \"a_opt_ir\",\n    src = \"a.ir\",\n)\n</code></pre> </li> <li> <p>Optimizing an IR file with a top entity defined.</p> <pre><code>xls_ir_opt_ir(\n    name = \"a_opt_ir\",\n    src = \"a.ir\",\n    opt_ir_args = {\n        \"inline_procs\" : \"true\",\n    },\n)\n</code></pre> </li> </ol> <p>PARAMETERS</p> Name Description Default Value name The name of the rule. none src The IR source file. A single source file must be provided. The file must have a '.ir' extension. none opt_ir_args Arguments of the IR optimizer tool. For details on the arguments, refer to the opt_main application at //xls/tools/opt_main.cc. Note: the 'top' argument is not assigned using this attribute. <code>{}</code> enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. <code>True</code> enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. <code>False</code> debug_srcs List of additional source files for debugging info. Allows opt_main to correctly display lines from original source file (e.g. the .cc file before the xlscc pass) when an error occurs. <code>[]</code> kwargs Keyword arguments. Named arguments. none <p></p>"},{"location":"bazel_rules_macros/#xls_ir_verilog","title":"xls_ir_verilog","text":"<pre>\nxls_ir_verilog(name, src, verilog_file, codegen_args, codegen_options_proto,\n               scheduling_options_proto, enable_generated_file, enable_presubmit_generated_file,\n               kwargs)\n</pre> <p>A macro that instantiates a build rule generating a Verilog file from an IR file and tests the build.</p> <p>The macro instantiates a build rule that generate a Verilog file from an IR file, and a 'build_test' testing that the build rule generating a Verilog file. If the build is not successful, an error is produced when executing a test command on the target.</p> <p>Example:</p> <pre><code>```\nxls_ir_verilog(\n    name = \"a_verilog\",\n    src = \"a.ir\",\n    codegen_args = {\n        \"pipeline_stages\": \"1\",\n        ...\n    },\n)\n```\n</code></pre> <p>PARAMETERS</p> Name Description Default Value name The name of the rule. none src The IR source file. A single source file must be provided. The file must have a '.ir' extension. none verilog_file The filename of Verilog file generated. The filename must have a '.v' or '.sv', extension. none codegen_args Arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. <code>{}</code> codegen_options_proto Filename of a protobuf with arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. <code>None</code> scheduling_options_proto Filename of a protobuf with scheduling options arguments of the codegen tool. For details on the arguments, refer to the codegen_main application at //xls/tools/codegen_main.cc. <code>None</code> enable_generated_file See 'enable_generated_file' from 'enable_generated_file_wrapper' function. <code>True</code> enable_presubmit_generated_file See 'enable_presubmit_generated_file' from 'enable_generated_file_wrapper' function. <code>False</code> kwargs Keyword arguments. Named arguments. none <p></p>"},{"location":"bazel_rules_macros/#xls_synthesis_metrics","title":"xls_synthesis_metrics","text":"<pre>\nxls_synthesis_metrics(name, srcs, kwargs)\n</pre> <p>Gather per-pipeline-stage metrics from log files.</p> <p>Gather per-stage post-synth metrics from the provided logs (from Yosys or OpenSTA) and save them in a \"DesignStats\" textproto. Recognized metrics from Yosys log:   Total cell area (um^2).   Logic levels   Cell count   Flop count Recognized metrics from OpenSTA log:   Critical path delay (ps)   Critical path start point   Critical path end point</p> <p>PARAMETERS</p> Name Description Default Value name Output \"DesignStats\" textproto will be <code>&lt;name&gt;.textproto</code> none srcs Targets from which log files will be scanned. For post-synth, use \"synthesize_rtl\" and \"run_opensta\" targets. none kwargs Accepts add'l keyword arguments. Passed to native.genrule(). none"},{"location":"build_system/","title":"Build system","text":"<p>XLS uses the Bazel build system for itself and all its dependencies. Bazel is an easy to configure and use, and has powerful extension facilities. (It's also well-documented!) XLS provides a number of Starlark rules and macros to define a build flow.</p> <ul> <li>Build system<ul> <li>Whirlwind Intro To Bazel<ul> <li>Where the output files go</li> </ul> </li> <li>XLS Project Build Rules</li> <li>Bazel queries<ul> <li>Finding transitive dependencies</li> <li>Finding dependees (\"reverse dependencies\")</li> </ul> </li> </ul> </li> </ul>"},{"location":"build_system/#whirlwind-intro-to-bazel","title":"Whirlwind Intro To Bazel","text":"<p>Many developers are familiar with a make-style build flow. Bazel, by contrast, provides more built-in structure for where generated files and binary artifacts are placed, in order to keep the source tree unmodified and the build process fully declarative / repeatable. In Bazel, one of the key principles is \"the user should not need to <code>bazel clean</code>\".</p> <p>A typical build command looks like:</p> <pre><code>$ bazel build -c opt //xls/tools:opt_main\n</code></pre> <p>The <code>-c opt</code> flag is requesting we produce an optimized build. Other options for development are:</p> <ul> <li><code>-c fastbuild</code>: fewer optimizations, quicker turn around time on builds, and</li> <li><code>-c dbg</code>: debug binaries, minimal optimization level and debug information     produced, e.g. for using binaries under <code>gdb</code></li> </ul> <p>Targets are referenced with <code>//</code> as the root of the current repository -- it is generally optional. From there you specify the path to a directory with a <code>BUILD</code> file, and then <code>:target_name</code> to reference a named target within that <code>BUILD</code> file. In the case above, the build target referenced is a C++ binary -- its build definition is described by a <code>cc_binary</code> rule in the <code>xls/tools/BUILD</code> file.</p>"},{"location":"build_system/#where-the-output-files-go","title":"Where the output files go","text":"<p>The above command notes the following in its output:</p> <pre><code>Target //xls/tools:opt_main up-to-date:\n  bazel-bin/xls/tools/opt_main\n</code></pre> <p>We can see binary result files go to <code>bazel-bin</code> within our repository's root directory. (Aside: <code>bazel-bin</code> is a convenient symlink to an out-of-tree location where build artifacts are placed.)</p> <p>Generated files that are intermediate entities in the build process are also visible via a similar symlink, <code>bazel-out</code>. Within the following directory:</p> <pre><code>$ ls bazel-out/host/bin/xls/ir/\n</code></pre> <p>We can see files that were part of the build of the IR library, like <code>op.h</code> and <code>op.cc</code>.</p>"},{"location":"build_system/#xls-project-build-rules","title":"XLS Project Build Rules","text":"<p>XLS provides a set of Bazel build rules and macros that allow users to quickly/easily create XLS-based design artifacts -- analogous to the way C++, Python, etc are done in Bazel. For example, <code>dslx_library</code> lets a user make a library target written in XLS' Domain Specific Language frontend.</p> <p>XLS build rules and macros are defined in xls/build_rules/xls_build_defs.bzl.</p> <p>Examples using the rules and macros are found at xls/build_rules/tests/BUILD.</p> <p>A detailed description of the bazel rules/macros can be found here.</p>"},{"location":"build_system/#bazel-queries","title":"Bazel queries","text":"<p>Understanding the build tree for a new project can be difficult, but fortunately Bazel provides a powerful query mechanism. <code>bazel query</code> enables a user to examine build targets, dependencies between them, and much more. A few usage examples are provided here, but the full documentation (linked above) is comprehensive.</p>"},{"location":"build_system/#finding-transitive-dependencies","title":"Finding transitive dependencies","text":"<p>To understand why, for example, the combinational verilog generator depends on the ABSL container algorithm library, one could run:</p> <pre><code>$ bazel query 'somepath(//xls/codegen:combinational_generator, @com_google_absl//absl/algorithm:container)'\n//xls/codegen:combinational_generator\n//xls/codegen:vast\n@com_google_absl//absl/algorithm:container\n</code></pre> <p>This result shows that one such path goes through the <code>:vast</code> target. Another such path goes through the xls/ir:ir target, then the xls/ir:value target. <code>somepath</code> provides some path, not all paths (that's what <code>allpaths</code> is for).</p>"},{"location":"build_system/#finding-dependees-reverse-dependencies","title":"Finding dependees (\"reverse dependencies\")","text":"<p>Sometimes it's useful to identify the set of targets depending on some other target - the <code>rdeps</code> query performs this:</p> <pre><code>$ bazel query 'rdeps(//xls/codegen:all, //xls/codegen:combinational_generator)'\n//xls/codegen:flattening_test\n//xls/ir:ir_test_base\n//xls/codegen:combinational_generator_test\n//xls/codegen:combinational_generator\n</code></pre> <p>This shows the transitive closure of all dependencies of the combinational generator, with the starting set being all targets in <code>//xls/codegen:all</code>. This set of dependencies can quickly grow to be unmanageable, so keep the initial set (the first argument) as small as possible, and consider specifying a third argument for maximum search depth.</p>"},{"location":"codegen_options/","title":"Codegen Options","text":"<p>This document outlines some useful knobs for running codegen on an XLS design. Codegen is the process of generating RTL from IR and is where operations are scheduled and mapped into RTL constructs. The output of codegen is suitable for simulation or implementation via standard tools that understand Verilog or SystemVerilog.</p> <ul> <li>Codegen Options</li> <li>Input specification</li> <li>Output locations</li> <li>Pipelining and Scheduling Options</li> <li>Feedback-driven Optimization (FDO) Options</li> <li>Naming</li> <li>Reset Signal Configuration</li> <li>Codegen Mapping<ul> <li>Format Strings</li> </ul> </li> <li>I/O Behavior</li> <li>RAMs (experimental)</li> <li>Optimization</li> </ul>"},{"location":"codegen_options/#input-specification","title":"Input specification","text":"<ul> <li><code>&lt;input.ir&gt;</code> is a positional argument giving the path to the ir file.</li> <li><code>--top</code> specifies the top function or proc to codegen.</li> <li><code>--codegen_options_proto=...</code> specifies the filename of a protobuf     containing the arguments to supply codegen other than the scheduling     arguments. Details can be found in codegen_flags.cc</li> <li><code>--scheduling_options_proto=...</code> specifies the filename of a protobuf     containing the scheduling arguments. Details can be found in     scheduling_options_flags.cc</li> </ul>"},{"location":"codegen_options/#output-locations","title":"Output locations","text":"<p>The following flags control where output files are put. In addition to Verilog, codegen can generate files useful for understanding or integrating the RTL.</p> <ul> <li><code>--output_verilog_path</code> is the path to the output Verilog file.</li> <li><code>--output_schedule_path</code> is the path to a textproto that shows into which     pipeline stage the scheduler put IR ops.</li> <li><code>--output_schedule_ir_path</code> is the path to the \"IR\" representation of the     design, a post-scheduling IR that includes any optimizations or transforms     during the scheduling pipeline.</li> <li><code>--output_block_ir_path</code> is the path to the \"block IR\" representation of the     design, a post-scheduling IR that is timed and includes registers, ports,     etc.</li> <li><code>--output_signature_path</code> is the path to the signature textproto. The     signature describes the ports, channels, external memories, etc.</li> <li><code>--output_verilog_line_map_path</code> is the path to the verilog line map     associating lines of verilog to lines of IR.</li> <li><code>--codegen_options_used_textproto_file</code> is the path to write a textproto     containing the actual configuration used for codegen.</li> </ul>"},{"location":"codegen_options/#pipelining-and-scheduling-options","title":"Pipelining and Scheduling Options","text":"<p>The following flags control how XLS maps IR operations to RTL, and if applicable control the scheduler.</p> <ul> <li><code>--generator=...</code> controls which generator to use. The options are     <code>pipeline</code> and <code>combinational</code>. The <code>pipeline</code> generator runs a scheduler     that partitions the IR ops into pipeline stages.</li> <li><code>--delay_model=...</code> selects the delay model to use when scheduling. See the     page here for more detail.</li> <li><code>--clock_period_ps=...</code> sets the target clock period. See     scheduling for more details on how scheduling works. Note     that this option is optional, without specifying clock period XLS will     estimate what the clock period should be.</li> <li><code>--pipeline_stages=...</code> sets the number of pipeline stages to use when     <code>--generator=pipeline</code>.</li> <li><code>--clock_margin_percent=...</code> sets the percentage to reduce the target clock     period before scheduling. See scheduling for more details.</li> <li><code>--period_relaxation_percent=...</code> sets the percentage that the computed     minimum clock period is increased. May not be specified with     <code>--clock_period_ps</code>.</li> <li><code>--minimize_clock_on_error</code> is enabled by default. If enabled, when     <code>--clock_period_ps</code> is given with an infeasible clock (in the sense that XLS     cannot pipeline this input for this clock, even with other constraints     relaxed), XLS will find and report the minimum feasible clock period if one     exists. If disabled, XLS will report only that the clock period was     infeasible, potentially saving time.</li> <li><code>--worst_case_throughput=...</code> sets the worst-case throughput bound to use     when <code>--generator=pipeline</code>. If set, allows scheduling a pipeline with     worst-case throughput no slower than once per N cycles (assuming no stalling     <code>recv</code>s). If not set, defaults to 1.<p>NOTE: If set to 0 or a negative value, no throughput minimum will be   enforced.</p> </li> </ul> <ul> <li><code>--additional_input_delay_ps=...</code> adds additional input delay to the inputs.     This can be helpful to meet timing when integrating XLS designs with other     RTL.</li> </ul> <ul> <li><code>--ffi_fallback_delay_ps=...</code> Delay of foreign function calls if not     otherwise specified. If there is no measurement or configuration for the     delay of an invoked modules, this is the value used in the scheduler.</li> </ul> <ul> <li><code>--io_constraints=...</code> adds constraints to the scheduler. The flag takes a     comma-separated list of constraints of the form <code>foo:send:bar:recv:3:5</code>     which means that sends on channel <code>foo</code> must occur between 3 and 5 cycles     (inclusive) before receives on channel <code>bar</code>. Note that for a constraint     like <code>foo:send:foo:send:3:5</code>, no constraint will be applied between a node     and itself; i.e.: this means all different pairs of nodes sending on <code>foo</code>     must be in cycles that differ by between 3 and 5. If the special     minimum/maximum value <code>none</code> is used, then the minimum latency will be the     lowest representable <code>int64_t</code>, and likewise for maximum latency. For an     example of the use of this, see     this example and     the associated BUILD rule.</li> </ul> <ul> <li><code>explain_infeasibility</code> configures what to do if scheduling fails. If set,     the scheduling problem is reformulated with extra slack variables in an     attempt to explain why scheduling failed.</li> </ul> <ul> <li><code>infeasible_per_state_backedge_slack_pool</code> If specified, the specified value     must be &gt; 0. Setting this configures how the scheduling problem is     reformulated in the case that scheduling fails. If specified, this value     will cause the reformulated problem to include per-state backedge slack     variables, which increases the complexity. This value scales the objective     such that adding slack to the per-state backedge is preferred up until total     slack reaches the pool size, after which adding slack to the shared backedge     slack variable is preferred. Increasing this value should give more specific     information about how much slack each failing backedge needs at the cost of     less actionable and harder to understand output.</li> </ul> <ul> <li><code>--scheduling_options_used_textproto_file</code> is the path to write a textproto     containing the actual configuration used for scheduling.</li> </ul>"},{"location":"codegen_options/#feedback-driven-optimization-fdo-options","title":"Feedback-driven Optimization (FDO) Options","text":"<p>The following flags control the feedback-driven optimizations in XLS. For now, an iterative SDC scheduling method is implemented, which can take low-level feedbacks (typically from downstream tools, e.g., OpenROAD) to guide the delay estimation refinements in XLS. For now, FDO is disabled by default (<code>--use_fdo=false</code>).</p> <ul> <li><code>--use_fdo=true/false</code> Enable FDO. If false, then the <code>--fdo_*</code> options are     ignored.</li> <li><code>--fdo_iteration_number=...</code> The number of FDO iterations during the     pipeline scheduling. Must be an integer &gt;= 2.</li> <li><code>--fdo_delay_driven_path_number=...</code> The number of delay-driven subgraphs in     each FDO iteration. Must be a non-negative integer.</li> <li><code>--fdo_fanout_driven_path_number=...</code> The number of fanout-driven subgraphs     in each FDO iteration. Must be a non-negative integer.</li> <li><code>--fdo_refinement_stochastic_ratio=...</code> *path_number over     refinement_stochastic_ratio paths are extracted and *path_number paths are     randomly selected from them for synthesis in each FDO iteration. Must be a     positive float &lt;= 1.0.</li> <li><code>--fdo_path_evaluate_strategy=...</code> Path evaluation strategy for FDO.     Supports path, cone, and window.</li> <li><code>--fdo_synthesizer_name=...</code> Name of synthesis backend for FDO. Only     supports yosys.</li> <li><code>--fdo_yosys_path=...</code> Absolute path of yosys.</li> <li><code>--fdo_sta_path=...</code> Absolute path of OpenSTA.</li> <li><code>--fdo_synthesis_libraries=...</code> Synthesis and STA libraries.</li> </ul>"},{"location":"codegen_options/#naming","title":"Naming","text":"<p>Some names can be set at codegen via the following flags:</p> <ul> <li><code>--module_name=...</code> sets the name of the generated verilog module.</li> <li>For functions, <code>--input_valid_signal=...</code> and <code>--output_valid_signal=...</code>     adds and sets the name of valid signals when <code>--generator</code> is set to     <code>pipeline</code>.</li> <li><code>--manual_load_enable_signal=...</code> adds and sets the name of an input that     sets the load-enable signals of each pipeline stage.</li> <li>For procs, <code>--streaming_channel_data_suffix=...</code>,     <code>--streaming_channel_valid_suffix=...</code>, and     <code>--streaming_channel_ready_suffix=...</code> set suffixes to be used on their     respective signals in ready/valid channels. For example,     <code>--streaming_channel_valid_suffix=_vld</code> for a channel named <code>ABC</code> would     result in a valid port called <code>ABC_vld</code>.</li> </ul>"},{"location":"codegen_options/#reset-signal-configuration","title":"Reset Signal Configuration","text":"<ul> <li><code>--reset=...</code> sets the name of the reset signal. If not specified, no reset     signal is used.</li> <li><code>--reset_active_low</code> sets if the reset is active low or high. Active high by     default.</li> <li><code>--reset_asynchronous</code> sets if the reset is synchronous or asynchronous     (synchronous by default).</li> <li><code>--reset_data_path</code> sets if the datapath should also be reset. True by     default.</li> </ul>"},{"location":"codegen_options/#codegen-mapping","title":"Codegen Mapping","text":"<ul> <li><code>--use_system_verilog</code> sets if the output should use SystemVerilog     constructs such as SystemVerilog array assignments, <code>@always_comb</code>,     <code>@always_ff</code>, asserts, covers, etc. True by default.</li> <li><code>--separate_lines</code> causes every subexpression to be emitted on a separate     line. False by default.</li> <li><code>--multi_proc</code> causes every proc to be codegen'd.</li> <li><code>max_trace_verbosity</code> is the maximum verbosity allowed for traces. Traces     with higher verbosity are stripped from codegen output. 0 by default.</li> </ul>"},{"location":"codegen_options/#format-strings","title":"Format Strings","text":"<p>For some XLS ops, flags can override their default codegen behavior via format string. These format strings use placeholders to fill in relevant information.</p> <ul> <li><code>--gate_format=...</code> sets the format string for <code>gate!</code> ops. Supported     placeholders are:<p>-   <code>{condition}</code>: Identifier (or expression) of the gate.   -   <code>{input}</code>: Identifier (or expression) for the data input of the gate.   -   <code>{output}</code>: Identifier for the output of the gate.   -   <code>{width}</code>: The bit width of the gate operation.</p> <p>For example, consider a format string which instantiates a particular custom   AND gate for gating:</p> <pre><code>my_and gated_{output} [{width}-1:0] (.Z({output}), .A({condition}), .B({input}))\n</code></pre> <p>And the IR gate operation is:</p> <p><code>the_result: bits[32] = gate(the_cond, the_data)</code></p> <p>This results in the following emitted Verilog:</p> <p><code>my_and gated_the_result [32-1:0] (.Z(the_result), .A(the cond),   .B(the_data));</code></p> <p>To ensure valid Verilog, the instantiated template must declare a value   named <code>{output}</code> (e.g. <code>the_result</code> in the example).</p> </li> </ul> <ul> <li><code>--assert_format=...</code> sets the format string for assert statements.     Supported placeholders are:<p>-   <code>{message}</code>: Message of the assert operation.   -   <code>{condition}</code>: Condition of the assert.   -   <code>{label}</code>: Label of the assert operation. It is an error not to use the       <code>label</code> placeholder.   -   <code>{clk}</code>: Name of the clock signal. It is an error not to use the <code>clk</code>       placeholder.   -   <code>{rst}</code>: Name of the reset signal. It is an error not to use the <code>rst</code>       placeholder.</p> <p>For example, the format string:</p> <p><code>{label}: `MY_ASSERT({condition}, \"{message}\")</code></p> <p>could result in the following emitted Verilog:</p> <p><code>my_label: `MY_ASSERT(foo &lt; 8'h42, \"Oh noes!\");</code></p> </li> </ul> <ul> <li><code>--smulp_format=...</code> and <code>--umulp_format=...</code> set the format strings for     <code>smulp</code> and <code>umulp</code> ops respectively. These ops perform partial (or split)     multiplies. Supported placeholders are:<p>-   <code>{input0}</code> and <code>{input1}</code>: The two inputs.   -   <code>{input0_width}</code> and <code>{input1_width}</code>: The width of the two inputs   -   <code>{output}</code>: Name of the output. Partial multiply IP generally produces       two outputs with the property that the sum of the two outputs is the       product of the inputs. <code>{output}</code> should be the concatenation of these       two outputs.   -   <code>{output_width}</code>: Width of the output.</p> <p>For example, the format string:</p> <pre><code>multp #(\n    .x_width({input0_width}),\n    .y_width({input1_width}),\n    .z_width({output_width}&gt;&gt;1)\n  ) {output}_inst (\n    .x({input0}),\n    .y({input1}),\n    .z0({output}[({output_width}&gt;&gt;1)-1:0]),\n    .z1({output}[({output_width}&gt;&gt;1)*2-1:({output_width}&gt;&gt;1)})])\n  );\n</code></pre> <p>could result in the following emitted Verilog:</p> <pre><code>multp #(\n  .x_width(16),\n  .y_width(16),\n  .z_width(32&gt;&gt;1)\n) multp_out_inst (\n  .x(lhs),\n  .y(rhs),\n  .z0(multp_out[(32&gt;&gt;1)-1:0]),\n  .z1(multp_out[(32&gt;&gt;1)*2-1:(32&gt;&gt;1)])\n);\n</code></pre> <p>Note the arithmetic performed on <code>output_width</code> to make the two-output   <code>multp</code> block fill the concatenated output expected by XLS.</p> </li> </ul>"},{"location":"codegen_options/#io-behavior","title":"I/O Behavior","text":"<ul> <li><code>--flop_inputs</code> and <code>--flop_outputs</code> control if inputs and outputs should be     flopped respectively. These flags are only used by the pipeline generator.<p>For procs, inputs and outputs are channels with ready/valid signalling and   have additional options controlling how inputs and outputs are registered.   <code>--flop_inputs_kind=...</code> and <code>--flop_outputs_kind=...</code> flags control what   the logic around the outputs and inputs look like respectively. The list   below enumerates the possible kinds of output flopping and shows what logic   is generated in each case.</p> <p>-   <code>flop</code>: Adds a pipeline stage at the beginning or end of the block to       hold inputs or outputs. This is essentially a single-element FIFO.</p> </li> </ul> <p></p> <pre><code>-   `skid`: Adds a skid buffer at the inputs or outputs of the block. The\n    skid buffer can hold 2 entries.\n</code></pre> <p></p> <pre><code>-   `zerolatency`: Adds a zero-latency buffer at the beginning or end of the\n    block. This is essentially a single-element FIFO with bypass.\n</code></pre> <p></p> <ul> <li><code>--flop_single_value_channels</code> controls if single-value channels should be     flopped.</li> </ul> <ul> <li><code>--add_idle_output</code> adds an additional output port named <code>idle</code>. <code>idle</code> is     the NOR of:<p>1.  Pipeline registers storing the valid bit for each pipeline stage.</p> <p>2.  All valid registers stored for the input/output buffers.</p> <p>3.  All valid signals for the input channels.</p> </li> </ul> <ul> <li>For functions, when <code>--generator</code> is set to <code>pipeline</code>, optional 'valid'     logic can be added by using <code>--input_valid_signal=...</code> and     <code>--output_valid_signal=...</code>, which also set the names for the valid I/O     signals. This logic has no 'ready' signal and thus provides no backpressure.     See also Naming.</li> </ul> <ul> <li>See also Reset Signal Configuration.</li> </ul>"},{"location":"codegen_options/#rams-experimental","title":"RAMs (experimental)","text":"<p>XLS has experimental support for using proc channels to drive an external RAM. For an example usage, see this delay implemented with a single-port RAM (modeled here). Note that receives on the response channel must be conditioned on performing a read, otherwise there will be deadlock.</p> <p>The codegen option <code>--ram_configurations</code> takes a comma-separated list of configurations in the format <code>ram_name:ram_kind[:kind-specific-configuration]</code>. For a <code>1RW</code> RAM, the format is <code>ram_name:1RW:req_channel_name:resp_channel_name[:latency]</code>, where latency is 1 if unspecified. For a <code>1RW</code> RAM, there are several requirements these channels must satisfy:</p> <ul> <li>The request channel must be a tuple type with 4 entries corresponding to     <code>(addr, wr_data, we, re)</code>. All entries must have type <code>bits</code>, and <code>we</code> and     <code>re</code> must be a single bit.</li> <li>The response channel must be a tuple type with a single entry corresponding     to <code>(rd_data)</code>. <code>rd_data</code> must have the same width as <code>wr_data</code>.</li> </ul> <p>Instead of the normal channel ports, the codegen option will produce the following ports:</p> <ul> <li><code>{ram_name}_addr</code></li> <li><code>{ram_name}_wr_data</code></li> <li><code>{ram_name}_we</code></li> <li><code>{ram_name}_re</code></li> <li><code>{ram_name}_rd_data</code></li> </ul> <p>Note that there are no ready/valid signals as RAMs have fixed latency. There is an internal buffer to catch the response and apply backpressure on requests if needed.</p> <p>When using <code>--ram_configurations</code>, you should generally add a scheduling constraint via <code>--io_constraints</code> to ensure the request-send and response-receive are scheduled to match the RAM's latency.</p>"},{"location":"codegen_options/#optimization","title":"Optimization","text":"<ul> <li><code>--gate_recvs</code> emits logic to gate the data value of a receive operation in     Verilog. In the XLS IR, the receive operation has the semantics that the     data value is zero when the predicate is <code>false</code>. Moreover, for a     non-blocking receive, the data value is zero when the data is invalid. When     set to true, the data is gated and has the previously described semantics.     However, the latter does utilize more resource/area. Setting this value to     false may reduce the resource/area utilization, but may also result in     mismatches between IR-level evaluation and Verilog simulation.</li> </ul> <ul> <li><code>--array_index_bounds_checking</code>: With this option set, an out of bounds     array access returns the maximal index element in the array. If this option     is not set, the result relies on the semantics of out-of-bounds array access     in Verilog which is not well-defined. Setting this option to <code>true</code> may     result in more resource/area. Setting this value to <code>false</code> may reduce the     resource/area utilization, but may also result in mismatches between     IR-level evaluation and Verilog simulation.</li> </ul> <ul> <li><code>--mutual_exclusion_z3_rlimit</code> controls how hard the mutual exclusion pass     will work to attempt to prove that sends and receives are mutually     exclusive. Concretely, this roughly limits the number of <code>malloc</code> calls done     by the Z3 solver, so the output should be deterministic across machines for     a given rlimit.</li> </ul> <ul> <li><code>--default_next_value_z3_rlimit</code> controls how hard our scheduling passes     will work to prove that state params are fully covered by their <code>next_value</code>     nodes, so that we can skip special handling for the case where no     <code>next_value</code> node triggers. This is purely an optimization; everything will     work correctly even if this is disabled (omitted, or set to -1). Concretely,     this roughly limits the number of <code>malloc</code> calls done by the Z3 solver, so     the output should be deterministic across machines for a given rlimit.</li> </ul> <ul> <li><code>--register_merge_strategy</code> controls how we merge registers between stages     which may be shared. The options are <code>identity</code> which merges registers which     can be shared and contain exactly the same value and <code>none</code> which disables     register merging. Registers are eligible for merging if the stages they are     read in are not simultaneously activatable and the registers are the same     type.</li> </ul>"},{"location":"contributing/","title":"How to Contribute","text":"<p>We'd love to accept your patches and contributions to XLS. We recommend filing an issue for back-and-forth discussion on implementation strategy before sending a PR. Also, note the community guidelines below.</p>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<p>This project follows Google's Open Source Community Guidelines.</p>"},{"location":"contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one.</p> <p>You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.</p>"},{"location":"contributing/#code-style","title":"Code style","text":"<p>When writing code contributions to the project, please make sure to follow the style guides: The Google C++ Style Guide and the Google Python Style Guide. There are a few small XLS clarifications for local style on this project where the style guide is ambiguous.</p>"},{"location":"contributing/#code-reviews","title":"Code reviews","text":"<p>All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.</p>"},{"location":"contributing/#pull-request-style","title":"Pull Request Style","text":"<p>We ask contributors to squash all the commits in the PR into a single one, in order to have a cleaner revision history.</p> <p>Specifically, when you initially send a PR, please ensure it has a single commit. If you'd like to address review comments by adding commits,<sup>1</sup> please be sure to squash them into one again once the PR is approved (though squashing continuously is also acceptable).</p> <p>Generally, squashing to a single commit can be accomplished by:</p> <pre><code>proj/xls$ # Here we assume origin points to google/xls.\nproj/xls$ git fetch origin main\nproj/xls$ git merge-base origin/main my-branch-name  # Tells you common ancesor COMMIT_HASH.\nproj/xls$ git reset --soft $COMMIT_HASH\nproj/xls$ git commit -a -m \"My awesome squashed commit message!!!1\"\nproj/xls$ # Now we can more easily rebase our squashed commit on main.\nproj/xls$ git rebase origin/main\n</code></pre> <p>Rebased branches can be pushed to their corresponding PRs with <code>--force</code>.</p> <p>See also this Stack Overflow question.</p>"},{"location":"contributing/#rendering-documentation","title":"Rendering Documentation","text":"<p>XLS uses mkdocs to render its documentation, and serves it via GitHub pages at https://google.github.io/xls. To render documentation locally as a preview, you can set up mkdocs as follows:</p> <pre><code>proj/xls$ mkvirtualenv xls-mkdocs-env\nproj/xls$ pip install mkdocs-material mkdocs-exclude mdx_truly_sane_lists\nproj/xls$ mkdocs serve\n</code></pre> <p>This will start a local server that you can browse to and that will update the documentation on the fly as you make changes.</p> <p>Note that the <code>mkvirtualenv</code> command assumes you're using virtualenvwrapper to manage your Python environment. You'll need to adjust these instrutions if you're doing something different. That can include explicitly adding <code>mkdocs</code> to your path, if locally installed Python binaries aren't available by default.</p>"},{"location":"contributing/#dsl-snippets-in-documentation","title":"DSL snippets in documentation","text":"<p>There are a few different language annotations we use in different circumstances in the Markdown docs:</p> <ul> <li><code>dslx</code>: A full code block that should be parsed/typechecked/tested.</li> <li><code>dslx-snippet</code>: A fragment that should be syntax highlighted, but not   parsed/typechecked/tested.</li> <li><code>dslx-bad</code>: An example of something that we expect to produce an error   when parsing/typechecking/testing.</li> </ul> <p>GitHub issue google/xls#378 tracks a script that does the parse/typecheck/test that ensures our documentation is up to date and correct.</p>"},{"location":"contributing/#github-issue-t-shirt-size-estimate-labels","title":"GitHub Issue \"T-Shirt Size\" Estimate Labels","text":"<p>We attempt to employ some lightweight processes for task size estimation for the GitHub issues in the XLS repository, as a way of making tasks available that fit for available development time as well as gut checking, if something takes longer than we expected, why and can we do things to mitigate the surprising amount of time required going forward.</p> <p>There's a practice of marking issues with \"t-shirt sizes\" for development tasks. An issue can be XS, S, M, L, XL, these are given in the \"estimate\" labels:</p> Name Abbreviation Time Scale eXtra Small XS ~few hours Small S ~a day Medium M ~1-3 days Large L ~a week eXtra Large XL ~multi-week <p>These are not \"load bearing\", just to note expectation. Generally the assumption is \"time expected for a person familiar with this matter / part of the code base\", so developers that would ramp on an issue would require more time than is indicated by the label. Feel free to change the label at will, ideally by providing a helpful explanation for why/how the estimate came to change.</p> <ol> <li> <p>Adding commits preserves the GitHub code review history and makes it   easier to review incremental changes, but causes an additional \"round trip\"   with the reviewer for the final squash after approval, so there is a small   procedural tradeoff.\u00a0\u21a9</p> </li> </ol>"},{"location":"data_layout/","title":"Data layout","text":"<p>For many uses, XLS types exist within their own conceptual space or domain, so \"portability\" concerns don't exist. When interacting with the JIT, however, XLS and host-native types must interact, so the data layouts of both must be understood and possibly reconciled.</p> <ul> <li>Data layout<ul> <li>XLS data layout</li> <li>Host data layout</li> <li>JIT data layout</li> <li>Packed views<ul> <li>Tuple types</li> </ul> </li> </ul> </li> </ul>"},{"location":"data_layout/#xls-data-layout","title":"XLS data layout","text":"<p>The concrete XLS <code>Bits</code> type is the ultimate container of actual data for any XLS IR type: tuples and arrays may be contain any number of tuple, array, or bits types, but whatever the layout of the type tree, all leaf nodes are Bits. When accessing the underlying storage of a <code>Bits</code> via the <code>ToBytes()</code> member function, the results are returned in a little-endian layout, i.e, with the least-significant data elements stored in the lowest addressable location. For example, the 32-bit value 12,345,678 (0xBC614E), would be returned as:</p> <pre><code>High  &lt;--  Low\n0x 00 BC 61 4E\n</code></pre>"},{"location":"data_layout/#host-data-layout","title":"Host data layout","text":"<p>Different architectures can use different native layouts. For example, x86 (and descendants) use little-endian (i.e., <code>0x00 BC 61 4E</code>), and modern ARM can be configurable as either. (There are actually other layouts, but they're best left to the dustbin of history).</p>"},{"location":"data_layout/#jit-data-layout","title":"JIT data layout","text":"<p>From the above, we can see that XLS' native layout differs from that of most modern hosts. When compiling XLS code, the [LLVM] JIT understandably uses the host's native layout. What this means is that any data fed into the JIT from XLS will need to be byte-swapped before ingestion.</p> <p>For Value or unpacked view input, this swapping is handled automatically, in <code>LlvmIrRuntime::PackArgs()</code> (via <code>LlvmIrRuntime::BlitValueToBuffer()</code>) - and the __un__swapping is also automatically performed in <code>LlvmIrRuntime::UnpackBuffer()</code>. Thus, for these uses, no special action is required of the user.</p>"},{"location":"data_layout/#packed-views","title":"Packed views","text":"<p>However, this is not the case for use of packed views. The motivating use case for packed views is to allow users to map native types directly into JIT-usable values - for example, to use an IEEE float32 (e.g., a C <code>float</code>) directly, without needing to be exploded into a <code>bits[1]</code> for the sign, a <code>bits[8]</code> for the exponent, and a <code>bits[23]</code> for the fractional part.</p> <p>When creating a packed view from a C <code>float</code>, no special action is needed - that <code>float</code> is in native host layout, which is the layout used by the JIT. If, however, data is coming from XLS (perhaps a <code>float</code> converted into a Value, manipulated in some way, then passed into the JIT), then the user must un-swap the bits back to native layout. This is because the JIT has no way of knowing the provenance of that data (if it's from a native type or XLS), so it's up to the provider of that data to ensure proper layout.</p> <p>Distilled into a simple rule of thumb: if packed view data is coming from XLS, it needs to be byte swapped before being passed into the JIT.</p>"},{"location":"data_layout/#tuple-types","title":"Tuple types","text":"<p>Another wrinkle is the usage of packed tuple views. When XLS emits a tuple type in Verilog, the first element in the tuple declaration is placed in the most significant bits, and so on, with the last-declared element placed in the least significant bits. To match this layout, PackedTupleView elements must be also declared from most significant to least significant element. This way, when running on a host, the in-memory layout of input data matches that expected by XLS tools. That means, using the usual float32 example, that the packed view declaration is:</p> <pre><code>PackedTupleView&lt;PackedBitsView&lt;1&gt;, PackedBitsView&lt;8&gt;, PackedBitsView&lt;23&gt;&gt;\n</code></pre> <p>(The non-packed-View tuple declaration is much the same, but matters less, as it doesn't directly correspond to in-memory data layout.)</p> <p>Be aware of this layout when accessing elements in a PackedTupleView. In the float32 above, accessing element 0 yields the sign bit (the most significant bit in memory), and accessing element 2 yields the fractional part (the least significant 23 bits in memory), as one would expect given the tuple type declaration order.</p> <p>While this may initially seem confusing, it suffices to remember that PackedTupleView element declaration order is the \"reverse\" of the in-memory order; refer to value_view_test.cc and function_jit_test.cc for test examples, or the [generated] float32_add_jit_wrapper.h/cc and float32_add_test.cc for practical usage.</p>"},{"location":"delay_estimation/","title":"Delay Estimation Methodology","text":""},{"location":"delay_estimation/#context","title":"Context","text":"<p>This doc describes the delay estimation methodology used in XLS and related background.</p> <p>Estimating delays through networks of CMOS gates is a rich topic, and Static Timing Analysis (STA) is used in chip backend flows to ensure that, even for parts operating at the tail end of the distribution, chips continue to function as specified logically in the netlist.</p> <p>In stark contrast to something with very concrete constraints, like a \"post-global routing, high-accuracy parasitics static timing analysis\", the HLS tool needs to estimate at a high level, reasonably near to the user's behavioral specification, what delay HLS operations will have when they are \"stacked on top\" of each other in a data dependent fashion in the design.</p> <p>This information lets the HLS tool schedule operations into cycles without violating timing constraints; e.g. if the user specifies 1GHz (= 1000ps) as their target clock frequency, the HLS tool may choose to pack as much data dependent work into the 1000ps budget (minus clock uncertainty) as it can on any given cycle.</p> <p>Although what we're estimating is a close relative of Static Timing Analysis, the fact it's being analyzed at a very high level presents a different set of tradeoffs, where coarse granularity estimation and conservative bounds are more relevant. Fine-grained / precise analyses are used later in the chip design process, in the backend flows, but the HLS tool acts more like an RTL designer, creating an RTL output that early timing analysis deems acceptable.</p> <p>Notably, we don't want to be too conservative, as being conservative on timing can lead to designs that take more area and consume more power, as the flops introduced by additional pipeline stages are significant. We want to be as accurate as possible while providing a good experience of being able to close timing quickly (say, in a single pass, or with a small number of iterations in case of a pathological design).</p>"},{"location":"delay_estimation/#background","title":"Background","text":""},{"location":"delay_estimation/#what-xls-is-compiling","title":"What XLS is compiling","text":"<p>XLS currently supports feed-forward pipelines -- once the user program is unrolled into a big \"sea of nodes\" we must schedule each of those operations (represented by nodes) to occur in a cycle.</p> <p>It seems clear that an operation like <code>add(bits[32], bits[32]) -&gt; bits[32]</code> takes some amount of time to produce a result -- we need to be able to determine what that amount of time is for packing that operation into a given cycle. <sup>1</sup> Note that XLS operations are parametric in their bitwidth, so <code>add(bits[17], bits[17]) -&gt; bits[17]</code> is just as possible as a value like <code>32</code>. This ain't C code.</p> <p>1: Note that we currently pack operations into cycles atomically -- that is, we don't break an <code>add</code> that would straddle a cycle boundary into <code>add.first_half</code> and <code>add.second_half</code> automatically to pull things as early as possible in the pipeline, but this is future work of interest. Ideally operations would be described structurally in a way that could automatically be cut up according to available delay budget. This would also permit operations in the IR that take more than a single cycle to produce a value (currently they would have to be \"legalized\" into operations that fit within a cycle, but that is not yet done, the user will simply receive a scheduling error).</p> <p>Some operations, such as <code>bit_slice</code> or <code>concat</code> are just wiring \"feng shui\"; however, they still have some relevance for delay calculations! Say we concatenate a value with zeros for zero extension. Even if we could schedule that in \"cycle 0\", if the consumer can only be placed in \"cycle 1\", we would want to \"sink\" the concat down into \"cycle 1\" as well to avoid unnecessary registers being materialized sending the zero values from \"cycle 0\".</p>"},{"location":"delay_estimation/#the-delay-problem","title":"The delay problem","text":"<p>Separately from XLS considerations, there are fundamental considerations in calculating the delay through clouds of functional logic in (generated) RTL.</p> <p>Between each launching and capturing flop is a functional network of logic gates, implemented with standard cells in our ASIC process flows. Chip designs target a particular clock frequency as their operating point, and the functional network has to produce its output value with a delay that meets the timing constraint of the clock frequency. The RTL designer typically has to iterate their design until:</p> <p>timing path delay &lt;= target clock period - clock uncertainty</p> <p>For all timing paths in their design, where clock uncertainty includes setup/hold time constraints, and slop that's built in as margin for later sources of timing variability (like instantiating a clock tree, which can skew the clock signal observed by different flops).</p> <p>In a reasonable model, gate delay is affected by a small handful of properties, as reflected in the \"(Method of) Logical Effort\" book:</p> <ul> <li>The transistor network used to implement a logic function (AKA logical   effort): on an input pin change, the gate of each transistor must be driven to   a point it recognizes whether a 0 or 1 voltage is being presented. More gates   to drive, or larger gates, means more work for the driver.</li> <li>The load being driven by the logic function (AKA electrical effort): fanning   out to more gates generally means more work to drive them all to their   threshold voltages. Being loaded down by bigger gates means more work to drive   it to its threshold voltage.</li> <li>Parasitic delays: RC elements in the system that leech useful work, typically   in a smaller way compared to the efforts listed above.</li> </ul> <p>The logical effort book describes a way to analyze the delays through a network of gates to find the minimal delay, and size transistors in a way that can achieve that minimal delay (namely by geometrically smoothing the ability for gate to drive capacitance).</p> <p>Confounding factors include:</p> <ul> <li>Medium/large wires: sizing transistors to smooth capacitance becomes difficult   as fixed-capacitance elements (wires) are introduced. It seems that small   wires have low enough capacitance they can generally be treated as parasitic.</li> <li>Divergence/reconvergence in the functional logic network (as a DAG). Different   numbers of logic levels and different drive currents may be presented from   different branches of a fork/join the logic graph, which forces delay analysis   into a system of equations to attempt to minimize the overall delay, as   observed by the critical path, with transistor sizing and gate choices. (Some   simplifications are possible, like buffering non-critical paths until they   have the same number of logic levels so they also have plenty of current to   supply at join points.)</li> </ul> <p>Somewhat orthogonal to the analytical modeling problem, there are also several industry standards for supplying process information to Static Timing Analysis engines for determining delay through a netlist. This information is often given in interpolated tables for each standard cell, for example in the NLDM model describing how delay changes as a function of input transition time and load (load capacitance).</p> <p>These models and supplied pieces of data are important to keep in mind for contrast, as we now ignore it all and do something very simple.</p>"},{"location":"delay_estimation/#simple-delay-estimation","title":"Simple Delay Estimation","text":"<p>Currently, XLS delay estimation follows a conceptually simple procedure:</p> <ul> <li> <p>For every operation in XLS (e.g. binary addition):</p> <ul> <li>For some relevant-seeming set of bitwidths; e.g. <code>{2, 4, 8, 16, ...,     2048}</code></li> <li>Find the maximum frequency at which that operation closes timing at that     bitwidth, in 100MHz units as determined by the synthesis tool.     <sup>2</sup> Call the clock period for this frequency     <code>t_best</code>. (Note that we currently just use a single process corner /     voltage for this sweep.)</li> <li>Subtract the clock uncertainty from <code>t_best</code>.</li> <li>Record that value in a table (with the keys of the table being operation     / bitwidth).</li> </ul> </li> </ul> <p>2: The timing report can provide the delay through a path at any clock frequency, but a wrinkle is that synthesis tools potentially only start using their more aggressive techniques as you bump up against the failure-to-close-timing point -- there it'll be more likely to change the structure of the design to make it more delay friendly. The sweep helps to try to cajole it in that way.</p> <p>Inspecting the data acquired in this way we observe all of the plots consist of one or more the following delay components:</p> <ul> <li>Constant as a function of bitwidth for a given op (e.g. binary-or just     requires a single gate for each bit regardless of the width of the inputs).</li> <li>Logarithmic as a function of bitwidth (e.g. adders may end up using     tree-like structures to minimize delay, single-selector muxes end up using a     tree to fan out the selector to the muxes, etc.).</li> <li>Linear as a function of bitwidth (e.g., ripple-carry adders and some     components of multipliers).</li> </ul> <p>So given this observation we fit a curve of the form:</p> <pre><code>a * bitwidth + b * log_2(bitwidth) + c\n</code></pre> <p>to the sweep data for each operation, giving us <code>(a, b, c)</code> values to use in our XLS delay estimator.</p> <p>The utility <code>delay_model_visualizer</code> under the <code>tools</code> directory renders a graph of the delay model estimate against the measured data points. This graph for add shows a good correspondence to the measured delay.</p> <p></p>"},{"location":"delay_estimation/#sweeping-multiple-dimensions","title":"Sweeping multiple dimensions","text":"<p>Operations with attributes in addition to bitwidth that affect delay are swept across multiple dimensions. An example is <code>Op::kOneHotSelect</code> which has dimensions of bitwidth and number of cases. For the <code>Op::kOneHotSelect</code> example the formula is:</p> <pre><code>a * bitwidth + b * log_2(bitwidth) + c * (# of cases) + d * log_2(# of cases) + c\n</code></pre> <p>Below is plot of delay for <code>Op::kOneHotSelect</code> showing the two dimensions of bitwidth and operand count affecting delay:</p> <p></p>"},{"location":"delay_estimation/#sources-of-pessimismoptimism","title":"Sources of pessimism/optimism","text":"<p>This simple method has both sources of optimism and pessimism, though we hope to employ a method that will be generally conservative, so that users can easily close timing and get a close-to-best-achievable result (say, within tens of percent) with a single HLS iteration.</p> <p>Sources of pessimism (estimate is conservative):</p> <ul> <li>The operation sweeps mentioned are not bisecting to the picosecond, so there     is inherent slop in the measurement on account of sweep granularity.</li> <li>We expect, in cycles where multiple dependent operations are present, there     would be \"K-map style\" logic reductions with adjacent operations. For     example, because we don't do cell library mapping in XLS delay estimation,     something a user wrote that mapped to an AOI21 cell would be the sum of     (and+or+invert) delays.</li> <li>[Unsure] May there be additional logic branch splitting options and     earlier-produced results available to the synthesis tool when there are more     operations in the graph (vs a lone critical path measured for a single     operation)?</li> </ul> <p>Sources of optimism (estimate is overeager):</p> <ul> <li>For purposes of the sweep the outputs of an operation are only loaded by a     single capture flop flop -- when operations have fanout the delay will     increase.<p>Note that we do account for fanout within individual operations as part of   this sweep; e.g. a 128-bit selector fanout (e.g. 128 ways to 128 muxes) for   a select is accounted for the in delay timing of the select operation. It is   the output logic layer that is only loaded by a single flop in our   characterization. Notably because most of these operations turn into trees   of logic, there are \\(\\(log_2(bitcount)\\)\\) layers of logic in which we can   potentially smoothly increase drive strength out to the output logic layer,   and paths can presumably be replicated by synthesis tools to reduce   pointwise fanout when multiple high-level operations are data-dependent   within a cycle. (Is it possible for a user to break up their 32-bit select   into bitwise pieces in their XLS code to mess with our modeling? Sure, but   probably not too expected, so we're currently sort of relying on the notion   people are using high level operations instead of compodecomposing them into   bitwise pieces in our graph.)</p> <p>A potential way to reconcile this output fanout in the future is to do a   delay sweep with a high-capacitive fanout (e.g. four flops of load) and then   ensure the IR has a maximum fanout of four for our delay estimation.</p> </li> </ul> <ul> <li>Wiring delay / load / congestion / length are not estimated. This will need     additional analysis / refinement as we run XLS through synthesis tools with     advanced timing analysis, as it is certainly not viable for arbitrary     designs (tight pipelines may be ok for now, though).</li> </ul>"},{"location":"delay_estimation/#iterative-refinement","title":"Iterative refinement","text":"<p>The caveats mentioned above seem somewhat daunting, but this first cut approach appears to work comfortably at target frequenties, in practice, for the real-world blocks being designed as XLS \"first samples\".</p> <p>Notably, human RTL designers fail to close timing on a first cut as well -- HLS estimations like the above assist in getting a numeric understanding (in lieu of an intuitive guess) of something that may close timing on a first cut. As this early model fails, we will continue to refine it; however, there is also a secondary procedure that can assist as the model improves.</p> <p>Let's call the delay estimation described above applied to a program a prediction of its delays. Let's call the first prediction we make <code>p0</code>: <code>p0</code> will either meet timing or fail to meet timing.</p> <p>When we meet timing with <code>p0</code>, there may be additional wins left on the table. If we're willing to put synthesis tool runs \"in the loop\" (say running a \"tuner\" overnight), we can refine XLS's estimates according to the realities of the current program, and, for example, try to squeeze as much as possible into as few cycles as possible if near-optimality of latency/area/power were a large consideration. This loop would generate <code>p1</code>, <code>p2</code>, ... as it refined its model according to empirical data observed from the synthesis tool's more refined analysis.</p> <p>When we fail to close timing with <code>p0</code>, we can feed back the negative slack delays for comparison with our estimates and relax estimates accordingly. Additionally, an \"aggression\" knob could be implemented that backs off delay estimations geometrically (say via a \"fudge factor\" coefficient) in order to ensure HLS developer time is not wasted unnecessarily to ensure. Once a combination of these mechanisms has obtained a design that closes timing, the \"meeting timing, now refine\" procedure can be employed as described above.</p>"},{"location":"delay_estimation/#on-hints","title":"On Hints","text":"<p>To whatever extent possible, XLS should be the tool that reasons about how to target the backend (vs having a tool that sits on top of it and messes with XLS' input in an attempt to achieve a result). User-facing hint systems are typically very fragile, owing to the fact they don't have easily obeyed semantics. XLS, by contrast, knows about its own internals, so can do things with awareness of what's happened upstream and what remains to happen downstream.</p> <p>By contrast, we should continue to add ways for users to provide more semantic information / intent as part of their program (e.g. via more high-level patterns that make high level structure more clear), and make XLS smarter about how to lower those constructs into hardware (and why it should be lowering them that way) in the face of some prioritized objectives (power/area/latency).</p> <p>That being said, because we're probably trying to produce hardware at a given point in time against a given technology, it likely makes sense to permit human users to specify things directly (at a point in time), even if those specifications might be ignored / handled very differently in the future or against different technology nodes. This would be the moral equivalent of what existing EDA tools do as a \"one-off TCL file\" used in a particular design, vs something carried from design to design. Recall, though, that the intent of XLS is to make things easier to carry from design to design and require fewer one-off modifications!</p>"},{"location":"delay_estimation/#tools","title":"Tools","text":"<p>XLS provides tools for analyzing its delay estimation model. (Note that the given IR should be in a form suitable for code generation; e.g. it has run through the <code>opt_main</code> binary).</p> <pre><code>$ bazel run -c opt //xls/tools:benchmark_main -- $PWD/bazel-bin/xls/examples/crc32.opt.ir --clock_period_ps=500 --delay_model=sky130\n&lt;snip&gt;\nCritical path delay: 8351ps\nCritical path entry count: 43\nCritical path:\n   8351ps (+ 21ps): not.37: bits[32] = not(xor.213: bits[32], id=37, pos=[(0,30,51)])\n   8330ps (+128ps): xor.213: bits[32] = xor(concat.203: bits[32], and.222: bits[32], id=213, pos=[(0,25,19)])\n   8202ps (+ 81ps): and.222: bits[32] = and(mask__7: bits[32], literal.395: bits[32], id=222, pos=[(0,25,33)])\n   8121ps (+621ps)!: mask__7: bits[32] = neg(concat.199: bits[32], id=202, pos=[(0,24,15)])\n   7330ps (+  0ps): concat.199: bits[32] = concat(literal.387: bits[31], bit_slice.198: bits[1], id=199, pos=[(0,24,21)])\n   7330ps (+  0ps): bit_slice.198: bits[1] = bit_slice(xor.196: bits[32], start=0, width=1, id=198, pos=[(0,24,21)])\n   7330ps (+128ps): xor.196: bits[32] = xor(concat.194: bits[32], and.221: bits[32], id=196, pos=[(0,25,19)])\n   7202ps (+ 81ps): and.221: bits[32] = and(mask__6: bits[32], literal.394: bits[32], id=221, pos=[(0,25,33)])\n&lt;snip&gt;\n</code></pre> <p>In addition to the critical path, the cycle-by-cycle breakdown of which operations have been scheduled is provided in stdout.</p>"},{"location":"dslx_bytecode_interpreter/","title":"Bytecode interpreter","text":"<p>DSLX provides a bytecode interpreter for expression evaluation. This style of interpreter can be started, stopped, and resumed more easily than an AST-walking native interpreter, as its full state can be captured as <code>{PC, stack}</code> instead of some traversal state in native execution, which makes it very suitable for modeling independent processes, such as <code>Proc</code>s.</p> <p>NOTE: The bytecode interpreter system is under active construction and does not yet support the full set of DSLX functionality.</p> <ul> <li>Bytecode interpreter<ul> <li>Structure</li> <li>ISA</li> <li>Bytecode generation</li> <li>Implementation details<ul> <li>map builtin</li> </ul> </li> </ul> </li> </ul>"},{"location":"dslx_bytecode_interpreter/#structure","title":"Structure","text":"<p>The interpreter is implemented as a stack virtual machine: it consists of a program counter (PC), a stack of frames, and \"slot\"-based locals within a given stack frame (conceptually part of the stack frame, but tracked separately in our implementation). Both the stack and local storage hold <code>InterpValues</code>, which can hold all DSLX data types: bits, tuples, and arrays (and others), thus there is no fundamental need for lower-level (i.e., byte) type representation. For the purposes of [de]serialization, this may change in the future. Local data is addressed by integer-typed \"slots\", being backed by a simple <code>std::vector</code>: in other words, slot indices are dense. All slots must be pre-allocated to contain all references to locals in the current function stack frame.</p> <p>On each \"tick\", the interpreter reads the current instruction, as given by the PC (conceptually, the only register in the virtual machine), executes the described operation (usually consuming values from the stack), and places the result on the stack.</p>"},{"location":"dslx_bytecode_interpreter/#isa","title":"ISA","text":"<p>Each instruction consists of an opcode plus, optionally, some piece of data, either <code>int64</code>- or <code>InterpValue</code>-typed, depending on the specific opcode.</p> <p>The below opcodes are supported by the interpreter:</p> <ul> <li><code>ADD</code>: Adds the two values at the top of the stack.</li> <li><code>CALL</code>: Invokes the function given as the optional data argument, consuming     a number of arguments from the stack as described by the function signature.     The N'th parameter will be present as the N'th value down the stack (such     that the last parameter will be the value initially on top of the stack.</li> <li><code>CREATE_TUPLE</code>: Groups together N items on the stack (given by the optional     data argument into a single <code>InterpValue</code>.</li> <li><code>EXPAND_TUPLE</code>: Expands the N-tuple at stack top by one level, placing     leading elements at stack top. In other words, expanding the tuple <code>(a, (b,     c))</code> will result in a stack of <code>(b, c), a</code>, where <code>a</code> is on top of the     stack.</li> <li><code>EQ</code>: Compares the two values on top of the stack for equality. Emits a     single-bit value.</li> <li><code>LOAD</code>: Loads the value from locals slot <code>n</code>, where <code>n</code> is given by the     optional data argument.</li> <li><code>LITERAL</code>: Places a literal value (given in the optional data argument) on     top of the stack.</li> <li><code>STORE</code>: Stores the value at stack top into slot <code>n</code> in locals storage.</li> </ul>"},{"location":"dslx_bytecode_interpreter/#bytecode-generation","title":"Bytecode generation","text":"<p>The bytecode emitter is responsible for converting a set of DSLX ASTs (one per function)) into a set of linear bytecode representations. It does this via a postorder traversal of the AST, converting XLS ops into bytecode instructions along the way, e.g., converting a DSLX <code>Binop</code> for adding two <code>NameRef</code>s into two <code>LOAD</code> instructions (one for each <code>NameRef</code>) and one <code>ADD</code> instruction.</p> <p>To do this, the emitter needs access to the full set of resolved type and import information: in other words, it requires a fully-populated <code>ImportData</code> and the top-level <code>TypeInfo</code> for the module containing the function to convert. This places bytecode emission in sequence after typechecking and deduction.</p>"},{"location":"dslx_bytecode_interpreter/#implementation-details","title":"Implementation details","text":""},{"location":"dslx_bytecode_interpreter/#map-builtin","title":"<code>map</code> builtin","text":"<p>The <code>map()</code> function built-in to DSLX accepts an array-typed value <code>x</code> and a mapping function <code>f</code> with the signature <code>T -&gt; U</code>; that is, it accepts a single value of type <code>T</code> and returns a single value with the type <code>U</code>. In operation, <code>map()</code> applies the mapping function <code>f</code> to every element in <code>x</code> and returns a new array containing the results (with element <code>i</code> in the output corresponding to element <code>i</code> in the input).</p> <p>Conceptually, <code>map()</code> destructures to a <code>for</code> loop over the elements in <code>x</code>, and that's essentially what the interpreter does with these opcodes. To avoid modifying the currently executing bytecode, the interpreter instead creates a new BytecodeFunction consisting of just that destructured <code>for</code> loop over the inputs, followed by a CreateArray op to collect the output(s). Finally, the interpreter begins execution of the new function by creating a new <code>Frame</code> on top of the execution stack.</p>"},{"location":"dslx_language_server/","title":"DSLX Language Server","text":"<p>Many popular editors in the modern era are speaking a common protocol in order to understand how to display, traverse, and maniulate languages: the \"Language Server Protocol\". This allows novel languages and DSLs, like DSLX, to expose a developer experience integrated in their preferred editors and IDEs.</p> <p>To use the language server protocol in your editor you do not need to know any details about the language server protocol.</p> <p>Language server feedback in the editor is useful to folks who are learning DSLX as well as those developing in it on a regular basis! The language server currently offers functionality such as:</p> <ul> <li>Go-to-definition</li> <li>Errors/warnings as you type</li> <li>An \"overview\" of the symbols defined in a module</li> </ul> <p>For more background on what the language server protocol can do, see the Language Server Protocol documentation and specification.</p>"},{"location":"dslx_language_server/#building-the-dslx-language-server-binary","title":"Building the DSLX Language Server binary","text":"<p>The following are instructions for building the DSLX language server binary. By a) placing the language server binary into your <code>PATH</code> and b) configuring your editor to handle <code>.x</code> files by using it.</p> <p>Follow the XLS build setup instructions so that the pre-requisites are available for building binaries via Bazel. Then, build the following <code>dslx_ls</code> binary and place it in your <code>PATH</code>:</p> <pre><code>~/xls$ bazel build -c opt //xls/dslx/lsp:dslx_ls\n~/xls$ mkdir ~/bin/\n~/xls$ cp -iv bazel-bin/xls/dslx/lsp/dslx_ls ~/bin/\n~/xls$ export PATH=$HOME/bin:$PATH\n</code></pre> <p>Now that the language server binary is available in your <code>PATH</code>, you must configure your editor to find/use it for <code>.x</code> files.</p>"},{"location":"dslx_language_server/#vim","title":"Vim","text":"<p>First we must install <code>vim-plug</code> -- follow the latest instructions at <code>https://github.com/junegunn/vim-plug</code>; e.g.:</p> <pre><code>$ curl -fLo ~/.vim/autoload/plug.vim --create-dirs \\\n    https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n</code></pre> <p>Add the following configuration to your <code>$HOME/.vimrc</code>:</p> <pre><code>call plug#begin()\nPlug 'prabirshrestha/vim-lsp'\nPlug 'prabirshrestha/asyncomplete-lsp.vim'\nPlug 'mattn/vim-lsp-settings'\n\ncall plug#end()\n\nlet g:lsp_log_verbose = 1\nlet g:lsp_log_file = expand('~/vim-lsp.log')\n\nif executable('dslx_ls')\n    au User lsp_setup call lsp#register_server({\n        \\ 'name': 'dslx_ls',\n        \\ 'cmd': {server_info-&gt;['dslx_ls']},\n        \\ 'allowlist': ['dslx', '.x'],\n        \\ })\nendif\n\nlet g:lsp_diagnostics_echo_cursor = 1\nlet g:lsp_diagnostics_highlights_enabled = 1\nlet g:lsp_diagnostics_signs_enabled = 1\n\nau BufRead,BufNewFile *.x set filetype=dslx\n</code></pre> <p>Run vim and execute <code>:PlugInstall</code> to get the new LSP plugins. After that completes successfully, quit Vim (famously, via <code>:q</code>).</p> <p>Open <code>foo.x</code> in vim and then paste the following contents:</p> <pre><code>fn main() -&gt; u8 { u8:256 }\n</code></pre> <p>The following should show in the display line:</p> <pre><code>LSP: uN[8] Value '256' does not fit in the bitwidth of a uN[8] (8). Valid values are [0, 255].\n</code></pre> <p>If not, try using <code>:LspStatus</code> To see if any diagnostics are available.</p> <p>If you correct the <code>u8</code> value to be 255 (and thus in range) you can run:</p> <p><code>:LspDocumentSymbol</code></p> <p>To see the defined symbol listing -- this shows all defined symbols in the file:</p> <pre><code>foo.x|1 col 1| method : main\n</code></pre>"},{"location":"dslx_language_server/#troubleshooting","title":"Troubleshooting","text":"<p>With the above <code>.vimrc</code> contents, logs should show up in <code>$HOME/vim-lsp.log</code>.</p> <p>Issues can be filed against https://github.com/google/xls/issues with associated contents/logs.</p>"},{"location":"dslx_language_server/#emacs","title":"Emacs","text":"<p>The following <code>.emacsrc</code> snippet wires up the language server in emacs, piggy-backing on the Rust major mode.</p> <pre><code>(which-key-mode)\n(require 'lsp-mode)\n(add-to-list 'auto-mode-alist '(\"\\\\.x\\\\'\" . rust-mode))\n;; DSLX can make rust-mode very slow for large files due to angle bracket\n;; matching which is inefficiently implemented. Disable the feature.\n(setq rust-match-angle-brackets nil)\n(add-to-list 'lsp-language-id-configuration '(rust-mode . \"dslx\"))\n(lsp-register-client\n (make-lsp-client :new-connection (lsp-stdio-connection \"~/bin/dslx_ls\")\n                  :major-modes '(rust-mode)\n                  :server-id 'dslx-ls))\n(add-hook 'rust-mode-hook 'lsp)\n</code></pre> <p>Additional details on Emacs enablement may be available in the Verible editor hook-up documentation.</p>"},{"location":"dslx_language_server/#sublime-text","title":"Sublime Text","text":""},{"location":"dslx_language_server/#create-a-dslx-syntax","title":"Create a DSLX syntax","text":"<p>Go to <code>Tools &gt; Developer &gt; New Syntax</code>, then replace values in the template:</p> <ul> <li>Change the <code>file_extensions</code> value to be <code>x</code></li> <li>Change scope to be <code>source.dslx</code></li> <li>Replace all <code>example-c</code> in the file with <code>dslx</code></li> </ul> <p>Now when you open a <code>.x</code> file it should show the syntax in the lower right-hand corner as <code>dslx</code>.</p>"},{"location":"dslx_language_server/#install-lsp-package","title":"Install LSP package","text":"<p>Instructions for installing the LSP package are given in the Verible documentation.</p>"},{"location":"dslx_language_server/#add-dslx-to-lsp-settings","title":"Add DSLX to LSP settings","text":"<p>Open the configuration via <code>Preferences &gt; Package Settings &gt; LSP &gt; Settings</code> and add the following client:</p> <pre><code>{\n    \"clients\": {\n        \"dslx_ls\": {\n          \"command\": [\"dslx_ls\"],\n          \"enabled\": true,\n          \"selector\": \"source.dslx\"\n        }\n    }\n}\n</code></pre> <p>Now open <code>foo.x</code> and paste in:</p> <p><code>fn main() -&gt; u8 { u8:256 }</code></p> <p>There should be a red squiggle under the number <code>256</code> indicating that the value is out of range for a <code>u8</code>.</p>"},{"location":"dslx_language_server/#other-editors","title":"Other Editors","text":"<p>For non-Vim editors, see the instructions provided by our sister project, Verible.</p>"},{"location":"dslx_reference/","title":"DSLX Reference","text":""},{"location":"dslx_reference/#overview","title":"Overview","text":"<p>DSLX is a domain specific, dataflow-oriented functional language used to build hardware that can also run effectively as host software. Within the XLS project, DSLX is also referred to as \"the DSL\". The DSL targets the XLS compiler (via conversion to XLS IR) to enable flows for FPGAs and ASICs.</p> <p>DSLX mimics Rust, while being an immutable expression-based dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph, etc. To avoid arbitrary new syntax/semantics choices, the DSL mimics Rust where it is reasonably possible; for example, integer conversions all follow the same semantics as Rust.</p> <p>Note: There are some unnecessary differences today from Rust syntax due to early experimentation, but they are quickly being removed to converge on Rust syntax.</p> <p>Note that other frontends to XLS core functionality will become available in the future; e.g. xlscc, for users familiar with the C++-and-pragma style of HLS computation. XLS team develops the DSL as part of the XLS project because we believe it can offer significant advantages over the C++-with-pragmas approach.</p> <p>Dataflow DSLs are a good fit for describing hardware, compared to languages whose design assumes von Neumann style computation (global mutable state, sequential mutation by a sequential thread of control). Using a Domain Specific Language (DSL) provides a more hardware-oriented representation of a given computation that matches XLS compiler (IR) constructs closely. The DSL also allows an exploration of HLS without being encumbered by C++ language or compiler limitations such as non-portable pragmas, magic macros, or semantically important syntactic conventions. The language is still experimental and likely to change, but it is already useful for experimentation and exploration.</p> <p>This document provides a reference for DSLX, mostly by example. Before perusing it in detail, we recommend you first read the DSLX tutorials to understand the broad strokes of the language.</p> <p>In this document we use the function to compute a CRC32 checksum to describe language features. The full code is in <code>examples/dslx_intro/crc32_one_byte.x</code>.</p>"},{"location":"dslx_reference/#comments","title":"Comments","text":"<p>Just as in languages like Rust/C++, comments start with <code>//</code> and last through the end of the line.</p>"},{"location":"dslx_reference/#identifiers","title":"Identifiers","text":"<p>All identifiers, eg., for function names, parameters, and values, follow the typical naming rules of other languages. The identifiers can start with a character or an underscore, and can then contain more characters, underscores, or numbers. Valid examples are:</p> <pre><code>a                 // valid\nCamelCase         // valid\nlike_under_scores // valid\n__also_ok         // valid\n_Ok123_321        // valid\n_                 // valid\n\n2ab               // not valid\n&amp;ade              // not valid\n</code></pre> <p>However, we suggest the following DSLX style rules, which mirror the Rust naming conventions.</p> <ul> <li>Functions are <code>written_like_this</code></li> </ul> <ul> <li>User-defined data types are <code>NamesLikeThis</code></li> </ul> <ul> <li>Constant bindings are <code>NAMES_LIKE_THIS</code></li> </ul> <ul> <li><code>_</code> is the \"black hole\" identifier -- a name that you can bind to but should     never read from, akin to Rust's wildcard pattern match or Python's \"unused     identifier\" convention. It should never be referred to in an expression     except as a \"sink\".</li> </ul> <p>NOTE Since mutable locals are not supported, there is also support for \"tick identifiers\", where a ' character may appear anywhere after the first character of an identifier to indicate \"prime\"; e.g. <code>let state' = update(state);</code>. By convention ticks usually come at the end of an identifier. Since this is not part of Rust's syntax, it is considered experimental at this time.</p>"},{"location":"dslx_reference/#unused-bindings","title":"Unused Bindings","text":"<p>If you bind a name and do not use it, a warning will be flagged, and warnings are errors by default; e.g. this will flag an unused warning:</p> <pre><code>#[test]\nfn my_test() {\n    let x = u32:42;  // Not used!\n}\n</code></pre> <p>For cases where it's more readable to keep a name, even though it's unused, you can prefix the name with a leading underscore, like so:</p> <pre><code>#[test]\nfn my_test() {\n    let (thing_one, _thing_two) = open_crate();\n    assert_eq(thing_one, u32:1);\n}\n</code></pre> <p>Note that <code>_thing_two</code> is unused, but a warning is not flagged because we indicated via a leading underscore that it was ok for the variable to go unused, because we felt it enhanced readability.</p>"},{"location":"dslx_reference/#functions","title":"Functions","text":"<p>Function definitions begin with the keyword <code>fn</code>, followed by the function name, a parameter list to the function in parenthesis, followed by an <code>-&gt;</code> and the return type of the function. After this, curly braces denote the begin and end of the function body.</p> <p>The list of parameters can be empty.</p> <p>A single input file can contain many functions.</p> <p>Simple examples:</p> <pre><code>fn ret3() -&gt; u32 {\n   u32:3   // This function always returns 3.\n}\n\nfn add1(x: u32) -&gt; u32 {\n   x + u32:1  // Returns x + 1, but you knew that!\n}\n</code></pre> <p>Functions return the result of their last computed expression as their return value. There are no explicit return statements. By implication, functions return exactly one expression; they can't return multiple expressions (but this may change in the future as we migrate towards some Rust semantics).</p> <p>Tuples should be returned if a function needs to return multiple values.</p>"},{"location":"dslx_reference/#parameters","title":"Parameters","text":"<p>Parameters are written as pairs <code>name</code> followed by a colon <code>:</code> followed by the <code>type</code> of that parameter. Each parameter needs to declare its own type.</p> <p>Examples:</p> <pre><code>// a simple parameter x of type u32\n   x: u32\n\n// t is a tuple with 2 elements.\n//   the 1st element is of type u32\n//   the 2nd element is a tuple with 3 elements\n//       the 1st element is of type u8\n//       the 2nd element is another tuple with 1 element of type u16\n//       the 3rd element is of type u8\n   t: (u32, (u8, (u16,), u8))\n</code></pre>"},{"location":"dslx_reference/#parametric-functions","title":"Parametric Functions","text":"<p>DSLX functions can be parameterized in terms of the types of its arguments and in terms of types derived from other parametric values. For instance:</p> <pre><code>fn double(n: u32) -&gt; u32 {\n  n * u32:2\n}\n\nfn self_append&lt;A: u32, B: u32 = {double(A)}&gt;(x: bits[A]) -&gt; bits[B] {\n  x++x\n}\n\nfn main() -&gt; bits[10] {\n  self_append(u5:1)\n}\n</code></pre> <p>In <code>self_append(bits[5]:1)</code>, we see that <code>A = 5</code> based off of formal argument instantiation. Using that value, we can evaluate <code>B = double(A=5)</code>. This derived expression is analogous to C++'s constexpr \u2013 a simple expression that can be evaluated at that point in compilation. Note that the expression must be wrapped in <code>{}</code> curly braces.</p> <p>See advanced understanding for more information on parametricity.</p>"},{"location":"dslx_reference/#explicit-parametric-instantiation","title":"Explicit parametric instantiation","text":"<p>In some cases, parametric values cannot be inferred from function arguments, such as in the <code>explicit_parametric_simple.x</code> test:</p> <pre><code>fn add_one&lt;E:u32, F:u32, G:u32 = E+F&gt;(lhs: bits[E]) -&gt; bits[G] { ... }\n</code></pre> <p>For this call to instantiable, both <code>E</code> and <code>F</code> must be specified. Since <code>F</code> can't be inferred from an argument, we must rely on explicit parametrics:</p> <pre><code>  add_one&lt;u32:1, {u32:2 + u32:3}&gt;(u1:1);\n</code></pre> <p>This invocation will bind <code>1</code> to <code>E</code>, <code>5</code> to <code>F</code>, and <code>6</code> to <code>G</code>. Note the curly braces around the expression-defined parametric: simple literals and constant references do not need braces (but they can have them), but any other expression requires them.</p>"},{"location":"dslx_reference/#expression-ambiguity","title":"Expression ambiguity","text":"<p>Without curly braces, explicit parametric expressions could be ambiguous; consider the following, slightly changed from the previous example:</p> <pre><code>  add_one&lt;u32:1, u32:2&gt;(u32:3)&gt;(u1:1);\n</code></pre> <p>Is the statement above computing <code>add_one&lt;1, (2 &gt; 3)&gt;(1)</code>, or is it computing <code>(add_one&lt;1, 2&gt;(3)) &gt; 1)</code>? Without additional (and subtle and perhaps surprising) contextual precedence rules, this would be ambiguous and could lead to a parse error or, even worse, unexpected behavior.</p> <p>Fortunately, we can look to Rust for inspiration. Rust's const generics RPF introduced the <code>{ }</code> syntax for disambiguating just this case in generic specifications. With this, any expressions present in a parametric specification must be contained within curly braces, as in the original example.</p> <p>At present, if the braces are omitted, some unpredictable error will occur. Work to improve this is tracked in XLS GitHub issue #321.</p>"},{"location":"dslx_reference/#function-calls","title":"Function Calls","text":"<p>Function calls are expressions and look and feel just like one would expect from other languages. For example:</p> <pre><code>fn callee(x: bits[32], y: bits[32]) -&gt; bits[32] {\n  x + y\n}\nfn caller() -&gt; u32 {\n  callee(u32:2, u32:3)\n}\n</code></pre> <p>If more than one value should be returned by a function, a tuple type should be returned.</p>"},{"location":"dslx_reference/#built-in-functions-and-standard-library","title":"Built-In Functions and Standard Library","text":"<p>The DSL has several built-in functions and standard library modules. For details on the available functions to invoke, see DSLX Built-In Functions and Standard Library.</p>"},{"location":"dslx_reference/#types","title":"Types","text":""},{"location":"dslx_reference/#bit-type","title":"Bit Type","text":"<p>The most fundamental type in DSLX is a variable length bit type denoted as <code>bits[n]</code>, where <code>n</code> is a constant. For example:</p> <pre><code>bits[1]   // a single bit\nuN[1]     // explicitly noting single bit is unsigned\nu1        // convenient shorthand for bits[1]\n\nbits[8]   // an 8-bit datatype, yes, a byte\nu8        // convenient shorthand for bits[8]\nbits[32]  // a 32-bit datatype\nu32       // convenient shorthand for bits[32]\nbits[256] // a 256-bit datatype\n\nbits[0]   // possible, but, don't do that\n</code></pre> <p>DSLX introduces aliases for commonly used types, such as <code>u8</code> for an 8-wide bit type, or <code>u32</code> for a 32-bit wide bit type. These are defined up to <code>u64</code>.</p> <p>All <code>u*</code>, <code>uN[*]</code>, and <code>bits[*]</code> types are interpreted as unsigned integers. Signed integers are specified via <code>s*</code> and <code>sN[*]</code>. Similarly to unsigned numbers, the <code>s*</code> shorthands are defined up to <code>s64</code>. For example:</p> <pre><code>sN[1]\ns1\n\nsN[64]\ns64\n\nsN[256]\n\nsN[0]\ns0\n</code></pre> <p>Signed numbers differ in their behavior from unsigned numbers primarily via operations like comparisons, (variable width) multiplications, and divisions.</p>"},{"location":"dslx_reference/#bit-type-attributes","title":"Bit Type Attributes","text":"<p>Bit types have helpful type-level attributes that provide limit values, similar to <code>std::numeric_limits</code> in C++. For example:</p> <pre><code>u3::MAX   // u3:0b111 the \"fill with ones\" value\ns3::MAX   // s3:0b011 the \"maximum signed\" value\n\nu3::ZERO  // u3:0b000 the \"fill with zeros\" value\ns3::ZERO  // s3:0b000 the \"fill with zeros\" value\n</code></pre>"},{"location":"dslx_reference/#character-constants","title":"Character Constants","text":"<p>Characters are a special case of bits types: they are implicitly-type as u8. Characters can be used just as traditional bits:</p> <pre><code>fn add_to_null(input: u8) -&gt; u8 {\n  let null:u8 = '\\0';\n  input + null\n}\n\n#[test]\nfn test_main() {\n  assert_eq('a', add_to_null('a'))\n}\n</code></pre> <p>DSLX character constants support the full Rust set of escape sequences with the exception of unicode.</p>"},{"location":"dslx_reference/#enum-types","title":"Enum Types","text":"<p>DSLX supports enumerations as a way of defining a group of related, scoped, named constants that do not pollute the module namespace. For example:</p> <pre><code>enum Opcode : u3 {\n  FIRE_THE_MISSILES = 0,\n  BE_TIRED = 1,\n  TAKE_A_NAP = 2,\n}\n\nfn get_my_favorite_opcode() -&gt; Opcode {\n  Opcode::FIRE_THE_MISSILES\n}\n</code></pre> <p>Note the use of the double-colon to reference the enum value. This code specifies that the enum behaves like a <code>u3</code>: its storage and extension (via casting) behavior are defined to be those of a <code>u3</code>. Attempts to define an enum value outside of the representable <code>u3</code> range will produce a compile time error.</p> <pre><code>enum Opcode : u3 {\n  FOO = 8  // Causes compile time error!\n}\n</code></pre> <p>Enums can be compared for equality/inequality, but they do not permit arithmetic operations, they must be cast to numerical types in order to perform arithmetic:</p> <pre><code>enum Opcode: u3 {\n  NOP = 0,\n  ADD = 1,\n  SUB = 2,\n  MUL = 3,\n}\n\nfn same_opcode(x: Opcode, y: Opcode) -&gt; bool {\n  x == y  // ok\n}\n\nfn next_in_sequence(x: Opcode, y: Opcode) -&gt; bool {\n  // x+1 == y // does not work, arithmetic!\n  x as u3 + u3:1 == (y as u3)  // ok, casted first\n}\n</code></pre> <p>As mentioned above, casting of enum-values works with the same casting/extension rules that apply to the underlying enum type definition. For example, this cast will sign extend because the source type for the enum is signed. (See numerical conversions for the full description of extension/truncation behavior.)</p> <pre><code>enum MySignedEnum : s3 {\n  LOW = -1,\n  ZERO = 0,\n  HIGH = 1,\n}\n\nfn extend_to_32b(x: MySignedEnum) -&gt; u32 {\n  x as u32  // Sign-extends because the source type is signed.\n}\n\n#[test]\nfn test_extend_to_32b() {\n  assert_eq(extend_to_32b(MySignedEnum::LOW), u32:0xffffffff)\n}\n</code></pre> <p>Casting to an enum is also permitted. However, in most cases errors from invalid casting can only be found at runtime, e.g., in the DSL interpreter or flagging a fatal error from hardware. Because of that, it is recommended to avoid such casts as much as possible.</p>"},{"location":"dslx_reference/#tuple-type","title":"Tuple Type","text":"<p>A tuple is a fixed-size ordered set, containing elements of heterogeneous types. Tuples elements can be any type, e.g. bits, arrays, structs, tuples. Tuples may be empty (an empty tuple is also known as the unit type), or contain one or more types.</p> <p>Examples of tuple values:</p> <pre><code>// The unit type, carries no information.\nlet unit = ();\n\n// A tuple containing two bits-typed elements.\nlet pair = (u3:0b100, u4:0b1101);\n</code></pre> <p>Example of a tuple type:</p> <pre><code>// The type of a tuple with 2 elements.\n//   the 1st element is of type u32\n//   the 2nd element is a tuple with 3 elements\n//       the 1st element is of type u8\n//       the 2nd element is another tuple with 1 element of type u16\n//       the 3rd element is of type u8\ntype MyTuple = (u32, (u8, (u16,), u8));\n</code></pre> <p>To access individual tuple elements use simple indices, starting at 0. For example, to access the second element of a tuple (index 1):</p> <pre><code>#[test]\nfn test_tuple_access() {\n  let t = (u32:2, u8:3);\n  assert_eq(u8:3, t.1)\n}\n</code></pre> <p>Such indices can only be numeric literals; parametric symbols are not allowed.</p> <p>Tuples can be \"destructured\", similarly to how pattern matching works in <code>match</code> expressions, which provides a convenient syntax to name elements of a tuple for subsequent use. See <code>a</code> and <code>b</code> in the following:</p> <pre><code>#[test]\nfn test_tuple_destructure() {\n  let t = (u32:2, u8:3);\n  let (a, b) = t;\n  assert_eq(u32:2, a);\n  assert_eq(u8:3, b)\n}\n</code></pre> <p>Just as values can be discarded in a <code>let</code> by using the \"black hole identifier\" <code>_</code>, don't-care values can also be discarded when destructuring a tuple:</p> <pre><code>#[test]\nfn test_black_hole() {\n  let t = (u32:2, u8:3, true);\n  let (_, _, v) = t;\n  assert_eq(v, true)\n}\n</code></pre>"},{"location":"dslx_reference/#struct-types","title":"Struct Types","text":"<p>Structures are similar to tuples, but provide two additional capabilities: we name the slots (i.e. struct fields have names while tuple elements only have positions), and we introduce a new type.</p> <p>The following syntax is used to define a struct:</p> <pre><code>struct Point {\n  x: u32,\n  y: u32\n}\n</code></pre> <p>Once a struct is defined it can be constructed by naming the fields in any order:</p> <pre><code>struct Point {\n  x: u32,\n  y: u32,\n}\n\n#[test]\nfn test_struct_equality() {\n  let p0 = Point { x: u32:42, y: u32:64 };\n  let p1 = Point { y: u32:64, x: u32:42 };\n  assert_eq(p0, p1)\n}\n</code></pre> <p>There is a simple syntax when creating a struct whose field names match the names of in-scope values:</p> <pre><code>struct Point { x: u32, y: u32, }\n\n#[test]\nfn test_struct_equality() {\n  let x = u32:42;\n  let y = u32:64;\n  let p0 = Point { x, y };\n  let p1 = Point { y, x };\n  assert_eq(p0, p1)\n}\n</code></pre> <p>Struct fields can also be accessed with \"dot\" syntax:</p> <pre><code>struct Point {\n  x: u32,\n  y: u32,\n}\n\nfn f(p: Point) -&gt; u32 {\n  p.x + p.y\n}\n\nfn main() -&gt; u32 {\n  f(Point { x: u32:42, y: u32:64 })\n}\n\n#[test]\nfn test_main() {\n  assert_eq(u32:106, main())\n}\n</code></pre> <p>Note that structs cannot be mutated \"in place\", the user must construct new values by extracting the fields of the original struct mixed together with new field values, as in the following:</p> <pre><code>struct Point3 {\n  x: u32,\n  y: u32,\n  z: u32,\n}\n\nfn update_y(p: Point3, new_y: u32) -&gt; Point3 {\n  Point3 { x: p.x, y: new_y, z: p.z }\n}\n\nfn main() -&gt; Point3 {\n  let p = Point3 { x: u32:42, y: u32:64, z: u32:256 };\n  update_y(p, u32:128)\n}\n\n#[test]\nfn test_main() {\n  let want = Point3 { x: u32:42, y: u32:128, z: u32:256 };\n  assert_eq(want, main())\n}\n</code></pre>"},{"location":"dslx_reference/#struct-update-syntax","title":"Struct Update Syntax","text":"<p>The DSL has syntax for conveniently producing a new value with a subset of fields updated to reduce verbosity. The \"struct update\" syntax is:</p> <pre><code>struct Point3 {\n  x: u32,\n  y: u32,\n  z: u32,\n}\n\nfn update_y(p: Point3) -&gt; Point3 {\n  Point3 { y: u32:42, ..p }\n}\n\nfn update_x_and_y(p: Point3) -&gt; Point3 {\n  Point3 { x: u32:42, y: u32:42, ..p }\n}\n</code></pre>"},{"location":"dslx_reference/#parametric-structs","title":"Parametric Structs","text":"<p>DSLX also supports parametric structs. For more information on how type-parametricity works, see the parametric functions section.</p> <pre><code>fn double(n: u32) -&gt; u32 { n * u32:2 }\n\nstruct Point&lt;N: u32, M: u32 = {double(N)}&gt; {\n  x: bits[N],\n  y: bits[M],\n}\n\nfn make_point&lt;A: u32, B: u32&gt;(x: bits[A], y: bits[B]) -&gt; Point&lt;A, B&gt; {\n  Point{ x, y }\n}\n\n#[test]\nfn test_struct_construction() {\n  let p = make_point(u16:42, u32:42);\n  assert_eq(u16:42, p.x)\n}\n</code></pre>"},{"location":"dslx_reference/#understanding-nominal-typing","title":"Understanding Nominal Typing","text":"<p>As mentioned above, a struct definition introduces a new type. Structs are nominally typed, as opposed to structurally typed (note that tuples are structurally typed). This means that structs with different names have different types, regardless of whether those structs have the same structure (i.e. even when all the fields of two structures are identical, those structures are a different type when they have a different name).</p> <pre><code>struct Point {\n  x: u32,\n  y: u32,\n}\n\nstruct Coordinate {\n  x: u32,\n  y: u32,\n}\n\nfn f(p: Point) -&gt; u32 {\n  p.x + p.y\n}\n\n#[test]\nfn test_ok() {\n  assert_eq(f(Point { x: u32:42, y: u32:64 }), u32:106)\n}\n</code></pre> <pre><code>#[test]\nfn test_type_checker_error() {\n  assert_eq(f(Coordinate { x: u32:42, y: u32:64 }), u32:106)\n}\n</code></pre>"},{"location":"dslx_reference/#array-type","title":"Array Type","text":"<p>Arrays can be constructed via bracket notation. All values that make up the array must have the same type. Arrays can be indexed with indexing notation (<code>a[i]</code>) to retrieve a single element.</p> <pre><code>fn main(a: u32[2], i: u1) -&gt; u32 {\n  a[i]\n}\n\n#[test]\nfn test_main() {\n  let x = u32:42;\n  let y = u32:64;\n  // Make an array with \"bracket notation\".\n  let my_array: u32[2] = [x, y];\n  assert_eq(main(my_array, u1:0), x);\n  assert_eq(main(my_array, u1:1), y);\n}\n</code></pre> <p>Because arrays with repeated trailing elements are common, the DSL supports ellipsis (<code>...</code>) at the end of an array to fill the remainder of the array with the last noted element. Because the compiler must know how many elements to fill, in order to use the ellipsis the type must be annotated explicitly as shown.</p> <pre><code>fn make_array(x: u32) -&gt; u32[3] {\n  u32[3]:[u32:42, x, ...]\n}\n\n#[test]\nfn test_make_array() {\n  assert_eq(u32[3]:[u32:42, u32:42, u32:42], make_array(u32:42));\n  assert_eq(u32[3]:[u32:42, u32:64, u32:64], make_array(u32:64));\n}\n</code></pre> <p>Note google/xls#917: arrays with length zero will typecheck, but fail to work in most circumstances. Eventually, XLS should support them but they can't be used currently.</p> <p>TODO(meheff): Explain arrays and the intricacies of our bits type interpretation and how it affects arrays of bits etc.</p>"},{"location":"dslx_reference/#character-string-constants","title":"Character String Constants","text":"<p>Character strings are a special case of array types, being implicitly-sized arrays of u8 elements. String constants can be used just as traditional arrays:</p> <pre><code>fn add_one&lt;N: u32&gt;(input: u8[N]) -&gt; u8[N] {\n  for (i, result) : (u32, u8[N]) in u32:0..N {\n    update(result, i, result[i] + u8:1)\n  }(input)\n}\n\n#[test]\nfn test_main() {\n  assert_eq(\"bcdef\", add_one(\"abcde\"))\n}\n</code></pre> <p>DSLX string constants support the full Rust set of escape sequences - note that unicode escapes get encoded to their UTF-8 byte sequence. In other words, the sequence <code>\\u{10CB2F}</code> will result in an array with hexadecimal values <code>F4 8C AC AF</code>.</p> <p>Moreover, string can be composed of characters.</p> <pre><code>fn string_composed_characters() -&gt; u8[10] {\n  ['X', 'L', 'S', ' ', 'r', 'o', 'c', 'k', 's', '!']\n}\n\n#[test]\nfn test_main() {\n  assert_eq(\"XLS rocks!\", string_composed_characters())\n}\n</code></pre>"},{"location":"dslx_reference/#type-aliases","title":"Type Aliases","text":"<p>DLSX supports the definition of type aliases.</p> <p>Type aliases can be used to provide a more human-readable name for an existing type. The new name is on the left, the existing name on the right:</p> <pre><code>type Weight = u6;\n</code></pre> <p>We can create an alias for an imported type:</p> <pre><code>// Note: this imports an external file in the codebase.\nimport xls.dslx.tests.mod_imported;\n\ntype MyEnum = mod_imported::MyEnum;\n\nfn main(x: u8) -&gt; MyEnum {\n  x as MyEnum\n}\n\n#[test]\nfn test_main() {\n  assert_eq(main(u8:42), MyEnum::FOO);\n  assert_eq(main(u8:64), MyEnum::BAR);\n}\n</code></pre> <p>Type aliases can also provide a descriptive name for a tuple type (which is otherwise anonymous). For example, to define a tuple type that represents a float number with a sign bit, an 8-bit mantissa, and a 23-bit mantissa, one would write:</p> <pre><code>type F32 = (u1, u8, u23);\n</code></pre> <p>After this definition, the <code>F32</code> may be used as a type annotation interchangeably with <code>(u1, u8, u23)</code>.</p> <p>Note, however, that structs are generally preferred for this purpose, as they are more readable and users do not need to rely on tuple elements having a stable order in the future (i.e., they are resilient to refactoring).</p>"},{"location":"dslx_reference/#type-casting","title":"Type Casting","text":"<p>Bit types can be cast from one bit-width to another with the <code>as</code> keyword. Types can be widened (increasing bit-width), narrowed (decreasing bit-width) and/or changed between signed and unsigned. Some examples are found below. See Numerical Conversions for a description of the semantics.</p> <pre><code>#[test]\nfn test_narrow_cast() {\n  let twelve = u4:0b1100;\n  assert_eq(twelve as u2, u2:0)\n}\n\n#[test]\nfn test_widen_cast() {\n  let three = u2:0b11;\n  assert_eq(three as u4, u4:3)\n}\n\n#[test]\nfn test_narrow_signed_cast() {\n  let negative_seven = s4:0b1001;\n  assert_eq(negative_seven as u2, u2:1)\n}\n\n#[test]\nfn test_widen_signed_cast() {\n  let negative_one = s2:0b11;\n  assert_eq(negative_one as s4, s4:-1)\n}\n\n#[test]\nfn test_widen_to_unsigned() {\n  let negative_one = s2:0b11;\n  assert_eq(negative_one as u3, u3:0b111)\n}\n\n#[test]\nfn test_widen_to_signed() {\n  let three = u2:0b11;\n  assert_eq(three as u3, u3:0b011)\n}\n</code></pre>"},{"location":"dslx_reference/#type-checking-and-inference","title":"Type Checking and Inference","text":"<p>DSLX performs type checking and produces an error if types in an expression don't match up.</p> <p><code>let</code> expressions also perform type inference, which is quite convenient. For example, instead of writing:</p> <pre><code>let ch: u32 = (e &amp; f) ^ ((!e) &amp; g);\nlet (h, g, f): (u32, u32, u32) = (g, f, e);\n</code></pre> <p>one can write the following, as long as the types can be properly inferred:</p> <pre><code>let ch = (e &amp; f) ^ ((!e) &amp; g);\nlet (h, g, f) = (g, f, e);\n</code></pre> <p>Note that type annotations can still be added and be used for program understanding, as they they will be checked by DSLX.</p>"},{"location":"dslx_reference/#type-inference-details","title":"Type Inference Details","text":""},{"location":"dslx_reference/#type-inference-background","title":"Type Inference Background","text":"<p>All expressions in the language's expression grammar have a deductive type inference rule. The types must be known for inputs to an operator/function<sup>1</sup> and every expression has a way to determine its type from its operand expressions.</p> <p>DSLX uses deductive type inference to check the types present in the program. Deductive type inference is a set of (typically straight-forward) deduction rules: Hindley-Milner style deductive type inference determines the result type of a function with a rule that only observes the input types to that function. (Note that operators like '+' are just slightly special functions in that they have pre-defined special-syntax-rule names.)</p>"},{"location":"dslx_reference/#bindings-and-environment","title":"Bindings and Environment","text":"<p>In DSLX code, the \"environment\" where names are bound (sometimes also referred to as a symbol table) is called the <code>Bindings</code> -- it maps identifiers to the AST node that defines the name (<code>{string: AstNode}</code>), which can be combined with a mapping from AST node to its deduced type (<code>{AstNode: ConcreteType}</code>) to resolve the type of an identifier in the program. <code>Let</code> is one of the key nodes that populates these <code>Bindings</code>, but anything that creates a bound name does as well (e.g. parameters, for loop induction variables, etc.).</p>"},{"location":"dslx_reference/#operator-example","title":"Operator Example","text":"<p>For example, consider the binary (meaning takes two operands) / infix (meaning it syntactically is placed in the center of its operands) '+' operator. The simple deductive type inference rule for '+' is:</p> <p><code>(T, T) -&gt; T</code></p> <p>Meaning that the left hand side operand to the '+' operator is of some type (call it T), the right hand side operand to the '+' operator must be of that same type, T, and the result of that operator is then (deduced) to be of the same type as its operands, T.</p> <p>Let's instantiate this rule in a function:</p> <pre><code>fn add_wrapper(x: bits[2], y: bits[2]) -&gt; bits[2] {\n  x + y\n}\n</code></pre> <p>This function wraps the '+' operator. It presents two arguments to the '+' operator and then checks that the annotated return type on <code>add_wrapper</code> matches the deduced type for the body of that function; that is, we ask the following question of the '+' operator (since the type of the operands must be known at the point the add is performed):</p> <p><code>(bits[2], bits[2]) -&gt; ?</code></p> <p>To resolve the '?' the following procedure is being used:</p> <ul> <li>Pattern match the rule given above <code>(T, T) -&gt; T</code> to determine the type T:     the left hand side operand is <code>bits[2]</code>, called T.</li> <li>Check that the right hand side is also that same T, which it is: another     <code>bits[2]</code>.</li> <li>Deduce that the result type is that same type T: <code>bits[2]</code>.</li> <li>That becomes the return type of the body of the function. Check that it is     the same type as the annotated return type for the function, and it is!</li> </ul> <p>The function is annotated to return <code>bits[2]</code>, and the deduced type of the body is also <code>bits[2]</code>. Qed.</p>"},{"location":"dslx_reference/#type-errors","title":"Type errors","text":"<p>A type error would occur in the following:</p> <pre><code>fn add_wrapper(x: bits[2], y: bits[3]) -&gt; bits[2] {\n  x + y\n}\n</code></pre> <p>Applying the type deduction rule for '+' finds an inconsistency. The left hand side operand has type <code>bits[2]</code>, called T, but the right hand side is <code>bits[3]</code>, which is not the same as T. Because the deductive type inference rule does not say what to do when the operand types are different, it results in a type error which is flagged at this point in the program.</p>"},{"location":"dslx_reference/#let-bindings-names-and-the-environment","title":"Let Bindings, Names, and the Environment","text":"<p>In the DSL, <code>let</code> is an expression. It may not seem obvious at a glance, but it is! As a primer see the type inference background and how names are resolved in an environment.</p> <p>\"let\" expressions are of the (Rust-inspired) form:</p> <p><code>let $name: $annotated_type = $expr; $subexpr</code></p> <p><code>$name</code> gets \"bound\" to a value of type <code>$annotated_type</code>. The <code>let</code> typecheck rule must both check that <code>$expr</code> is of type <code>$annotated_type</code>, as well as determine the type of <code>$subexpr</code>, which is the type of the overall \"let expression\".</p> <p>In this example, the result of the <code>let</code> expression is the return value -- <code>$subexpr</code> (<code>x+x</code>) can use the <code>$name</code> (<code>x</code>) which was \"bound\":</p> <pre><code>fn main(y: u32) -&gt; u64 {\n  let x: u64 = y as u64;\n  x+x\n}\n</code></pre> <p>If we invoke <code>main(u32:2)</code> we will the evaluate <code>let</code> expression -- it creates a binding of <code>x</code> to the value <code>u64:2</code>, and then evaluates the expression <code>x+x</code> in that environment, so the result of the <code>let</code> expression's <code>$subexpr</code> is <code>u64:4</code>.</p>"},{"location":"dslx_reference/#statements","title":"Statements","text":""},{"location":"dslx_reference/#imports","title":"Imports","text":"<p>DSLX modules can import other modules via the <code>import</code> keyword. Circular imports are not permitted (the dependencies among DSLX modules must form a DAG, as in languages like Go).</p> <p>The import statement takes the following form (note the lack of semicolon):</p> <pre><code>import path.to.my.imported_module;\n</code></pre> <p>With that statement, the module will be accessible as (the trailing identifier after the last dot) <code>imported_module</code>; e.g. the program can refer to <code>imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT</code>.</p> <p>NOTE Imports are relative to the Bazel \"depot root\" -- for external use of the tools a <code>DSLX_PATH</code> will be exposed, akin to a <code>PYTHONPATH</code>, for users to indicate paths where were should attempt module discovery.</p> <p>NOTE Importing does not introduce any names into the current file other than the one referred to by the import statement. That is, if <code>imported_module</code> had a constant defined in it <code>FOO</code>, this is referred to via <code>imported_module::FOO</code>, <code>FOO</code> does not \"magically\" get put in the current scope. This is analogous to how wildcard imports are discouraged in other languages (e.g. <code>from import *</code> in Python) on account of leading to \"namespace pollution\" and needing to specify what happens when names conflict.</p> <p>If you want to change the name of the imported module (for reference inside of the importing file) you can use the <code>as</code> keyword:</p> <pre><code>import path.to.my.imported_module as im;\n</code></pre> <p>Just using the above construct, <code>imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT</code> is not valid, only <code>im::IMPORTED_MODULE_PUBLIC_CONSTANT</code>. However, both statements can be used on different lines:</p> <pre><code>import path.to.my.imported_module;\nimport path.to.my.imported_module as im;\n</code></pre> <p>In this case, either <code>im::IMPORTED_MODULE_PUBLIC_CONSTANT</code> or <code>imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT</code> can be used to refer to the same thing.</p> <p>Here is an example using the same function via two different aliases for the same module:</p> <pre><code>// Note: this imports an external file in the codebase under two different\n// names.\nimport xls.dslx.tests.mod_imported;\nimport xls.dslx.tests.mod_imported as mi;\n\nfn main(x: u3) -&gt; u1 {\n  mod_imported::my_lsb(x) || mi::my_lsb(x)\n}\n\n#[test]\nfn test_main() {\n  assert_eq(u1:0b1, main(u3:0b001))\n}\n</code></pre>"},{"location":"dslx_reference/#public-module-members","title":"Public module members","text":"<p>Module members are private by default and not accessible from any importing module. To make a member public/visible to importing modules, the <code>pub</code> keyword must be added as a prefix; e.g.</p> <pre><code>const FOO = u32:42;      // Not accessible to importing modules.\npub const BAR = u32:64;  // Accessible to importing modules.\n</code></pre> <p>This applies to other things defined at module scope as well: functions, enums, type aliases, etc.</p> <pre><code>import xls.dslx.tests.mod_imported;\nimport xls.dslx.tests.mod_imported as mi;\n\nfn main(x: u3) -&gt; u1 {\n  mod_imported::my_lsb(x) || mi::my_lsb(x)\n}\n\n#[test]\nfn test_main() {\n  assert_eq(u1:0b1, main(u3:0b001))\n}\n</code></pre>"},{"location":"dslx_reference/#module-attributes","title":"Module attributes","text":"<p>A limited number of attributes are able to be applied at module scope (currently just one), using the following syntax, which is conventionally placed at the top of the module (<code>.x</code> file):</p> <pre><code>#![allow(nonstandard_constant_naming)]\n\n// .. rest of the module ..\n</code></pre> <p>This disables the warning that is usually produced for non-standard constant names -- typically DSLX warns if they are not <code>SCREAMING_SNAKE_CASE</code> as per the Rust style guide. (This is useful for things like automatically generated files where perhaps we'd prefer not to rewrite names vs leaving them in some other, nonstandard, identifier form.)</p>"},{"location":"dslx_reference/#const","title":"Const","text":"<p>The <code>const</code> keyword is used to define module-level constant values. Named constants should be usable anywhere a literal value can be used:</p> <pre><code>const FOO = u8:42;\n\nfn match_const(x: u8) -&gt; u8 {\n  match x {\n    FOO =&gt; u8:0,\n    _ =&gt; u8:42,\n  }\n}\n\n#[test]\nfn test_match_const_not_binding() {\n  assert_eq(u8:42, match_const(u8:0));\n  assert_eq(u8:42, match_const(u8:1));\n  assert_eq(u8:0, match_const(u8:42));\n}\n\nfn h(t: (u8, (u16, u32))) -&gt; u32 {\n  match t {\n    (FOO, (x, y)) =&gt; (x as u32) + y,\n    (_, (y, u32:42)) =&gt; y as u32,\n    _ =&gt; u32:7,\n  }\n}\n\n#[test]\nfn test_match_nested() {\n  assert_eq(u32:3, h((u8:42, (u16:1, u32:2))));\n  assert_eq(u32:1, h((u8:0, (u16:1, u32:42))));\n  assert_eq(u32:7, h((u8:0, (u16:1, u32:0))));\n}\n</code></pre>"},{"location":"dslx_reference/#expressions","title":"Expressions","text":""},{"location":"dslx_reference/#literals","title":"Literals","text":"<p>DSLX supports construction of literals using the syntax <code>Type:Value</code>. For example <code>u16:1</code> is a 16-wide bit array with its least significant bit set to one. Similarly <code>s8:12</code> is an 8-wide bit array with its least significant four bits set to <code>1100</code>.</p> <p>DSLX supports initializing using binary, hex or decimal syntax. So</p> <pre><code>#[test]\nfn test_literal_initialization() {\n  assert_eq(u8:12, u8:0b00001100);\n  assert_eq(u8:12, u8:0x0c);\n}\n</code></pre> <p>When constructing literals DSLX will trigger an error if the constant will not fit in a bit array of the annotated sized, so for example trying to construct the literal <code>u8:256</code> will trigger an error of the form:</p> <p><code>TypeInferenceError: uN[8] Value '256' does not fit in the bitwidth of a uN[8] (8)</code></p> <p>But what about <code>s8:128</code> ? This is a valid literal, even though a signed 8-bit integer cannot represent it. The following code offers a clue.</p> <pre><code>#[test]\nfn test_signed_literal_initialization() {\n  assert_eq(s8:128, s8:-128);\n  assert_eq(s8:128, s8:0b10000000);\n}\n</code></pre> <p>What is happening here is that, 128 is being used as a bit pattern rather than as the number 128 to initialize the literal. It is only when the bit pattern cannot fit in the width of the literal that an error is triggered.</p> <p>Note that behaviour is different from Rust, where it will trigger an error, and the fact that DSLX considers this valid may change in the future.</p>"},{"location":"dslx_reference/#grouping-expression","title":"Grouping Expression","text":"<p>As in mathematical notation and many programming languages, an expression can be surrounded with an opening and closing parenthesis to make it the highest precedence (or simply for readability). For example, this expression evaluates to <code>u32:9</code>: <code>(u32:1 + u32:2) * u32:3</code>.</p>"},{"location":"dslx_reference/#unary-expressions","title":"Unary Expressions","text":"<p>DSLX supports two types of unary expressions with type signature <code>(xN[N]) -&gt; xN[N]</code>:</p> <ul> <li>bit-wise not (the <code>!</code> operator)</li> <li>negate (the <code>-</code> operator, computes the two's complement negation)</li> </ul>"},{"location":"dslx_reference/#binary-expressions","title":"Binary Expressions","text":"<p>DSLX supports a familiar set of binary expressions. There are two categories of binary expressions. A category where both operands to the expression must be of the same bit type (i.e., not arrays or tuples), and a category where the operands can be of arbitrary bit types (i.e. shift expressions).</p>"},{"location":"dslx_reference/#expressions-with-operands-of-the-same-type","title":"Expressions with operands of the same type.","text":"<p>The following expressions have type signature <code>(xN[N], xN[N]) -&gt; xN[N]</code>.</p> <ul> <li>bit-wise or (<code>|</code>)</li> <li>bit-wise and (<code>&amp;</code>)</li> <li>add (<code>+</code>)</li> <li>subtract (<code>-</code>)</li> <li>xor (<code>^</code>)</li> <li>multiply (<code>*</code>)</li> <li>logical or (<code>||</code>)</li> <li>logical and (<code>&amp;&amp;</code>)</li> </ul> <p>Things like <code>std::smul</code> are convenient helpers when you are working with mixed widths. Because these expressions return the same type as the operands, if you want a carry you need to widen the inputs (e.g. <code>std::uadd_with_overflow</code> ). The optimizer will narrow the operands and produce efficient hardware, especially with trivial zero-/sign-extended operands like <code>std::smul</code> and <code>std::uadd_with_overflow</code>.</p>"},{"location":"dslx_reference/#shift-expressions","title":"Shift Expressions","text":"<p>Shift expressions include:</p> <ul> <li>shift-right logical (<code>&gt;&gt;</code>)</li> <li>shift-left (<code>&lt;&lt;</code>)</li> </ul> <p>These are binary operations that don't require the same type on the left and right hand side. The right hand side must be unsigned, but it does not need to be the same type or width as the left hand side, i.e. the type signature for these operations is: <code>(xN[M], uN[N]) -&gt; xN[M]</code>. If the right hand side is a literal value it does not need to be type annotated. For example:</p> <pre><code>fn shr_two(x: s32) -&gt; s32 {\n  x &gt;&gt; 2\n}\n</code></pre> <p>Note that, as in Rust, the semantics of the shift-right (<code>&gt;&gt;</code>) operation depends on the signedness of the left hand side. For a signed-type left hand side, the shift-right (<code>&gt;&gt;</code>) operation performs a shift-right arithmetic and, for a unsigned-type left hand side, the shift-right (<code>&gt;&gt;</code>) operation performs a shift-right (logical).</p>"},{"location":"dslx_reference/#comparison-expressions","title":"Comparison Expressions","text":"<p>For comparison expressions the types of both operands must match. However these operations return a result of type <code>bits[1]</code>, aka <code>bool</code>.</p> <ul> <li>equal (<code>==</code>)</li> <li>not-equal (<code>!=</code>)</li> <li>greater-equal (<code>&gt;=</code>)</li> <li>greater (<code>&gt;</code>)</li> <li>less-equal (<code>&lt;=</code>)</li> <li>less (<code>&lt;</code>)</li> </ul>"},{"location":"dslx_reference/#concat-expression","title":"Concat Expression","text":"<p>Bitwise concatenation is performed with the <code>++</code> operator. The value on the left hand side becomes the most significant bits, the value on the right hand side becomes the least significant bits. Both of the operands must be unsigned (see numerical conversions for details on converting signed numbers to unsigned).</p> <p>Concatenation operations may be chained together as shown:</p> <pre><code>#[test]\nfn test_bits_concat() {\n  assert_eq(u8:0b11000000, u2:0b11 ++ u6:0b000000);\n  assert_eq(u8:0b00000111, u2:0b00 ++ u6:0b000111);\n  assert_eq(u6:0b100111, u1:1 ++ u2:0b00 ++ u3:0b111);\n  assert_eq(u6:0b001000, u1:0 ++ u2:0b01 ++ u3:0b000);\n  assert_eq(u32:0xdeadbeef, u16:0xdead ++ u16:0xbeef);\n}\n</code></pre>"},{"location":"dslx_reference/#block-expressions","title":"Block Expressions","text":"<p>Block expressions enable subordinate scopes to be defined, e.g.:</p> <pre><code>let a = {\n  let b = u32:1;\n  b + u32:3\n};\n</code></pre> <p>Above, <code>a</code> is equal to <code>4</code>.</p> <p>The value of a block expression is that of its last contained expression, or (), if a final expression is omitted:</p> <pre><code>let a = { let b = u32:1; };\n</code></pre> <p>In the above case, <code>a</code> is equal to <code>()</code>.</p> <p>Since DSLX does not currently have the concept of lifetimes, and since names can be rebound (i.e., this is valid: <code>let a = u32:0; let a = u32:1;</code>), blocks have the following uses:</p> <ul> <li>to syntactically form the body of functions and loops</li> <li>to limit the scope of variables</li> <li>to increase readability</li> </ul>"},{"location":"dslx_reference/#match-expression","title":"Match Expression","text":"<p>Match expressions permit \"pattern matching\" on data, like a souped-up switch statement. It can both test for values (like a conditional guard) and bind values to identifiers for subsequent use. For example:</p> <pre><code>fn f(t: (u8, u32)) -&gt; u32 {\n  match t {\n    (u8:42, y) =&gt; y,\n    (_, y) =&gt; y+u32:77\n  }\n}\n</code></pre> <p>If the first member of the tuple is the value is <code>42</code>, we pass the second tuple member back as-is from the function. Otherwise, we add <code>77</code> to the value and return that. The <code>_</code> symbolizes \"I don't care about this value\".</p> <p>Just like literal constants, pattern matching can also match via named constants; For example, consider this variation on the above:</p> <pre><code>const MY_FAVORITE_NUMBER = u8:42;\nfn f(t: (u8, u32)) -&gt; u32 {\n  match t {\n    (MY_FAVORITE_NUMBER, y) =&gt; y,\n    (_, y) =&gt; y+u32:77\n  }\n}\n</code></pre> <p>This also works with nested tuples; for example:</p> <pre><code>const MY_FAVORITE_NUMBER = u8:42;\nfn f(t: (u8, (u16, u32))) -&gt; u32 {\n  match t {\n    (MY_FAVORITE_NUMBER, (y, z)) =&gt; y as u32 + z,\n    (_, (y, u32:42)) =&gt; y as u32,\n    _ =&gt; u32:7\n  }\n}\n</code></pre> <p>Here we use a \"catch all\" wildcard pattern in the last match arm to ensure the match expression always matches the input somehow.</p> <p>We can also match on ranges of values using the range syntax:</p> <pre><code>fn f(x: u32) -&gt; u32 {\n  match x {\n    u32:1..u32:3 =&gt; u32:0,\n    _ =&gt; x\n  }\n}\n\n#[test]\nfn test_f() {\n  assert_eq(f(u32:1), u32:0);\n  assert_eq(f(u32:2), u32:0);\n  // Note: the limit of the range syntax is exclusive.\n  assert_eq(f(u32:3), u32:3);\n}\n</code></pre>"},{"location":"dslx_reference/#redundant-patterns","title":"Redundant Patterns","text":"<p><code>match</code> will flag an error if a syntactically identical pattern is typed twice; e.g.</p> <pre><code>const FOO = u32:42;\nfn f(x: u32) -&gt; u2 {\n  match x {\n    FOO =&gt; u2:0,\n    FOO =&gt; u2:1,  // Identical pattern!\n    _ =&gt; u2:2,\n  }\n}\n</code></pre> <p>Only the first pattern will ever match, so it is fully redundant (and therefore likely a user error they'd like to be informed of). Note that equivalent but not syntactically identical patterns will not be flagged in this way.</p> <pre><code>const FOO = u32:42;\nconst BAR = u32:42;  // Compares `==` to `FOO`.\nfn f(x: u32) -&gt; u2 {\n  match x {\n    FOO =&gt; u2:0,\n    BAR =&gt; u2:1,  // _Equivalent_ pattern, but not syntactically identical.\n    _ =&gt; u2:2,\n  }\n}\n</code></pre>"},{"location":"dslx_reference/#let-expression","title":"<code>let</code> Expression","text":"<p>let expressions work the same way as let expressions in other functional languages (such as the ML family languages). let expressions provide a nested, lexically-scoped, list of binding definitions. The scope of the binding is the expression on the right hand side of the declaration. For example:</p> <pre><code>let a: u32 = u32:1 + u32:2;\nlet b: u32 = a + u32:3;\nb\n</code></pre> <p>would bind (and return as a value) the value <code>6</code> which corresponds to <code>b</code> when evaluated. In effect there is little difference to other languages like C/C++ or Python, where the same result would be achieved with code similar to this:</p> <pre><code>a = 1 + 2\nb = a + 3\nreturn b\n</code></pre> <p>However, <code>let</code> expressions are lexically scoped. In above example, the value <code>3</code> is bound to <code>a</code> only during the combined let expression sequence. There is no other type of scoping in DSLX.</p>"},{"location":"dslx_reference/#if-expression","title":"If Expression","text":"<p>DSLX offers an <code>if</code> expression, which is very similar to the Rust <code>if</code> expression. Blueprint:</p> <pre><code>if condition { consequent } else { alternate }\n</code></pre> <p>This corresponds to the C/C++ ternary <code>?:</code> operator:</p> <pre><code>condition ? consequent : alternate\n</code></pre> <p>Note: both the <code>if</code> and <code>else</code> are required to be present, as with the <code>?:</code> operator, unlike a C++ <code>if</code> statement. This is because it is an expression that produces a result value, not a statement that causes a mutating effect.</p> <p>Furthermore, you can have multiple branches via <code>else if</code>:</p> <pre><code>if condition0 { consequent0 } else if condition1 { consequent1 } else { alternate }\n</code></pre> <p>which corresponds to the C/C++:</p> <pre><code>condition0 ? consequent0 : (contition1 ? consequent1 : alternate)\n</code></pre> <p>Note: a <code>match</code> expression can often be a better choice than having a long <code>if/else if/.../else</code> chain.</p> <p>For example, in the FP adder module (modules/fp32_add_2.x), there is code like the following:</p> <pre><code>[...]\nlet result_fraction = if wide_exponent &lt; u9:255 { result_fraction } else { u23:0 };\nlet result_exponent = if wide_exponent &lt; u9:255 { wide_exponent as u8 } else { u8:255 };\n</code></pre>"},{"location":"dslx_reference/#iterable-expression","title":"Iterable Expression","text":"<p>Iterable expressions are used in counted for loops. DSLX currently supports two types of iterable expressions, <code>range</code> and <code>enumerate</code>.</p> <p>The range expression <code>m..n</code> represents a range of values from m to n-1. This example will run from 0 to 4 (exclusive):</p> <pre><code>for (i, accum): (u32, u32) in u32:0..u32:4 {\n</code></pre> <p>There also exists a <code>range()</code> builtin function that performs the same operation.</p> <p><code>enumerate</code> iterates over the elements of an array type and produces pairs of <code>(index, value)</code>, similar to enumeration constructs in languages like Python or Go.</p> <p>In the example below, the loop will iterate 8 times, following the array dimension of <code>x</code>. Each iteration produces a tuple with the current index (<code>i</code> ranging from 0 to 7) and the value at the index (<code>e = x[i]</code>).</p> <pre><code>fn prefix_scan_eq(x: u32[8]) -&gt; bits[8,3] {\n  let (_, _, result) =\n    for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3]))\n        in enumerate(x) {...\n</code></pre>"},{"location":"dslx_reference/#for-expression","title":"for Expression","text":"<p>DSLX currently supports synthesis of \"counted\" for loops (loops that have a clear upper bound on their number of iterations). These loops are capable of being generated as unrolled pipeline stages: when generating a pipeline, the XLS compiler will unroll and specialize the iterations.</p> <p>NOTE In the future support for loops with an unbounded number of iterations may be permitted, but will only be possible to synthesize as a time-multiplexed implementation, since pipelines cannot be unrolled indefinitely.</p>"},{"location":"dslx_reference/#blueprint","title":"Blueprint","text":"<pre><code>for (index, accumulator): (type-of-index, type-of-accumulator) in iterable {\n   body-expression\n} (initial-accumulator-value)\n</code></pre> <p>The type annotation in the above \"blueprint\" is optional, but often helpful to include for increased clarity.</p> <p>Because DSLX is a pure dataflow description, a for loop is an expression that produces a value. As a result, you grab the output of a for loop just like any other expression:</p> <pre><code>let final_accum = for (i, accum) in u32:0..u32:8 {\n  let new_accum = f(accum);\n  new_accum\n}(init_accum);\n</code></pre> <p>Conceptually the for loop \"evolves\" the accumulator as it iterates, and ultimately pops it out as the result of its evaluation.</p>"},{"location":"dslx_reference/#examples","title":"Examples","text":"<p>Add up all values from 0 to 4 (exclusive). Note that we pass the accumulator's initial value in as a parameter to this expression.</p> <pre><code>for (i, accum): (u32, u32) in u32:0..u32:4 {\n  accum + i\n}(u32:0)\n</code></pre> <p>To add up values from 7 to 11 (exclusive), one would write:</p> <pre><code>let base = u32:7;\nfor (i, accum): (u32, u32) in u32:0..u32:4 {\n  accum + base + i\n}(u32:0)\n</code></pre> <p>\"Loop invariant\" values (values that do not change as the loop runs) can be used in the loop body, for example, note the use of <code>outer_thing</code> below:</p> <pre><code>let outer_thing: u32 = u32:42;\nfor (i, accum): (u32, u32) in u32:0..u32:4 {\n    accum + i + outer_thing\n}(u32:0)\n</code></pre> <p>Both the index and accumulator can be of any valid type, in particular, the accumulator can be a tuple type, which is useful for evolving a bunch of values. For example, this for loop \"evolves\" two arrays:</p> <pre><code>for (i, (xs, ys)): (u32, (u16[3], u8[3])) in u32:0..u32:4 {\n  ...\n}((init_xs, init_ys))\n</code></pre> <p>Note in the above example arrays are dataflow values just like anything else. To conditionally update an array every other iteration:</p> <pre><code>let result: u4[8] = for (i, array) in u32:0..u32:8 {\n  // Update every other cell with the square of the index.\n  if i % 2 == 0 { update(array, i, i*i) } else { array }\n}(u4[8]:[0, ...]);\n</code></pre>"},{"location":"dslx_reference/#numerical-conversions","title":"Numerical Conversions","text":"<p>DSLX adopts the Rust rules for semantics of numeric casts:</p> <ul> <li>Casting from larger bit-widths to smaller bit-widths will truncate (to     the LSbs).     *   This means that truncating signed values does not preserve the         previous value of the sign bit.</li> <li>Casting from a smaller bit-width to a larger bit-width will zero-extend if     the source is unsigned, sign-extend if the source is signed.</li> <li>Casting from a bit-width to its own bit-width, between signed/unsigned, is a     no-op.</li> </ul> <pre><code>#[test]\nfn test_numerical_conversions() {\n  let s8_m2 = s8:-2;\n  let u8_m2 = u8:0xfe;\n\n  // Sign extension (source type is signed, and we widen it).\n  assert_eq(s32:-2, s8_m2 as s32);\n  assert_eq(u32:0xfffffffe, s8_m2 as u32);\n  assert_eq(s16:-2, s8_m2 as s16);\n  assert_eq(u16:0xfffe, s8_m2 as u16);\n\n  // Zero extension (source type is unsigned, and we widen it).\n  assert_eq(u32:0xfe, u8_m2 as u32);\n  assert_eq(s32:0xfe, u8_m2 as s32);\n\n  // Nop (bitwidth is unchanged).\n  assert_eq(s8:-2, s8_m2 as s8);\n  assert_eq(s8:-2, u8_m2 as s8);\n  assert_eq(u8:0xfe, u8_m2 as u8);\n  assert_eq(s8:-2, u8_m2 as s8);\n}\n</code></pre>"},{"location":"dslx_reference/#array-conversions","title":"Array Conversions","text":"<p>Casting to an array takes bits from the MSb to the LSb; that is, the group of bits including the MSb ends up as element 0, the next group ends up as element 1, and so on.</p> <p>Casting from an array to bits performs the inverse operation: element 0 becomes the MSbs of the resulting value.</p> <p>All casts between arrays and bits must have the same total bit count.</p> <pre><code>fn cast_to_array(x: u6) -&gt; u2[3] {\n  x as u2[3]\n}\n\nfn cast_from_array(a: u2[3]) -&gt; u6 {\n  a as u6\n}\n\nfn concat_arrays(a: u2[3], b: u2[3]) -&gt; u2[6] {\n  a ++ b\n}\n\n#[test]\nfn test_cast_to_array() {\n  let a_value: u6 = u6:0b011011;\n  let a: u2[3] = cast_to_array(a_value);\n  let a_array = u2[3]:[1, 2, 3];\n  assert_eq(a, a_array);\n  // Note: converting back from array to bits gives the original value.\n  assert_eq(a_value, cast_from_array(a));\n\n  let b_value: u6 = u6:0b111001;\n  let b_array: u2[3] = u2[3]:[3, 2, 1];\n  let b: u2[3] = cast_to_array(b_value);\n  assert_eq(b, b_array);\n  assert_eq(b_value, cast_from_array(b));\n\n  // Concatenation of bits is analogous to concatenation of their converted\n  // arrays. That is:\n  //\n  //  convert(concat(a, b)) == concat(convert(a), convert(b))\n  let concat_value: u12 = a_value ++ b_value;\n  let concat_array: u2[6] = concat_value as u2[6];\n  assert_eq(concat_array, concat_arrays(a_array, b_array));\n\n  // Show a few classic \"endianness\" example using 8-bit array values.\n  let x = u32:0xdeadbeef;\n  assert_eq(x as u8[4], u8[4]:[0xde, 0xad, 0xbe, 0xef]);\n  let y = u16:0xbeef;\n  assert_eq(y as u8[2], u8[2]:[0xbe, 0xef]);\n}\n</code></pre>"},{"location":"dslx_reference/#bit-slice-expressions","title":"Bit Slice Expressions","text":"<p>DSLX supports Python-style bit slicing over unsigned bits types. Note that bits are numbered 0..N starting \"from the right (as you would write it on paper)\" -- least significant bit, AKA LSb -- for example, for the value <code>u7:0b100_0111</code>:</p> <pre><code>    Bit    6 5 4 3 2 1 0\n  Value    1 0 0 0 1 1 1\n</code></pre> <p>A slice expression <code>[N:M]</code> means to get from bit <code>N</code> (inclusive) to bit <code>M</code> exclusive. The start and limit in the slice expression must be signed integral values.</p> <p>Aside: This can be confusing, because the <code>N</code> stands to the left of <code>M</code> in the expression, but bit <code>N</code> would be to the 'right' of <code>M</code> in the classical bit numbering. Additionally, this is not the case in the classical array visualization, where element 0 is usually drawn on the left.</p> <p>For example, the expression <code>[0:2]</code> would yield:</p> <pre><code>    Bit    6 5 4 3 2 1 0\n  Value    1 0 0 0 1 1 1\n                     ^ ^  included\n                   ^      excluded\n\n  Result:  0b11\n</code></pre> <p>Note that, as of now, the indices for this <code>[N:M]</code> form must be literal numbers (so the compiler can determine the width of the result). To perform a slice with a non-literal-number start position, see the <code>+:</code> form described below.</p> <p>The slicing operation also support the python style slices with offsets from start or end. To visualize, one can think of <code>x[ : -1]</code> as the equivalent of <code>x[from the start : bitwidth - 1]</code>. Correspondingly, <code>x[-1 : ]</code> can be visualized as <code>[ bitwidth - 1 : to the end]</code>.</p> <p>For example, to get all bits, except the MSb (from the beginning, until the top element minus 1):</p> <pre><code>x[:-1]\n</code></pre> <p>Or to get the two most significant bits:</p> <pre><code>x[-2:]\n</code></pre> <p>This results in the nice property that a the original complete value can be sliced into complementary slices such as <code>:-2</code> (all but the two most significant bits) and <code>-2:</code> (the two most significant bits):</p> <pre><code>#[test]\nfn slice_into_two_pieces() {\n  let x = u5:0b11000;\n  let (lo, hi): (u3, u2) = (x[:-2], x[-2:]);\n  assert_eq(hi, u2:0b11);\n  assert_eq(lo, u3:0b000);\n}\n</code></pre>"},{"location":"dslx_reference/#width-slice","title":"Width Slice","text":"<p>There is also a \"width slice\" form <code>x[start +: bits[N]]</code> - starting from a specified bit, slice out the next <code>N</code> bits. This is equivalent to: <code>bits[N]:(x &gt;&gt; start)</code>. The type can be specified as either signed or unsigned; e.g. <code>[start +: s8]</code> will produce an 8-bit signed value starting at <code>start</code>, whereas <code>[start +: u4]</code> will produce a 4-bit unsigned number starting at <code>start</code>.</p> <p>Here are many more examples:</p>"},{"location":"dslx_reference/#bit-slice-examples","title":"Bit Slice Examples","text":"<pre><code>// Identity function helper.\nfn id&lt;N: u32&gt;(x: bits[N]) -&gt; bits[N] { x }\n\n#[test]\nfn test_bit_slice_syntax() {\n  let x = u6:0b100111;\n  // Slice out two bits.\n  assert_eq(u2:0b11, x[0:2]);\n  assert_eq(u2:0b11, x[1:3]);\n  assert_eq(u2:0b01, x[2:4]);\n  assert_eq(u2:0b00, x[3:5]);\n\n  // Slice out three bits.\n  assert_eq(u3:0b111, x[0:3]);\n  assert_eq(u3:0b011, x[1:4]);\n  assert_eq(u3:0b001, x[2:5]);\n  assert_eq(u3:0b100, x[3:6]);\n\n  // Slice out from the end.\n  assert_eq(u1:0b1, x[-1:]);\n  assert_eq(u1:0b1, x[-1:6]);\n  assert_eq(u2:0b10, x[-2:]);\n  assert_eq(u2:0b10, x[-2:6]);\n  assert_eq(u3:0b100, x[-3:]);\n  assert_eq(u3:0b100, x[-3:6]);\n  assert_eq(u4:0b1001, x[-4:]);\n  assert_eq(u4:0b1001, x[-4:6]);\n\n  // Slice both relative to the end (MSb).\n  assert_eq(u2:0b01, x[-4:-2]);\n  assert_eq(u2:0b11, x[-6:-4]);\n\n  // Slice out from the beginning (LSb).\n  assert_eq(u5:0b00111, x[:-1]);\n  assert_eq(u4:0b0111, x[:-2]);\n  assert_eq(u3:0b111, x[:-3]);\n  assert_eq(u2:0b11, x[:-4]);\n  assert_eq(u1:0b1, x[:-5]);\n\n  // Slicing past the end just means we hit the end (as in Python).\n  assert_eq(u1:0b1, x[5:7]);\n  assert_eq(u1:0b1, x[-7:1]);\n  assert_eq(bits[0]:0, x[-7:-6]);\n  assert_eq(bits[0]:0, x[-6:-6]);\n  assert_eq(bits[0]:0, x[6:6]);\n  assert_eq(bits[0]:0, x[6:7]);\n  assert_eq(u1:1, x[-6:-5]);\n\n  // Slice of a slice.\n  assert_eq(u2:0b11, x[:4][1:3]);\n\n  // Slice of an invocation.\n  assert_eq(u2:0b01, id(x)[2:4]);\n\n  // Explicit-width slices.\n  assert_eq(u2:0b01, x[2+:u2]);\n  assert_eq(s3:0b100, x[3+:s3]);\n  assert_eq(u3:0b001, x[5+:u3]);\n}\n</code></pre>"},{"location":"dslx_reference/#advanced-understanding-parametricity-constraints-and-unification","title":"Advanced Understanding: Parametricity, Constraints, and Unification","text":"<p>An infamous wrinkle is introduced for parametric functions: consider the following function:</p> <pre><code>// (Note: DSLX does not currently support the `T: type` construct shown here,\n// it is for example purposes only.)\nfn add_wrapper&lt;T: type, U: type&gt;(x: T, y: U) -&gt; T {\n  x + y\n}\n</code></pre> <p>Based on the inference rule, we know that '+' can only type check when the operand types are the same. This means we can conclude that type <code>T</code> is the same as type <code>U</code>. Once we determine this, we need to make sure anywhere <code>U</code> is used it is consistent with the fact it is the same as <code>T</code>. In a sense the + operator is \"adding a constraint\" that <code>T</code> is equivalent to <code>U</code>, and trying to check that fact is valid is under the purview of type inference. The fact that the constraint is added that <code>T</code> and <code>U</code> are the same type is referred to as \"unification\", as what was previously two entities with potentially different constraints now has a single set of constraints that comes from the union of its operand types.</p> <p>DSLX's typechecker will go through the body of parametric functions per invocation. As such, the typechecker will always have the invocation's parametric values for use in asserting type consistency against \"constraints\" such as derived parametric expressions, body vs. annotated return type equality, and expression inference rules.</p>"},{"location":"dslx_reference/#operator-precedence","title":"Operator Precedence","text":"<p>DSLX's operator precedence matches Rust's. Listed below are DSLX's operators in descending precedence order. Binary operators at the same level share the same associativity and will be grouped accordingly.</p> Operator Associativity <code>(...)</code> n/a unary <code>-</code> <code>!</code> n/a <code>as</code> Left to right <code>*</code> <code>/</code> <code>%</code> Left to right <code>+</code> <code>-</code> <code>++</code> Left to right <code>&lt;&lt;</code> <code>&gt;&gt;</code> Left to right <code>&amp;</code> Left to right <code>^</code> Left to right <code>|</code> Left to right <code>==</code> <code>!=</code> <code>&lt;</code> <code>&gt;</code> <code>&lt;=</code> <code>&gt;=</code> Left to right <code>&amp;&amp;</code> Left to right <code>||</code> Left to right"},{"location":"dslx_reference/#testing-and-debugging","title":"Testing and Debugging","text":"<p>DSLX allows specifying tests right in the implementation file via the <code>test</code> and <code>quickcheck</code> directives.</p> <p>Having key test code in the implementation file serves two purposes. It helps to ensure the code behaves as expected. Additionally it serves as 'executable' documentation, similar in spirit to Python doc strings.</p>"},{"location":"dslx_reference/#unit-tests","title":"Unit Tests","text":"<p>Unit tests are specified by the <code>test</code> directive, as seen below:</p> <pre><code>#[test]\nfn test_reverse() {\n  assert_eq(u1:1, rev(u1:1));\n  assert_eq(u2:0b10, rev(u2:0b01));\n  assert_eq(u2:0b00, rev(u2:0b00));\n}\n</code></pre> <p>The DSLX interpreter will execute all functions that are proceeded by a <code>test</code> directive. These functions should be non-parametric, take no arguments, and should return a unit-type.</p> <p>Unless otherwise specified in the implementation's build configs, functions called by unit tests are also converted to XLS IR and run through the toolchain's LLVM JIT. The resulting values from the DSLX interpreter and the LLVM JIT are compared against each other to assert equality. This is to ensure DSLX implementations are IR-convertible and that IR translation is correct.</p>"},{"location":"dslx_reference/#test-filtering","title":"Test Filtering","text":"<p>The DSLX main (runner) binary can also filter what tests are run from a file via the <code>--test_filter=REGEXP</code> flag.</p> <p>Unit tests run via Bazel can also be filtered via the typical Bazel <code>--test_filter</code> flag; i.e.</p> <pre><code>bazel test -c opt //xls/dslx/stdlib:apfloat_dslx_test --test_output=streamed\n</code></pre> <p>vs selecting one test:</p> <pre><code>bazel test -c opt //xls/dslx/stdlib:apfloat_dslx_test --test_output=streamed --test_filter=one_x_one_plus_one_f32\n</code></pre> <p>vs selecting multiple tests to run via regular expression:</p> <pre><code>bazel test -c opt //xls/dslx/stdlib:apfloat_dslx_test --test_output=streamed --test_filter=.*f32.*\n</code></pre>"},{"location":"dslx_reference/#quickcheck","title":"QuickCheck","text":"<p>QuickCheck is a testing framework concept founded on property-based testing. Instead of specifying expected and test values, QuickCheck asks for properties of the implementation that should hold true against any input of the specified type(s). In DSLX, we use the <code>quickcheck</code> directive to designate functions to be run via the toolchain's QuickCheck framework. Here is an example that complements the unit testing of DSLX's <code>rev</code> implementation from above:</p> <pre><code>// Reversing a value twice gets you the original value.\n\n#[quickcheck]\nfn prop_double_reverse(x: u32) -&gt; bool {\n  x == rev(rev(x))\n}\n</code></pre> <p>The DSLX interpreter will also execute all functions that are proceeded by a <code>quickcheck</code> directive. These functions should be non-parametric and return a <code>bool</code>. The framework will provide randomized input based on the types of the arguments to the function (e.g. above, the framework will provided randomized <code>u32</code>'s as <code>x</code>).</p> <p>By default, the framework will run the function against 1000 sets of randomized inputs. This default may be changed by specifying the <code>test_count</code> key in the <code>quickcheck</code> directive before a particular test:</p> <pre><code>#[quickcheck(test_count=50000)]\n</code></pre> <p>The framework also allows programmers to specify a seed to use in generating the random inputs, as opposed to letting the framework pick one. The seed chosen for production can be found in the execution log.</p> <p>For determinism, the DSLX interpreter should be run with the <code>seed</code> flag: <code>./interpreter_main --seed=1234 &lt;DSLX source file&gt;</code></p> <ol> <li> <p>Otherwise there'd be a use-before-definition error.\u00a0\u21a9</p> </li> </ol>"},{"location":"dslx_std/","title":"DSLX Built-In Functions and Standard Library","text":"<p>This page documents the DSLX built-in functions (i.e. functions that don't require an import and are available in the top level namespace) and standard library modules (i.e. which are imported with an unqualified name like <code>import std</code>).</p> <p>Generally built-in functions have some capabilities that cannot be easily done in a user-defined function, and thus do not live in the standard library.</p> <ul> <li>DSLX Built-In Functions and Standard Library<ul> <li>Built-ins<ul> <li>add_with_carry</li> <li>widening_cast and checked_cast</li> <li>smulp and umulp</li> <li>map</li> <li>zip</li> <li>array_rev</li> <li>const_assert!</li> <li>clz, ctz</li> <li>decode</li> <li>encode</li> <li>one_hot</li> <li>one_hot_sel</li> <li>signex</li> <li>slice</li> <li>rev</li> <li>bit_slice_update</li> <li>bit-wise reductions: and_reduce, or_reduce, xor_reduce</li> <li>update</li> <li>assert_eq, assert_lt</li> <li>zero!&lt;T&gt;</li> <li>trace_fmt!</li> <li>fail!: assertion failure</li> <li>cover!</li> <li>gate!</li> </ul> </li> <li>import std<ul> <li>Bits Type Properties<ul> <li>std::?_min_value</li> <li>std::?_max_value</li> <li>std::sizeof_?</li> </ul> </li> <li>Bit Manipulation Functions<ul> <li>std::to_signed &amp; std::to_unsigned</li> <li>std::lsb</li> <li>std::msb</li> <li>std::convert_to_bits_msb0</li> <li>std::convert_to_bools_lsb0</li> <li>std::mask_bits</li> <li>std::concat3</li> <li>std::rrot</li> <li>std::popcount</li> <li>std::extract_bits</li> <li>std::vslice</li> </ul> </li> <li>Mathematical Functions<ul> <li>std::bounded_minus_1</li> <li>std::abs</li> <li>std::is_pow2</li> <li>std::?add</li> <li>std::?mul</li> <li>std::iterative_div</li> <li>std::div_pow2</li> <li>std::mod_pow2</li> <li>std::ceil_div</li> <li>std::round_up_to_nearest</li> <li>std::round_up_to_nearest_pow2_?</li> <li>std::?pow</li> <li>std::clog2</li> <li>std:flog2</li> <li>std::?max</li> <li>std::?min</li> <li>std::uadd_with_overflow</li> <li>std::umul_with_overflow</li> </ul> </li> <li>Misc Functions<ul> <li>Signed comparison - std::{sge, sgt, sle, slt}</li> <li>std::find_index</li> </ul> </li> </ul> </li> <li>import acm_random<ul> <li>acm_random::rng_deterministic_seed</li> <li>acm_random::rng_new</li> <li>acm_random::rng_next</li> <li>acm_random::rng_next64</li> </ul> </li> </ul> </li> </ul>"},{"location":"dslx_std/#built-ins","title":"Built-ins","text":"<p>This section describes the built-in functions provided for use in the DSL that do not need to be explicitly imported.</p> <p>NOTE: A brief note on \"Parallel Primitives\": the DSL is expected to grow additional support for use of high-level parallel primitives over time, adding operators for order-insensitive reductions, scans, groupings, and similar. By making these operations known to the compiler in their high level form, we potentially enable optimizations and analyses on their higher level (\"lifted\") form. As of now, <code>map</code> is the sole parallel-primitive-oriented built-in.</p>"},{"location":"dslx_std/#add_with_carry","title":"<code>add_with_carry</code>","text":"<p>Operation that produces the result of the add, as well as the carry bit as an output. The binary add operators works similar to software programming languages, preserving the length of the input operands, so this built-in can assist when easy access to the carry out value is desired. Has the following signature:</p> <pre><code>fn add_with_carry&lt;N&gt;(x: uN[N], y: uN[N]) -&gt; (u1, uN[N])\n</code></pre>"},{"location":"dslx_std/#widening_cast-and-checked_cast","title":"<code>widening_cast</code> and <code>checked_cast</code>","text":"<p><code>widening_cast</code> and <code>checked_cast</code> cast bits-type values to bits-type values with additional checks compared to casting with <code>as</code>.</p> <p><code>widening_cast</code> will report a static error if the type casted to is unable to respresent all values of the type casted from (ex. <code>widening_cast&lt;u5&gt;(s3:0)</code> will fail because s3:-1 cannot be respresented as an unsigned number).</p> <p><code>checked_cast</code> will cause a runtime error during dslx interpretation if the value being casted is unable to fit within the type casted to (ex. <code>checked_cast&lt;u5&gt;(s3:0)</code> will succeed while <code>checked_cast&lt;u5&gt;(s3:-1)</code> will cause the dslx interpreter to fail.</p> <p>Currently both <code>widening_cast</code> and <code>checked_cast</code> will lower into a normal IR cast and will not generate additional assertions at the IR or Verilog level.</p> <pre><code>fn widening_cast&lt;sN[N]&gt;(value: uN[M]) -&gt; sN[N]; where N &gt; M\nfn widening_cast&lt;sN[N]&gt;(value: sN[M]) -&gt; sN[N]; where N &gt;= M\nfn widening_cast&lt;uN[N]&gt;(value: uN[M]) -&gt; uN[N]; where N &gt;= M\n\nfn checked_cast&lt;xN[M]&gt;(value: uN[N]) -&gt; xN[M]\nfn checked_cast&lt;xN[M]&gt;(value: sN[N]) -&gt; xN[M]\n</code></pre>"},{"location":"dslx_std/#smulp-and-umulp","title":"<code>smulp</code> and <code>umulp</code>","text":"<p><code>smulp</code> and <code>umulp</code> perform signed and unsigned partial multiplications. These operations return a two-element tuple with the property that the sum of the two elements is equal to the product of the original inputs. Performing a partial multiplication allows for a pipeline stage in the middle of a multiply. These operations have the following signatures:</p> <pre><code>fn smulp&lt;N&gt;(lhs: sN[N], rhs: sN[N]) -&gt; (sN[N], sN[N])\nfn umulp&lt;N&gt;(lhs: uN[N], rhs: uN[N]) -&gt; (uN[N], uN[N])\n</code></pre>"},{"location":"dslx_std/#map","title":"<code>map</code>","text":"<p><code>map</code>, similarly to other languages, executes a transformation function on all the elements of an original array to produce the resulting \"mapped' array. For example: taking the absolute value of each element in an input array:</p> <pre><code>import std;\n\nfn main(x: s3[3]) -&gt; s3[3] {\n  let y: s3[3] = map(x, std::abs);\n  y\n}\n\n#[test]\nfn main_test() {\n  let got: s3[3] = main(s3[3]:[-1, 1, 0]);\n  assert_eq(s3[3]:[1, 1, 0], got)\n}\n</code></pre> <p>Note that map is special, in that we can pass it a callee as if it were a value. As a function that \"takes\" a function as an argument, <code>map</code> is a special built-in -- in language implementor parlance it is a higher order function.</p> <p>Implementation note: Functions are not first class values in the DSL, so the name of the function must be referred to directly.</p> <p>Note: Novel higher order functions (e.g. if a user wanted to write their own <code>map</code>) cannot currently be written in user-level DSL code.</p>"},{"location":"dslx_std/#zip","title":"<code>zip</code>","text":"<p><code>zip</code> places elements of two same-sized arrays together in an array of 2-tuples.</p> <p>Its signature is: <code>zip(lhs: T[N], rhs: U[N]) -&gt; (T, U)[N]</code>.</p> <pre><code>#[test]\nfn test_zip_array_size_1() {\n    const LHS = u8[1]:[42];\n    const RHS = u16[1]:[64];\n    const WANT = (u8, u16)[1]:[(42, 64)]\n    assert_eq(zip(LHS, RHS), WANT);\n}\n\n#[test]\nfn test_zip_array_size_2() {\n    assert_eq(zip(u32[2]:[1, 2], u64[2]:[10, 11]), (u32, u64)[2]:[(1, 10), (2, 11)]);\n}\n</code></pre>"},{"location":"dslx_std/#array_rev","title":"<code>array_rev</code>","text":"<p><code>array_rev</code> reverses the elements of an array.</p> <pre><code>#[test]\nfn test_array_rev() {\n  assert_eq(array_rev(u8[1]:[42]), u8[1]:[42]);\n  assert_eq(array_rev(u3[2]:[1, 2]), u3[2]:[2, 1]);\n  assert_eq(array_rev(u3[3]:[2, 3, 4]), u3[3]:[4, 3, 2]);\n  assert_eq(array_rev(u4[3]:[0xf, 0, 0]), u4[3]:[0, 0, 0xf]);\n  assert_eq(array_rev(u3[0]:[]), u3[0]:[]);\n}\n</code></pre>"},{"location":"dslx_std/#const_assert","title":"<code>const_assert!</code>","text":"<p>Performs a check on a constant expression that only fails at compile-time (not at runtime).</p> <p>Note that this can only be applied to compile-time-constant expressions.</p> <pre><code>#[test]\nfn test_const_assert() {\n  const N = u32:42;\n  const_assert!(N &gt;= u32:1&lt;&lt;5);\n}\n</code></pre> <p>Keep in mind that all <code>const_assert!</code>s in a function evaluate, similar to <code>static_assert</code> in C++ -- they are effectively part of the type system, so you cannot suppress them by putting them in a conditional:</p> <pre><code>fn f() {\n  if false {\n    const_assert!(false);  // &lt;-- still fails even inside the \"if false\"\n  }\n}\n</code></pre>"},{"location":"dslx_std/#clz-ctz","title":"<code>clz</code>, <code>ctz</code>","text":"<p>DSLX provides the common \"count leading zeroes\" and \"count trailing zeroes\" functions:</p> <pre><code>  let x0 = u32:0x0FFFFFF8;\n  let x1 = clz(x0);\n  let x2 = ctz(x0);\n  assert_eq(u32:4, x1);\n  assert_eq(u32:3, x2)\n</code></pre>"},{"location":"dslx_std/#decode","title":"<code>decode</code>","text":"<p>Converts a binary-encoded value into a one-hot value. For an operand value of <code>n``interpreted as an unsigned number, the</code>n<code>-th result bit and only the</code>n`-th result bit is set. Has the following signature:</p> <pre><code>fn decode&lt;uN[W]&gt;(x: uN[N]) -&gt; uN[W]\n</code></pre> <p>The width of the decode operation may be less than the maximum value expressible by the input (<code>2**N - 1</code>). If the encoded operand value is larger than the number of bits of the result, the result is zero.</p> <p>Example usage: <code>dslx/tests/decode.x</code>.</p> <p>See also the IR semantics for the <code>decode</code> op.</p>"},{"location":"dslx_std/#encode","title":"<code>encode</code>","text":"<p>Converts a one-hot value to a binary-encoded value of the \"hot\" bit of the input. If the <code>n</code>-th bit and only the <code>n</code>-th bit of the operand is set, the result is equal to the value <code>n</code> as an unsigned number. Has the following signature:</p> <pre><code>fn encode(x: uN[N]) -&gt; uN[ceil(log2(N))]\n</code></pre> <p>If multiple bits of the input are set, the result is equal to the logical or of the results produced by the input bits individually. For example, if bit 3 and bit 5 of an encode input are set the result is equal to <code>3 | 5 = 7</code>.</p> <p>Example usage: <code>dslx/tests/encode.x</code>.</p> <p>See also the IR semantics for the <code>encode</code> op.</p>"},{"location":"dslx_std/#one_hot","title":"<code>one_hot</code>","text":"<p>Converts a value to one-hot form. Has the following signature:</p> <pre><code>fn one_hot&lt;N, NP1=N+1&gt;(x: uN[N], lsb_is_prio: bool) -&gt; uN[NP1]\n</code></pre> <p>When <code>lsb_is_prio</code> is true, the least significant bit that is set becomes the one-hot bit in the result. When it is false, the most significant bit that is set becomes the one-hot bit in the result.</p> <p>When all bits in the input are unset, the additional bit present in the output value (MSb) becomes set.</p> <p>Example usage: <code>dslx/tests/one_hot.x</code>.</p> <p>See also the IR semantics for the <code>one_hot</code> op.</p>"},{"location":"dslx_std/#one_hot_sel","title":"<code>one_hot_sel</code>","text":"<p>Produces the result of 'or'-ing all case values for which the corresponding bit of the selector is enabled. In cases where the selector has exactly one bit set (it is in one-hot form) this is equivalent to a match.</p> <pre><code>fn one_hot_sel&lt;N, M&gt;(selector: uN[N], cases: uN[N][M]) -&gt; uN[N]\n</code></pre> <p>Evaluates each case value and <code>or</code>s each case together if the corresponding bit in the selector is set. The first element of <code>cases</code> is included if the LSB is set, the second if the next least significant bit and so on. If no selector bits are set this evaluates to zero. This function is not generally used directly though the compiler will when possible synthesize the equivalent code from a <code>match</code> expression. This is included for testing purposes and for bespoke 'intrinsic-style programming' use cases.</p>"},{"location":"dslx_std/#signex","title":"<code>signex</code>","text":"<p>Casting has well-defined extension rules, but in some cases it is necessary to be explicit about sign-extensions, if just for code readability. For this, there is the <code>signex</code> built-in.</p> <p>To invoke the <code>signex</code> built-in, provide it with the operand to sign extend (lhs), as well as the target type to extend to: these operands may be either signed or unsigned. Note that the value of the right hand side is ignored, only its type is used to determine the result type of the sign extension.</p> <pre><code>#[test]\nfn test_signex() {\n  let x = u8:0xff;\n  let s: s32 = signex(x, s32:0);\n  let u: u32 = signex(x, u32:0);\n  assert_eq(s as u32, u)\n}\n</code></pre> <p>Note that both <code>s</code> and <code>u</code> contain the same bits in the above example.</p>"},{"location":"dslx_std/#slice","title":"slice","text":"<p>Array-slice built-in operation. Note that the \"want\" argument is not used as a value, but is just used to reflect the desired slice type. (Prior to constexprs being passed to built-in functions, this was the canonical way to reflect a constexpr in the type system.) Has the following signature:</p> <pre><code>fn slice&lt;T: type, N, M, S&gt;(xs: T[N], start: uN[M], want: T[S]) -&gt; T[S]\n</code></pre>"},{"location":"dslx_std/#rev","title":"<code>rev</code>","text":"<p><code>rev</code> is used to reverse the bits in an unsigned bits value. The LSb in the input becomes the MSb in the result, the 2nd LSb becomes the 2nd MSb in the result, and so on.</p> <pre><code>// (Dummy) wrapper around reverse.\nfn wrapper&lt;N: u32&gt;(x: bits[N]) -&gt; bits[N] {\n  rev(x)\n}\n\n// Target for IR conversion that works on u3s.\nfn main(x: u3) -&gt; u3 {\n  wrapper(x)\n}\n\n// Reverse examples.\n#[test]\nfn test_reverse() {\n  assert_eq(u3:0b100, main(u3:0b001));\n  assert_eq(u3:0b001, main(u3:0b100));\n  assert_eq(bits[0]:0, rev(bits[0]:0));\n  assert_eq(u1:1, rev(u1:1));\n  assert_eq(u2:0b10, rev(u2:0b01));\n  assert_eq(u2:0b00, rev(u2:0b00));\n}\n</code></pre>"},{"location":"dslx_std/#bit_slice_update","title":"<code>bit_slice_update</code>","text":"<p><code>bit_slice_update(subject, start, value)</code> returns a copy of the bits-typed value <code>subject</code> where the contiguous bits starting at index <code>start</code> (where 0 is the least-significant bit) are replaced with <code>value</code>. The bit-width of the returned value is the same as the bit-width of <code>subject</code>. Any updated bit indices which are out of bounds (if <code>start + bit-width(value) &gt;= bit-width(subject)</code>) are ignored. Example usage: <code>dslx/tests/bit_slice_update.x</code>.</p>"},{"location":"dslx_std/#bit-wise-reductions-and_reduce-or_reduce-xor_reduce","title":"bit-wise reductions: <code>and_reduce</code>, <code>or_reduce</code>, <code>xor_reduce</code>","text":"<p>These are unary reduction operations applied to a bits-typed value:</p> <ul> <li><code>and_reduce</code>: evaluates to bool:1 if all bits of the input are set, and 0     otherwise.</li> <li><code>or_reduce</code>: evaluates to bool:1 if any bit of the input is set, and 0     otherwise.</li> <li><code>xor_reduce</code>: evaluates to bool:1 if there is an odd number of bits set in     the input, and 0 otherwise.</li> </ul> <p>These functions return the identity element of the respective operation for trivial (0 bit wide) inputs:</p> <pre><code>#[test]\nfn test_trivial_reduce() {\n  assert_eq(and_reduce(bits[0]:0), true);\n  assert_eq(or_reduce(bits[0]:0), false);\n  assert_eq(xor_reduce(bits[0]:0), false);\n}\n</code></pre>"},{"location":"dslx_std/#update","title":"<code>update</code>","text":"<p><code>update(array, index, new_value)</code> returns a copy of <code>array</code> where <code>array[index]</code> has been replaced with <code>new_value</code>, and all other elements are unchanged. Note that this is not an in-place update of the array, it is an \"evolution\" of <code>array</code>. It is the compiler's responsibility to optimize by using mutation instead of copying, when it's safe to do. The compiler makes a best effort to do this, but can't guarantee the optimization is always made.</p>"},{"location":"dslx_std/#assert_eq-assert_lt","title":"<code>assert_eq</code>, <code>assert_lt</code>","text":"<p>In a unit test pseudo function all valid DSLX code is allowed. To evaluate test results DSLX provides the <code>assert_eq</code> primitive (we'll add more of those in the future). Here is an example of a <code>divceil</code> implementation with its corresponding tests:</p> <pre><code>fn divceil(x: u32, y: u32) -&gt; u32 {\n  (x-u32:1) / y + u32:1\n}\n\n#[test]\nfn test_divceil() {\n  assert_eq(u32:3, divceil(u32:5, u32:2));\n  assert_eq(u32:2, divceil(u32:4, u32:2));\n  assert_eq(u32:2, divceil(u32:3, u32:2));\n  assert_eq(u32:1, divceil(u32:2, u32:2));\n}\n</code></pre> <p><code>assert_eq</code> cannot currently be synthesized into equivalent Verilog. Because of that it is recommended to use it within <code>test</code> constructs (interpretation) only.</p>"},{"location":"dslx_std/#zerot","title":"<code>zero!&lt;T&gt;</code>","text":"<p>DSLX has a macro for easy creation of zero values, even from aggregate types. Invoke the macro with the type parameter as follows:</p> <pre><code>struct MyPoint {\n  x: u32,\n  y: u32,\n}\n\nenum MyEnum : u2 {\n  ZERO = u2:0,\n  ONE = u2:1,\n}\n\n#[test]\nfn test_zero_macro() {\n  assert_eq(zero!&lt;u32&gt;(), u32:0);\n  assert_eq(zero!&lt;MyPoint&gt;(), MyPoint{x: u32:0, y: u32:0});\n  assert_eq(zero!&lt;MyEnum&gt;(), MyEnum::ZERO);\n}\n</code></pre> <p>The <code>zero!&lt;T&gt;</code> macro can also be used with the struct update syntax to initialize a subset of fields to zero. In the example below all fields except <code>foo</code> are initialized to zero in the struct returned by <code>f</code>.</p> <pre><code>struct MyStruct {\n  foo: u1,\n  bar: u2,\n  baz: u3,\n  bat: u4,\n}\n\nfn f() -&gt; MyStruct {\n  MyStruct{foo: u1:1, ..zero!&lt;MyStruct&gt;()}\n}\n</code></pre>"},{"location":"dslx_std/#trace_fmt","title":"<code>trace_fmt!</code>","text":"<p>DSLX supports printf-style debugging via the <code>trace_fmt!</code> builtin, which allows dumping of current values to stdout. For example:</p> <pre><code>// Note: to see `trace_fmt!` output you need to be seeing `INFO` level logging,\n// enabled by adding the '--alsologtostderr' flag to the command line (among\n// other means). For example:\n// bazel run -c opt //xls/dslx:interpreter_main  /path/to/dslx/file.x -- --alsologtostderr\n\nfn shifty(x: u8, y: u3) -&gt; u8 {\n  trace_fmt!(\"x: {:x} y: {}\", x, y);\n  // Note: y looks different as a negative number when the high bit is set.\n  trace_fmt!(\"y as s8: {}\", y as s2);\n  x &lt;&lt; y\n}\n\n#[test]\nfn test_shifty() {\n  assert_eq(shifty(u8:0x42, u3:4), u8:0x20);\n  assert_eq(shifty(u8:0x42, u3:7), u8:0);\n}\n</code></pre> <p>would produce the following output, with each trace being annotated with its corresponding source position:</p> <pre><code>[...]\n[ RUN UNITTEST  ] test_shifty\nI0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] x: 42 y: 4\nI0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] y as s8: 4\nI0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] x: 42 y: 7\nI0510 14:31:17.516227 1247677 bytecode_interpreter.cc:994] y as s8: -1\n[            OK ]\n[...]\n</code></pre> <p>Note: <code>trace!</code> currently exists as a builtin but is in the process of being removed, as it provided the user with only a \"global flag\" way of specifying the desired format for output values -- <code>trace_fmt!</code> is more powerful.</p>"},{"location":"dslx_std/#fail-assertion-failure","title":"<code>fail!</code>: assertion failure","text":"<p>The <code>fail!</code> builtin indicates dataflow that should not be occurring in practice. Its general signature is:</p> <pre><code>fail!(label, fallback_value)\n</code></pre> <p>The <code>fail!</code> builtin can be thought of as a \"fatal assertion macro\". It is used to annotate dataflow that should not occur in practice and, if triggered, should raise a fatal error in simulation (e.g. via a JIT-execution failure status or a Verilog assertion when running in RTL simulation).</p> <p>Note, however, that XLS will permit users to avoid inserting fatal-error-signaling hardware that correspond to this <code>fail!</code> -- assuming it will not be triggered in practice minimizes its cost in synthesized form. In this situation, when it is \"erased\", it acts as the identity function, propagating the <code>fallback_value</code>. This allows XLS to keep well defined semantics even when fatal assertion hardware is not present.</p> <p>Example: if only these two enum values shown should be possible (say, as a documented precondition for <code>main</code>):</p> <pre><code>enum EnumType: u2 {\n  FIRST = 0,\n  SECOND = 1,\n}\n\nfn main(x: EnumType) -&gt; u32 {\n  match x {\n    EnumType::FIRST =&gt; u32:0,\n    EnumType::SECOND =&gt; u32:1,\n    _ =&gt; fail!(\"unknown_EnumType\", u32:0),\n  }\n}\n</code></pre> <p>The <code>fail!(\"unknown_EnumType\", u32:0)</code> above indicates that a) that match arm should not be reached (and if it is in the JIT or RTL simulation it will cause an error status or assertion failure respectively), but b) provides a fallback value to use (of the appropriate type) in case it were to happen in synthesized gates which did not insert fatal-error-indicating hardware.</p> <p>The associated label (the first argument) must be a valid Verilog identifier and is used for identifying the failure when lowered to SystemVerilog. At higher levels in the stack, it's unused.</p>"},{"location":"dslx_std/#cover","title":"<code>cover!</code>","text":"<p>NOTE: Currently, <code>cover!</code> has no effect in RTL simulators supported in XLS open source (i.e. iverilog). See google/xls#436.</p> <p>The <code>cover!</code> builtin tracks how often some condition is satisfied. It desugars into SystemVerilog cover points. Its signature is:</p> <pre><code>cover!(&lt;name&gt;, &lt;condition&gt;);\n</code></pre> <p>Where <code>name</code> is a function-unique literal string identifying the coverpoint and <code>condition</code> is a boolean element. When <code>condition</code> is true, a counter with the given name is incremented that can be inspected upon program termination. Coverpoints can be used to give an indication of code \"coverage\", i.e. to see what paths of a design are exercised in practice. The name of the coverpoint must begin with either a letter or underscore, and its remainder must consist of letters, digits, underscores, or dollar signs.</p>"},{"location":"dslx_std/#gate","title":"<code>gate!</code>","text":"<p>The <code>gate!</code> built-in is used for operand gating, of the form:</p> <pre><code>let gated_value = gate!(&lt;pass_value&gt;, &lt;value&gt;);\n</code></pre> <p>This will generally use a special Verilog macro to avoid the underlying synthesis tool doing boolean optimization, and will turn <code>gated_value</code> to <code>0</code> when the predicate <code>pass_value</code> is <code>false</code>. This can be used in attempts to manually avoid toggles based on the gating predicate.</p> <p>It is expected that XLS will grow facilities to inserting gating ops automatically, but manual user insertion is a practical step in this direction. Additionally, it is expected that if, in the resulting Verilog, gating occurs on a value that originates from a flip flop, the operand gating may be promoted to register-based load-enable gating.</p>"},{"location":"dslx_std/#import-std","title":"<code>import std</code>","text":""},{"location":"dslx_std/#bits-type-properties","title":"Bits Type Properties","text":""},{"location":"dslx_std/#std_min_value","title":"<code>std::?_min_value</code>","text":"<pre><code>pub fn unsigned_min_value&lt;N: u32&gt;() -&gt; uN[N]\npub fn signed_min_value&lt;N: u32&gt;() -&gt; sN[N]\n</code></pre> <p>Returns the minimum signed or unsigned value contained in N bits.</p>"},{"location":"dslx_std/#std_max_value","title":"<code>std::?_max_value</code>","text":"<pre><code>pub fn unsigned_max_value&lt;N: u32&gt;() -&gt; uN[N];\npub fn signed_max_value&lt;N: u32&gt;() -&gt; sN[N];\n</code></pre> <p>Returns the maximum signed or unsigned value contained in N bits.</p>"},{"location":"dslx_std/#stdsizeof_","title":"<code>std::sizeof_?</code>","text":"<pre><code>pub fn sizeof_unsigned&lt;N: u32&gt;(x : uN[N]) -&gt; u32\npub fn sizeof_signed&lt;N: u32&gt;(x : sN[N]) -&gt; u32\n</code></pre> <p>Returns the number of bits (sizeof) of unsigned or signed bit value.</p>"},{"location":"dslx_std/#bit-manipulation-functions","title":"Bit Manipulation Functions","text":""},{"location":"dslx_std/#stdto_signed-stdto_unsigned","title":"<code>std::to_signed</code> &amp; <code>std::to_unsigned</code>","text":"<pre><code>pub fn to_signed&lt;N: u32&gt;(x: uN[N]) -&gt; sN[N]\npub fn to_unsigned&lt;N: u32&gt;(x: sN[N]) -&gt; uN[N] {\n</code></pre> <p>Convenience helper that converts an unsigned bits argument to its same-sized signed type, or visa-versa. This is morally equivalent to (but slightly more convenient than) a pattern like:</p> <pre><code>    x as sN[std::sizeof_unsigned(x)]\n</code></pre>"},{"location":"dslx_std/#stdlsb","title":"<code>std::lsb</code>","text":"<pre><code>pub fn lsb&lt;N: u32&gt;(x: uN[N]) -&gt; u1\n</code></pre> <p>Extracts the LSb (Least Significant bit) from the value <code>x</code> and returns it.</p>"},{"location":"dslx_std/#stdmsb","title":"<code>std::msb</code>","text":"<pre><code>pub fn msb&lt;N: u32&gt;(x: uN[N]) -&gt; u1\n</code></pre> <p>Extracts the MSb (Most Significant bit) from the value <code>x</code> and returns it.</p>"},{"location":"dslx_std/#stdconvert_to_bits_msb0","title":"<code>std::convert_to_bits_msb0</code>","text":"<pre><code>pub fn convert_to_bits_msb0&lt;N: u32&gt;(x: bool[N]) -&gt; uN[N]\n</code></pre> <p>Converts an array of <code>N</code> bools to a <code>bits[N]</code> value.</p> <p>Note well: the boolean value at index 0 of the array becomes the most significant bit in the resulting bit value. Similarly, the last index of the array becomes the least significant bit in the resulting bit value.</p> <pre><code>import std;\n\n#[test]\nfn convert_to_bits_test() {\n  let _ = assert_eq(u3:0b001, std::convert_to_bits(bool[3]:[false, false, true]));\n  let _ = assert_eq(u3:0b100, std::convert_to_bits(bool[3]:[true, false, false]));\n  ()\n}\n</code></pre> <p>There's always a source of confusion in these orderings:</p> <ul> <li>Mathematically we often indicate the least significant digit as \"digit 0\"</li> <li>But, in a number as we write the digits from left-to-right on a piece of   paper, if you made an array from the written characters, the digit at \"array   index 0\" would be the most significant bit.</li> </ul> <p>So, it's somewhat ambiguous whether \"index 0\" in the array would become the least significant bit or the most significant bit. This routine uses the \"as it looks on paper\" conversion; e.g. <code>[true, false, false]</code> becomes <code>0b100</code>.</p>"},{"location":"dslx_std/#stdconvert_to_bools_lsb0","title":"<code>std::convert_to_bools_lsb0</code>","text":"<pre><code>pub fn fn convert_to_bools_lsb0&lt;N:u32&gt;(x: uN[N]) -&gt; bool[N]\n</code></pre> <p>Convert a \"word\" of bits to a corresponding array of booleans.</p> <p>Note well: The least significant bit of the word becomes index 0 in the array.</p>"},{"location":"dslx_std/#stdmask_bits","title":"<code>std::mask_bits</code>","text":"<pre><code>pub fn mask_bits&lt;X: u32&gt;() -&gt; bits[X]\n</code></pre> <p>Returns a value with X bits set (of type bits[X]).</p>"},{"location":"dslx_std/#stdconcat3","title":"<code>std::concat3</code>","text":"<pre><code>pub fn concat3&lt;X: u32, Y: u32, Z: u32, R: u32 = X + Y + Z&gt;(x: bits[X], y: bits[Y], z: bits[Z]) -&gt; bits[R]\n</code></pre> <p>Concatenates 3 values of arbitrary bitwidths to a single value.</p>"},{"location":"dslx_std/#stdrrot","title":"<code>std::rrot</code>","text":"<pre><code>pub fn rrot&lt;N: u32&gt;(x: bits[N], y: bits[N]) -&gt; bits[N]\n</code></pre> <p>Rotate <code>x</code> right by <code>y</code> bits.</p>"},{"location":"dslx_std/#stdpopcount","title":"<code>std::popcount</code>","text":"<pre><code>pub fn popcount&lt;N: u32&gt;(x: bits[N]) -&gt; bits[N]\n</code></pre> <p>Counts the number of bits in <code>x</code> that are '1'.</p>"},{"location":"dslx_std/#stdextract_bits","title":"<code>std::extract_bits</code>","text":"<pre><code>pub fn extract_bits&lt;from_inclusive: u32, to_exclusive: u32, fixed_shift: u32,\n                    N: u32&gt;(x : bits[N]) -&gt; bits[std::max(0, to_exclusive - from_inclusive)] {\n    let x_extended = x as uN[max(unsigned_sizeof(x) + fixed_shift, to_exclusive)];\n    (x_extended &lt;&lt; fixed_shift)[from_inclusive:to_exclusive]\n}\n</code></pre> <p>Extracts a bit-slice from x shifted left by fixed_shift.  This function behaves as-if x as resonably infinite precision so that the shift does not drop any bits and that the bit slice will be in-range.</p> <p>If <code>to_exclusive &lt;= from_excsuive</code>, the result will be a zero-bit <code>bits[0]</code>.</p>"},{"location":"dslx_std/#stdvslice","title":"<code>std::vslice</code>","text":"<pre><code>pub fn vslice&lt;MSB: u32, LSB: u32&gt;(x: bits[IN]) -&gt; bits[OUT]\n</code></pre> <p>Similar to <code>extract_bits</code> above, but corresponds directly to the \"part-select\" Verilog syntax. That is:</p> <pre><code>y &lt;= x[3:0];\n</code></pre> <p>Corresponds to:</p> <pre><code>let y: u4 = vslice&lt;u32:3, u32:0&gt;(x);\n</code></pre> <p>This is useful when porting code literally as a way to avoid transcription errors. For new code the DSLX first-class slicing syntax (either range-slicing or width-slicing) is preferred.</p>"},{"location":"dslx_std/#mathematical-functions","title":"Mathematical Functions","text":""},{"location":"dslx_std/#stdbounded_minus_1","title":"<code>std::bounded_minus_1</code>","text":"<pre><code>pub fn bounded_minus_1&lt;N: u32&gt;(x: uN[N]) -&gt; uN[N]\n</code></pre> <p>Returns the value of <code>x - 1</code> with saturation at <code>0</code>.</p>"},{"location":"dslx_std/#stdabs","title":"<code>std::abs</code>","text":"<pre><code>pub fn abs&lt;BITS: u32&gt;(x: sN[BITS]) -&gt; sN[BITS]\n</code></pre> <p>Returns the absolute value of <code>x</code> as a signed number.</p>"},{"location":"dslx_std/#stdis_pow2","title":"<code>std::is_pow2</code>","text":"<pre><code>pub fn is_pow2&lt;N: u32&gt;(x: uN[N]) -&gt; bool\n</code></pre> <p>Returns true when x is a non-zero power-of-two.</p>"},{"location":"dslx_std/#stdadd","title":"<code>std::?add</code>","text":"<pre><code>pub fn uadd&lt;N: u32, M: u32, R: u32 = umax(N,M) + 1&gt;(x: uN[N], y: uN[M]) -&gt; uN[R]\npub fn sadd&lt;N: u32, M: u32, R: u32 = umax(N,M) + 1&gt;(x: sN[N], y: sN[M]) -&gt; sN[R]\n</code></pre> <p>Returns sum of <code>x</code> (<code>N</code> bits) and <code>y</code> (<code>M</code> bits) as a <code>umax(N,M)+1</code> bit value.</p>"},{"location":"dslx_std/#stdmul","title":"<code>std::?mul</code>","text":"<pre><code>pub fn umul&lt;N: u32, M: u32, R: u32 = N + M&gt;(x: uN[N], y: uN[M]) -&gt; uN[R]\npub fn smul&lt;N: u32, M: u32, R: u32 = N + M&gt;(x: sN[N], y: sN[M]) -&gt; sN[R]\n</code></pre> <p>Returns product of <code>x</code> (<code>N</code> bits) and <code>y</code> (<code>M</code> bits) as an <code>N+M</code> bit value.</p>"},{"location":"dslx_std/#stditerative_div","title":"<code>std::iterative_div</code>","text":"<pre><code>pub fn iterative_div&lt;N: u32, DN: u32 = N * u32:2&gt;(x: uN[N], y: uN[N]) -&gt; uN[N]\n</code></pre> <p>Calculate <code>x / y</code> one bit at a time. This is an alternative to using the division operator '/' which may not synthesize nicely.</p>"},{"location":"dslx_std/#stddiv_pow2","title":"<code>std::div_pow2</code>","text":"<pre><code>pub fn div_pow2&lt;N: u32&gt;(x: bits[N], y: bits[N]) -&gt; bits[N]\n</code></pre> <p>Returns <code>x / y</code> where <code>y</code> must be a non-zero power-of-two.</p>"},{"location":"dslx_std/#stdmod_pow2","title":"<code>std::mod_pow2</code>","text":"<pre><code>pub fn mod_pow2&lt;N: u32&gt;(x: bits[N], y: bits[N]) -&gt; bits[N]\n</code></pre> <p>Returns <code>x % y</code> where <code>y</code> must be a non-zero power-of-two.</p>"},{"location":"dslx_std/#stdceil_div","title":"<code>std::ceil_div</code>","text":"<pre><code>pub fn ceil_div&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; uN[N]\n</code></pre> <p>Returns the ceiling of (x divided by y).</p>"},{"location":"dslx_std/#stdround_up_to_nearest","title":"<code>std::round_up_to_nearest</code>","text":"<pre><code>pub fn round_up_to_nearest(x: u32, y: u32) -&gt; u32\n</code></pre> <p>Returns <code>x</code> rounded up to the nearest multiple of <code>y</code>.</p>"},{"location":"dslx_std/#stdround_up_to_nearest_pow2_","title":"std::round_up_to_nearest_pow2_?","text":"<p>Returns <code>x</code> rounded up to the nearest multiple of <code>y</code>, where <code>y</code> is a known positive power of 2. This functionality is the same as <code>std::round_up_to_nearest</code> but optimized when <code>y</code> is a power of 2.</p>"},{"location":"dslx_std/#stdpow","title":"<code>std::?pow</code>","text":"<pre><code>pub fn upow&lt;N: u32&gt;(x: uN[N], n: uN[N]) -&gt; uN[N]\npub fn spow&lt;N: u32&gt;(x: sN[N], n: uN[N]) -&gt; sN[N]\n</code></pre> <p>Performs integer exponentiation as in Hacker's Delight, Section 11-3. Only non-negative exponents are allowed, hence the uN parameter for spow.</p>"},{"location":"dslx_std/#stdclog2","title":"<code>std::clog2</code>","text":"<pre><code>pub fn clog2&lt;N: u32&gt;(x: bits[N]) -&gt; bits[N]\n</code></pre> <p>Returns <code>ceiling(log2(x))</code>, with one exception: When <code>x = 0</code>, this function differs from the true mathematical function: <code>clog2(0) = 0</code> where as <code>ceil(log2(0)) = -infinity</code></p> <p>This function is frequently used to calculate the number of bits required to represent <code>x</code> possibilities. With this interpretation, it is sensible to define <code>clog2(0) = 0</code>.</p> <p>Example: <code>clog2(7) = 3</code>.</p>"},{"location":"dslx_std/#stdflog2","title":"<code>std:flog2</code>","text":"<pre><code>pub fn flog2&lt;N: u32&gt;(x: bits[N]) -&gt; bits[N]\n</code></pre> <p>Returns <code>floor(log2(x))</code>, with one exception:</p> <p>When x=0, this function differs from the true mathematical function: <code>flog2(0) = 0</code> where as <code>floor(log2(0)) = -infinity</code></p> <p>This function is frequently used to calculate the number of bits required to represent an unsigned integer <code>n</code> to define <code>flog2(0) = 0</code>, so that <code>flog(n)+1</code> represents the number of bits needed to represent the <code>n</code>.</p> <p>Example: <code>flog2(7) = 2</code>, <code>flog2(8) = 3</code>.</p>"},{"location":"dslx_std/#stdmax","title":"<code>std::?max</code>","text":"<pre><code>pub fn smax&lt;N: u32&gt;(x: sN[N], y: sN[N]) -&gt; sN[N]\npub fn umax&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; uN[N]\n</code></pre> <p>Returns the maximum of two integers.</p>"},{"location":"dslx_std/#stdmin","title":"<code>std::?min</code>","text":"<pre><code>pub fn smin&lt;N: u32&gt;(x: sN[N], y: sN[N]) -&gt; sN[N]\npub fn umin&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; uN[N]\n</code></pre> <p>Returns the minimum of two unsigned integers.</p>"},{"location":"dslx_std/#stduadd_with_overflow","title":"<code>std::uadd_with_overflow</code>","text":"<pre><code>pub fn uadd_with_overflow&lt;V: u32&gt;(x: uN[N], y: uN[M]) -&gt; (bool, uN[V])\n</code></pre> <p>Returns a 2-tuple indicating overflow (boolean) and a sum <code>(x + y) as uN[V]</code>. An overflow occurs if the result does not fit within a <code>uN[V]</code>.</p>"},{"location":"dslx_std/#stdumul_with_overflow","title":"<code>std::umul_with_overflow</code>","text":"<pre><code>pub fn umul_with_overflow&lt;V: u32&gt;(x: uN[N], y: uN[M]) -&gt; (bool, uN[V])\n</code></pre> <p>Returns a 2-tuple indicating overflow (boolean) and a product <code>(x * y) as uN[V]</code>. An overflow occurs if the result does not fit within a <code>uN[V]</code>.</p>"},{"location":"dslx_std/#misc-functions","title":"Misc Functions","text":""},{"location":"dslx_std/#signed-comparison-stdsge-sgt-sle-slt","title":"<code>Signed comparison - std::{sge, sgt, sle, slt}</code>","text":"<pre><code>pub fn sge&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; bool\npub fn sgt&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; bool\npub fn sle&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; bool\npub fn slt&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; bool\n</code></pre> <p>Explicit signed comparison helpers for working with unsigned values, can be a bit more convenient and a bit more explicit intent than doing casting of left hand side and right hand side.</p>"},{"location":"dslx_std/#stdfind_index","title":"<code>std::find_index</code>","text":"<pre><code>pub fn find_index&lt;BITS: u32, ELEMS: u32&gt;( array: uN[BITS][ELEMS], x: uN[BITS]) -&gt; (bool, u32)\n</code></pre> <p>Returns (<code>found</code>, <code>index</code>) given an array and the element to find within the array.</p> <p>Note that when <code>found</code> is false, the <code>index</code> is <code>0</code> -- <code>0</code> is provided instead of a value like <code>-1</code> to prevent out-of-bounds accesses from occurring if the index is used in a match expression (which will eagerly evaluate all of its arms), to prevent it from creating an error at simulation time if the value is ultimately discarded from the unselected match arm.</p>"},{"location":"dslx_std/#import-acm_random","title":"<code>import acm_random</code>","text":"<p>Port of ACM random number generator to DSLX.</p> <p>DO NOT use <code>acm_random.x</code> for any application where security -- unpredictability of subsequent output and previous output -- is needed. ACMRandom is in NO WAY a cryptographically secure pseudorandom number generator, and using it where recipients of its output may wish to guess earlier/later output values would be very bad.</p>"},{"location":"dslx_std/#acm_randomrng_deterministic_seed","title":"<code>acm_random::rng_deterministic_seed</code>","text":"<pre><code>pub fn rng_deterministic_seed() -&gt; u32\n</code></pre> <p>Returns a fixed seed for use in the random number generator.</p>"},{"location":"dslx_std/#acm_randomrng_new","title":"<code>acm_random::rng_new</code>","text":"<pre><code>pub fn rng_new(seed: u32) -&gt; State\n</code></pre> <p>Create the state for a new random number generator using the given seed.</p>"},{"location":"dslx_std/#acm_randomrng_next","title":"<code>acm_random::rng_next</code>","text":"<pre><code>pub fn rng_next(s: State) -&gt; (State, u32)\n</code></pre> <p>Returns a pseudo-random number in the range <code>[1, 2^31-2]</code>.</p> <p>Note that this is one number short on both ends of the full range of non-negative 32-bit integers, which range from <code>0</code> to <code>2^31-1</code>.</p>"},{"location":"dslx_std/#acm_randomrng_next64","title":"<code>acm_random::rng_next64</code>","text":"<pre><code>pub fn rng_next(s: State) -&gt; (State, u64)\n</code></pre> <p>Returns a pseudo random number in the range <code>[1, (2^31-2)^2]</code>.</p> <p>Note that this does not cover all non-negative values of int64, which range from <code>0</code> to <code>2^63-1</code>. The top two bits are ALWAYS ZERO.</p>"},{"location":"faq/","title":"XLS FAQ","text":""},{"location":"faq/#q-how-do-i-select-bubble-strategies-in-pipeline-generation","title":"Q: How do I select bubble strategies in pipeline generation?","text":"<p>Tags: codegen, pipeline, configuration</p> <p>I/O behavior is described in the codegen options documentation. (\"codegen\" for XLS is the concept of \"exactly what verilog should be produced?\") There are currently options for controlling queueing behavior at the inputs or outputs of the block, but not internal to the pipeline, ability to stall with queueing that is embedded in the generated pipeline is tracked in issue #255.</p>"},{"location":"faq/#q-what-is-the-granularity-of-readyvalid-signaling","title":"Q: What is the granularity of ready/valid signaling?","text":"<p>Tags: codegen, pipeline, configuration</p> <p>Ready/valid signals are associated with a (streaming) channel, and in general I/O signaling is configured on a per-channel basis. Users are expected to send things \"broadside\" (all together at once) if they should share the same ready/valid signaling; e.g. by sending a struct or array over the channel.</p> <p>See the <code>--streaming_channel_*</code> options within the codegen options documentation.</p>"},{"location":"faq/#q-how-do-i-call-my-xls-functions-from-c","title":"Q: How do I call my XLS functions from C++?","text":"<p>Tags: native, simulation, cpp</p> <p>The steps are:</p> <ol> <li>Wrap up your XLS so it can be called from C++ (using a utility).</li> <li>Include the created header.</li> <li>Call the \"Run\" API with XLS-understood values.</li> </ol> <p>The <code>cc_xls_ir_jit_wrapper</code> rule in the Bazel rule set invokes a tool (the JIT wrapper generator) that makes a shim that helpfully JIT compiles the IR to native code (e.g. x64 code), and provides an object that can be used as a C++ callable.</p> <p>As an example, see the float32 multiply test, which calls <code>Run()</code> on the float32 multiplier which is written in XLS and wrapped in the <code>float32_mul_jit_wrapper</code> build target.</p> <p>Note that for some APIs, e.g. those taking a XLS single precision float, the created interface will be able to accept a native C++ <code>float</code> directly, and similar for types like <code>uint32_t</code>, <code>uint64_t</code>, etc.</p>"},{"location":"floating_point/","title":"Floating-point routines","text":"<p>XLS provides implementations of several floating-point operations and may add more at any time. Here are listed notable details of our implementations or of floating-point operations in general. Unless otherwise specified, not possible or out-of-scope, all operations and types should be IEEE-754 compliant.</p> <p>For example, floating-point exceptions have not been implemented; they're outside our current scope. The numeric results of multiplication, on the other hand, should exactly match those of any other compliant implementation.</p>"},{"location":"floating_point/#apfloat","title":"APFloat","text":"<p>Floating-point operations are, in general, defined by the same sequence of steps regardless of their underlying bit widths: fractional parts must be expanded then aligned, then an operation (add, multiply, etc.) must be performed, interpreting the fractions as integral types, followed by rounding, special case handling, and reconstructing an output FP type.</p> <p>This observation leads to the possibility of generic floating-point routines: a fully parameterized add, for example, which can be instantiated with and 8-bit exponent and 23-bit fractional part for binary32 types, and an 11-bit exponent and 52-bit fractional part for binary64 types. Even more interesting, a hypothetical bfloat32 type could immediately be supported by, say, instantiating that adder with, say, 15 exponent bits and 16 fractional ones.</p> <p>As much as possible, XLS implements FP operations in terms of its <code>APFloat</code> (arbitrary-precision floating-point) type. <code>APFloat</code> is a parameterized floating-point structure with a fixed one-bit sign and specifiable exponent and fractional part size. For ease of use, common types, such as <code>float32</code>, are defined in terms of those <code>APFloat</code> types.</p> <p>For example, the generic \"is X infinite\" operation is defined in <code>apfloat.x</code> as:</p> <pre><code>// Returns whether or not the given APFloat represents an infinite quantity.\npub fn is_inf&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool {\n  (x.bexp == std::mask_bits&lt;EXP_SZ&gt;() &amp;&amp; x.fraction == bits[FRACTION_SZ]:0)\n}\n</code></pre> <p>Whereas in <code>float32.x</code>, <code>F32</code> is defined as:</p> <pre><code>pub type F32 = apfloat::APFloat&lt;u32:8, u32:23&gt;;\n</code></pre> <p>and <code>is_inf</code> is exposed as:</p> <pre><code>pub fn is_inf(f: F32) -&gt; bool { apfloat::is_inf&lt;u32:8, u32:23&gt;(f) }\n</code></pre> <p>In this way, users can refer to <code>F32</code> types and can use them as and with <code>float32::is_inf(f)</code>, giving them simplified access to a generic operation.</p>"},{"location":"floating_point/#supported-operations","title":"Supported operations","text":"<p>Here are listed the routines so far implemented in XLS. Unless otherwise specified, operations are implemented in terms of <code>APFloat</code>s such that they can support any precisions (aside from corner cases, such as a zero-byte fractional part).</p>"},{"location":"floating_point/#apfloattag","title":"<code>apfloat::tag</code>","text":"<pre><code>pub fn tag&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(input_float: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; APFloatTag\n</code></pre> <p>Returns the type of float as one of <code>APFloatTag::ZERO</code>, <code>APFloatTag::SUBNORMAL</code>, <code>APFloatTag::INFINITY</code>, <code>APFloatTag::NAN</code> and <code>APFloatTag::NORMAL</code>.</p>"},{"location":"floating_point/#apfloatqnan","title":"<code>apfloat::qnan</code>","text":"<pre><code>pub fn qnan&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;() -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns a <code>quiet NaN</code>.</p>"},{"location":"floating_point/#apfloatis_nan","title":"<code>apfloat::is_nan</code>","text":"<pre><code>pub fn is_nan&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns whether or not the given <code>APFloat</code> represents <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatinf","title":"<code>apfloat::inf</code>","text":"<pre><code>pub fn inf&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(sign: bits[1]) -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns a positive or a negative infinity depending upon the given sign parameter.</p>"},{"location":"floating_point/#apfloatis_inf","title":"<code>apfloat::is_inf</code>","text":"<pre><code>pub fn is_inf&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns whether or not the given <code>APFloat</code> represents an infinite quantity.</p>"},{"location":"floating_point/#apfloatis_pos_inf","title":"<code>apfloat::is_pos_inf</code>","text":"<pre><code>pub fn is_pos_inf&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns whether or not the given <code>APFloat</code> represents a positive infinite quantity.</p>"},{"location":"floating_point/#apfloatis_neg_inf","title":"<code>apfloat::is_neg_inf</code>","text":"<pre><code>pub fn is_neg_inf&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns whether or not the given <code>APFloat</code> represents a negative infinite quantity.</p>"},{"location":"floating_point/#apfloatzero","title":"<code>apfloat::zero</code>","text":"<pre><code>pub fn zero&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(sign: bits[1]) -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns a positive or negative zero depending upon the given sign parameter.</p>"},{"location":"floating_point/#apfloatone","title":"<code>apfloat::one</code>","text":"<pre><code>pub fn one&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(sign: bits[1]) -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns one or minus one depending upon the given sign parameter.</p>"},{"location":"floating_point/#apfloatnegate","title":"<code>apfloat::negate</code>","text":"<pre><code>pub fn negate&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns the negative of <code>x</code> unless it is a <code>NaN</code>, in which case it will change it from a quiet to signaling <code>NaN</code> or from signaling to a quiet <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatmax_normal_exp","title":"<code>apfloat::max_normal_exp</code>","text":"<pre><code>pub fn max_normal_exp&lt;EXP_SZ : u32&gt;() -&gt; sN[EXP_SZ]\n</code></pre> <p>Maximum value of the exponent for normal numbers with EXP_SZ bits in the exponent field. For single precision floats this value is 127.</p>"},{"location":"floating_point/#apfloatmin_normal_exp","title":"<code>apfloat::min_normal_exp</code>","text":"<pre><code>pub fn min_normal_exp&lt;EXP_SZ : u32&gt;() -&gt; sN[EXP_SZ]\n</code></pre> <p>Minimum value of the exponent for normal numbers with EXP_SZ bits in the exponent field. For single precision floats this value is -126.</p>"},{"location":"floating_point/#apfloatunbiased_exponent","title":"<code>apfloat::unbiased_exponent</code>","text":"<pre><code>pub fn unbiased_exponent&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(f: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; sN[EXP_SZ]\n</code></pre> <p>Returns the unbiased exponent. For normal numbers it is <code>bexp - 2^EXP_SZ + 1``and for subnormals it is,</code>2 - 2^EXP_SZ<code>. For infinity and `NaN</code>, there are no guarantees, as the unbiased exponent has no meaning in that case.</p> <p>For example, for single precision normal numbers the unbiased exponent is <code>bexp - 127``and for subnormal numbers it is</code>-126`.</p>"},{"location":"floating_point/#apfloatbias","title":"<code>apfloat::bias</code>","text":"<pre><code>pub fn bias&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(unbiased_exponent: sN[EXP_SZ]) -&gt; bits[EXP_SZ]\n</code></pre> <p>Returns the biased exponent which is equal to <code>unbiased_exponent + 2^EXP_SZ - 1</code></p> <p>Since the function only takes as input the unbiased exponent, it cannot distinguish between normal and subnormal numbers, as a result it assumes that the input is the exponent for a normal number.</p>"},{"location":"floating_point/#apfloatflatten","title":"<code>apfloat::flatten</code>","text":"<pre><code>pub fn flatten&lt;EXP_SZ:u32, FRACTION_SZ:u32,\n               TOTAL_SZ:u32 = {u32:1+EXP_SZ+FRACTION_SZ}&gt;(\n               x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; bits[TOTAL_SZ]\n</code></pre> <p>Returns a bit string of size <code>1 + EXP_SZ + FRACTION_SZ</code> where the first bit is the sign bit, the next <code>EXP_SZ</code> bit encode the biased exponent and the last <code>FRACTION_SZ</code> are the significand without the hidden bit.</p>"},{"location":"floating_point/#apfloatunflatten","title":"<code>apfloat::unflatten</code>","text":"<pre><code>pub fn unflatten&lt;EXP_SZ:u32, FRACTION_SZ:u32,\n                 TOTAL_SZ:u32 = {u32:1+EXP_SZ+FRACTION_SZ}&gt;(x: bits[TOTAL_SZ])\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns a <code>APFloat</code> struct whose flattened version would be the input string <code>x</code>.</p>"},{"location":"floating_point/#apfloatldexp","title":"<code>apfloat:ldexp</code>","text":"<pre><code>pub fn ldexp&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(\n             fraction: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n             exp: s32) -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p><code>ldexp</code> (load exponent) computes <code>fraction * 2^exp</code>. Note that:</p> <ul> <li>Input denormals are treated as/flushed to 0. (denormals-are-zero / DAZ).    Similarly, denormal results are flushed to 0.</li> <li>No exception flags are raised/reported.</li> <li>We emit a single, canonical representation for NaN (qnan) but accept all    <code>NaN</code> representations as input.</li> </ul>"},{"location":"floating_point/#apfloatcast_from_fixed_using_rne","title":"<code>apfloat::cast_from_fixed_using_rne</code>","text":"<pre><code>pub fn cast_from_fixed_using_rne&lt;EXP_SZ:u32, FRACTION_SZ:u32, NUM_SRC_BITS:u32&gt;(\n                                 to_cast: sN[NUM_SRC_BITS])\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt; {\n</code></pre> <p>Casts the fixed point number to a floating point number using RNE (Round to Nearest Even) as the rounding mode.</p>"},{"location":"floating_point/#apfloatcast_from_fixed_using_rz","title":"<code>apfloat::cast_from_fixed_using_rz</code>","text":"<pre><code>pub fn cast_from_fixed_using_rz&lt;EXP_SZ:u32, FRACTION_SZ:u32, NUM_SRC_BITS:u32&gt;(\n                                 to_cast: sN[NUM_SRC_BITS])\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt; {\n</code></pre> <p>Casts the fixed point number to a floating point number using RZ (Round to Zero) as the rounding mode.</p>"},{"location":"floating_point/#apfloatupcast","title":"<code>apfloat::upcast</code>","text":"<pre><code>fn upcast&lt;TO_EXP_SZ: u32, TO_FRACTION_SZ: u32, FROM_EXP_SZ: u32, FROM_FRACTION_SZ: u32&gt;\n    (f: APFloat&lt;FROM_EXP_SZ, FROM_FRACTION_SZ&gt;) -&gt; APFloat&lt;TO_EXP_SZ, TO_FRACTION_SZ&gt; {\n</code></pre> <p>Upcast the given apfloat to another (larger) apfloat representation. Note: denormal inputs get flushed to zero.</p>"},{"location":"floating_point/#apfloatnormalize","title":"<code>apfloat::normalize</code>","text":"<pre><code>pub fn normalize&lt;EXP_SZ:u32, FRACTION_SZ:u32,\n                 WIDE_FRACTION:u32 = {FRACTION_SZ + u32:1}&gt;(\n                 sign: bits[1], exp: bits[EXP_SZ],\n                 fraction_with_hidden: bits[WIDE_FRACTION])\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns a normalized APFloat with the given components. <code>fraction_with_hidden</code> is the fraction (including the hidden bit). This function only normalizes in the direction of decreasing the exponent. Input must be a normal number or zero. Subnormals/Denormals are flushed to zero in the result.</p>"},{"location":"floating_point/#apfloatis_zero_or_subnormal","title":"<code>apfloat::is_zero_or_subnormal</code>","text":"<pre><code>pub fn is_zero_or_subnormal&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(\n                            x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns <code>true</code> if <code>x == 0</code> or <code>x</code> is a subnormal number.</p>"},{"location":"floating_point/#apfloatcast_to_fixed","title":"<code>apfloat::cast_to_fixed</code>","text":"<pre><code>pub fn cast_to_fixed&lt;NUM_DST_BITS:u32, EXP_SZ:u32, FRACTION_SZ:u32&gt;(\n                     to_cast: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; sN[NUM_DST_BITS]\n</code></pre> <p>Casts the floating point number to a fixed point number. Unrepresentable numbers are cast to the minimum representable number (largest magnitude negative number).</p>"},{"location":"floating_point/#apfloateq_2","title":"<code>apfloat::eq_2</code>","text":"<pre><code>pub fn eq_2&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(\n            x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n            y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns <code>true</code> if <code>x == y</code>. Denormals are Zero (DAZ). Always returns <code>false</code> if <code>x</code> or <code>y</code> is <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatgt_2","title":"<code>apfloat::gt_2</code>","text":"<pre><code>pub fn gt_2&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(\n            x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n            y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns <code>true</code> if <code>x &gt; y</code>. Denormals are Zero (DAZ). Always returns <code>false</code> if <code>x</code> or <code>y</code> is <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatgte_2","title":"<code>apfloat::gte_2</code>","text":"<pre><code>pub fn gte_2&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(\n             x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n             y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns <code>true</code> if <code>x &gt;= y</code>. Denormals are Zero (DAZ). Always returns <code>false</code> if <code>x</code> or <code>y</code> is <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatlte_2","title":"<code>apfloat::lte_2</code>","text":"<pre><code>pub fn lte_2&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(\n             x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n             y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns <code>true</code> if <code>x &lt;= y</code>. Denormals are Zero (DAZ). Always returns <code>false</code> if <code>x</code> or <code>y</code> is <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatlt_2","title":"<code>apfloat::lt_2</code>","text":"<pre><code>pub fn lt_2&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(\n            x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n            y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; bool\n</code></pre> <p>Returns <code>true</code> if <code>x &lt; y</code>. Denormals are Zero (DAZ). Always returns <code>false</code> if <code>x</code> or <code>y</code> is <code>NaN</code>.</p>"},{"location":"floating_point/#apfloatround_towards_zero","title":"<code>apfloat::round_towards_zero</code>","text":"<pre><code>pub fn round_towards_zero&lt;EXP_SZ:u32, FRACTION_SZ:u32&gt;(\n                          x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns an <code>APFloat</code> with all its bits past the decimal point set to <code>0</code>.</p>"},{"location":"floating_point/#apfloatto_int","title":"<code>apfloat::to_int</code>","text":"<pre><code>pub fn to_int&lt;EXP_SZ: u32, FRACTION_SZ: u32, RESULT_SZ:u32&gt;(\n              x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; sN[RESULT_SZ]\n</code></pre> <p>Returns the signed integer part of the input float, truncating any fractional bits if necessary.</p>"},{"location":"floating_point/#apfloataddsub","title":"<code>apfloat::add/sub</code>","text":"<pre><code>pub fn add&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n                                          y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n\npub fn sub&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n                                          y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns the sum/difference of <code>x</code> and <code>y</code> based on a generalization of IEEE 754 single-precision floating-point addition, with the following exceptions:</p> <ul> <li>Both input and output denormals are treated as/flushed to 0.</li> <li>Only round-to-nearest mode is supported.</li> <li>No exception flags are raised/reported.</li> </ul> <p>In all other cases, results should be identical to other conforming implementations (modulo exact fraction values in the <code>NaN</code> case.</p>"},{"location":"floating_point/#implementation-details","title":"Implementation details","text":"<p>Floating-point addition, like any FP operation, is much more complicated than integer addition, and has many more steps. Being the first operation described, we'll take extra care to explain floating-point addition:</p> <ol> <li>Expand fractions: Floating-point operations are computed with bits    beyond that in their normal representations for increased precision. For    IEEE 754 numbers, there are three extra, called the guard, rounding and    sticky bits. The first two behave normally, but the last, the \"sticky\" bit,    is special. During shift operations (below), if a \"1\" value is ever shifted    into the sticky bit, it \"sticks\" - the bit will remain \"1\" through any    further shift operations. In this step, the fractions are expanded by these    three bits.</li> <li> <p>Align fractions: To ensure that fractions are added with appropriate    magnitudes, they must be aligned according to their exponents. To do so, the    smaller significant needs to be shifted to the right (each right shift is    equivalent to increasing the exponent by one).</p> <ul> <li>The extra precision bits are populated in this shift.</li> <li>As part of this step, the leading 1 bit... and a sign bit Note: The    sticky bit is calculated and applied in this step.</li> </ul> </li> <li> <p>Sign-adjustment: if the fractions differ in sign, then the fraction with    the smaller initial exponent needs to be (two's complement) negated.</p> </li> <li> <p>Add the fractions and capture the carry bit. Note that, if the signs of    the fractions differs, then this could result in higher bits being cleared.</p> </li> <li> <p>Normalize the fractions: Shift the result so that the leading '1' is    present in the proper space. This means shifting right one place if the    result set the carry bit, and to the left some number of places if high bits    were cleared.</p> <ul> <li>The sticky bit must be preserved in any of these shifts!</li> </ul> </li> <li> <p>Rounding: Here, the extra precision bits are examined to determine if    the result fraction's last bit should be rounded up. IEEE 754 supports five    rounding modes:</p> <ul> <li>Round towards 0: just chop off the extra precision bits.</li> <li>Round towards +infinity: round up if any extra precision bits are set.</li> <li>Round towards -infinity: round down if any extra precision bits are set.</li> <li>Round to nearest, ties away from zero: Rounds to the nearest value. In    cases where the extra precision bits are halfway between values, i.e.,    0b100, then the result is rounded up for positive numbers and down for    negative ones.</li> <li> <p>Round to nearest, ties to even: Rounds to the nearest value. In cases    where the extra precision bits are halfway between values, then the    result is rounded in whichever direction causes the LSB of the result    significant to be 0.</p> <ul> <li>This is the most commonly-used rounding mode.</li> <li>This is [currently] the only supported mode by the DSLX implementation.</li> </ul> </li> </ul> </li> <li> <p>Special case handling: The results are examined for special cases such    as NaNs, infinities, or (optionally) subnormals.</p> </li> </ol>"},{"location":"floating_point/#result-sign-determination","title":"Result sign determination","text":"<p>The sign of the result will normally be the same as the sign of the operand with the greater exponent, but there are two extra cases to consider. If the operands have the same exponent, then the sign will be that of the greater fraction, and if the result is 0, then we favor positive 0 vs. negative 0 (types are as for a C <code>float</code> implementation):</p> <pre><code>  let fraction = (addend_x as s29) + (addend_y as s29);\n  let fraction_is_zero = fraction == s29:0;\n  let result_sign = match (fraction_is_zero, fraction &lt; s29:0) {\n    (true, _) =&gt; u1:0,\n    (false, true) =&gt; !greater_exp.sign,\n    _ =&gt; greater_exp.sign,\n  };\n</code></pre>"},{"location":"floating_point/#rounding","title":"Rounding","text":"<p>As complicated as rounding is to describe, its implementation is relatively straightforward (types are as for a C <code>float</code> implementation):</p> <pre><code>  let normal_chunk = shifted_fraction[0:3];\n  let half_way_chunk = shifted_fraction[2:4];\n  let do_round_up =\n      if (normal_chunk &gt; u3:0x4) | (half_way_chunk == u2:0x3) { u1:1 }\n      else { u1:0 };\n\n  // We again need an extra bit for carry.\n  let rounded_fraction = if do_round_up { (shifted_fraction as u28) + u28:0x8 }\n                         else { shifted_fraction as u28 };\n  let rounding_carry = rounded_fraction[-1:];\n</code></pre>"},{"location":"floating_point/#apfloatmul","title":"<code>apfloat::mul</code>","text":"<pre><code>pub fn mul&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(x: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n                                          y: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt;\n</code></pre> <p>Returns the product of <code>x</code> and <code>y</code>, with the following exceptions:</p> <ul> <li>Both input and output denormals are treated as/flushed to 0.</li> <li>Only round-to-nearest mode is supported.</li> <li>No exception flags are raised/reported.</li> </ul> <p>In all other cases, results should be identical to other conforming implementations (modulo exact fraction values in the NaN case).</p>"},{"location":"floating_point/#apfloatfma","title":"<code>apfloat::fma</code>","text":"<pre><code>pub fn fma&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt;(a: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n                                          b: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;,\n                                          c: APFloat&lt;EXP_SZ, FRACTION_SZ&gt;)\n    -&gt; APFloat&lt;EXP_SZ, FRACTION_SZ&gt; {\n</code></pre> <p>Returns the Fused Multiply and Add (FMA) result of computing <code>a*b + c</code>.</p> <p>The FMA operation is a three-operand operation that computes the product of the first two and the sum of that with the third. The IEEE 754-2008 description of the operation states that the operation should be performed \"as if with unbounded range and precision\", limited only by rounding of the final result. In other words, this differs from a sequence of a separate multiply followed by an add in that there is only a single rounding step (instead of the two involved in separate operations).</p> <p>In practice, this means A) that the precision of an FMA is higher than individual ops, and thus that B) an FMA requires significantly more internal precision bits than naively expected.</p> <p>For binary32 inputs, to achieve the standard-specified precision, the initial mul requires the usual 48 ((23 fraction + 1 \"hidden\") * 2) fraction bits. When performing the subsequent add step, though, it is necessary to maintain 72 fraction bits ((23 fraction + 1 \"hidden\") * 3). Fortunately, this sum includes the guard, round, and sticky bits (so we don't need 75). The mathematical derivation of the exact amount will not be given here (as I've not done it), but the same calculated size would apply for other data types (i.e., 54 * 2 = 108 and 54 * 3 = 162 for binary64).</p> <p>Aside from determining the necessary precision bits, the FMA implementation is rather straightforward, especially after reviewing the adder and multiplier.</p>"},{"location":"floating_point/#float64","title":"float64","text":"<p>To help with the <code>float64</code> or double precision floating point numbers, <code>float32.x</code> defines the following type aliases.</p> <pre><code>pub type F64 = apfloat::APFloat&lt;11, 52&gt;;\npub type FloatTag = apfloat::APFloatTag;\npub type TaggedF64 = (FloatTag, F64);\n</code></pre> <p>Besides <code>float64</code> specializations of the functions in <code>apfloat.x</code>, the following functions are defined just for <code>float64</code>.</p>"},{"location":"floating_point/#float64to_int64","title":"<code>float64::to_int64</code>","text":"<pre><code>pub fn to_int64(x: F64) -&gt; s64;\n</code></pre> <p>Convert the <code>F64</code> struct into a 64 bit integer.</p>"},{"location":"floating_point/#float32","title":"float32","text":"<p>To help with the <code>float32</code> or single precision floating point numbers, <code>float32.x</code> defines the following type aliases.</p> <pre><code>pub type F32 = apfloat::APFloat&lt;8, 23&gt;;\npub type FloatTag = apfloat::APFloatTag;\npub type TaggedF32 = (FloatTag, F32);\npub const F32_ONE_FLAT = u32:0x3f800000;\n</code></pre> <p>Besides <code>float32</code> specializations of the functions in <code>apfloat.x</code>, the following functions are defined just for <code>float32</code>.</p>"},{"location":"floating_point/#float32to_int32-float32from_int32","title":"<code>float32::to_int32</code>, <code>float32::from_int32</code>","text":"<p><pre><code>pub fn to_int32(x: F32) -&gt; s32\npub fn from_int32(x: s32) -&gt; F32\n</code></pre> Convert the <code>F32</code> struct to and from a 32bit integer.</p>"},{"location":"floating_point/#float32fixed_fraction","title":"<code>float32::fixed_fraction</code>","text":"<pre><code>pub fn fixed_fraction(input_float: F32) -&gt; u23\n</code></pre> <p>TBD</p>"},{"location":"floating_point/#float32fast_rsqrt_config_refinementsfloat32fast_rsqrt","title":"<code>float32::fast_rsqrt_config_refinements</code>/<code>float32::fast_rsqrt</code>","text":"<pre><code>pub fn fast_rsqrt_config_refinements&lt;NUM_REFINEMENTS: u32 = {u32:1}&gt;(x: F32) -&gt; F32\npub fn fast_rsqrt(x: F32) -&gt; F32\n</code></pre> <p>Floating point (fast (approximate) inverse square root)[ https://en.wikipedia.org/wiki/Fast_inverse_square_root]. This should be able to compute <code>1.0 / sqrt(x)</code> using fewer hardware resources than using a <code>sqrt</code> and division module, although this hasn't been benchmarked yet. Latency is expected to be lower as well. The tradeoff is that this offers slighlty less precision (error is &lt; 0.2% in worst case). Note that:</p> <ul> <li>Input denormals are treated as/flushed to 0. (denormals-are-zero / DAZ).</li> <li>Only round-to-nearest mode is supported.</li> <li>No exception flags are raised/reported.</li> <li>We emit a single, canonical representation for NaN (qnan) but accept    all NaN respresentations as input.</li> </ul> <p><code>fast_rsqrt_config_refinements</code> allows the user to specify the number of Computes an approximation of 1.0 / sqrt(x). <code>NUM_REFINEMENTS</code> can be increased to tradeoff more hardware resources for more accuracy.</p> <p><code>fast_rsqrt</code> does exactly one refinement.</p>"},{"location":"floating_point/#bfloat16","title":"bfloat16","text":"<p>To help with the <code>bfloat16</code> floating point numbers, <code>bfloat16.x</code> defines the following type aliases.</p> <p><pre><code>pub type BF16 = apfloat::APFloat&lt;8, 7&gt;;\npub type FloatTag = apfloat::APFloatTag;\npub type TaggedBF16 = (FloatTag, BF16);\n</code></pre> Besides <code>bfloat16</code> specializations of the functions in <code>apfloat.x</code>, the following functions are defined just for <code>bfloat16</code>.</p>"},{"location":"floating_point/#bfloat16to_int16","title":"<code>bfloat16:to_int16</code>","text":"<p><pre><code>pub fn to_int16(x: BF16) -&gt; s16\n</code></pre> Convert the the <code>BF16</code> struct into a 16 bit integer.</p>"},{"location":"floating_point/#bfloat16increment_fraction","title":"<code>bfloat16:increment_fraction</code>","text":"<pre><code>pub fn increment_fraction(input: BF16) -&gt; BF16\n</code></pre> <p>Increments the fraction of the input BF16 by one and returns the normalized result. Input must be a normal non-zero number.</p>"},{"location":"floating_point/#testing","title":"Testing","text":"<p>Several different methods are used to test these routines, depending on applicability. These are:</p> <ul> <li>Reference comparison: exhaustive testing</li> <li>Reference comparison: space-sampling</li> <li>Formal proving</li> </ul> <p>When comparing to a reference, a natural question is the stability of the reference, i.e., is the reference answer the same across all versions or environments? Will the answer given by glibc/libm on AArch64 be the same as one given by a hardware FMA unit on a GPU? Fortunately, all \"correct\" implementations will give the same results for the same inputs. <sup>1</sup> In addition, POSIX has the same result-precision language. It's worth noting that -ffast-math doesn't currently affect FMA emission/fusion/fission/etc.</p> <p>differ between implementations due to the table maker's dilemma.</p>"},{"location":"floating_point/#exhaustive-testing","title":"Exhaustive testing","text":"<p>This is the happiest case - where the input space is so small that we can iterate over every possible input, effectively treating the input as a binary iteration counter. Sadly, this is uncommon (except, perhaps for ML math), as binary32 is the precision floor for most problems, and a 64-bit input space is well beyond our current abilities. Still - if your problem can be exhaustively tested (with respect to a trusted reference), it should be exhaustively tested!</p> <p>None of our current ops are tested in this way, although the bf16 cases could/should be.</p>"},{"location":"floating_point/#space-sampling","title":"Space-sampling","text":"<p>When the problem input space is too large for exhaustive testing, then random samples can be tested instead. This approach can't give complete verification of an implementation, but, given enough samples, it can yield a high degree of confidence.</p> <p>The existing modules are tested in this way. This could be improved by preventing re-testing of any given sample (at the cost of memory and, perhaps, atomic/locking costs) and by identifying interesting \"corner cases\" of the input space and focusing on those.</p>"},{"location":"floating_point/#formal-verification","title":"Formal verification","text":"<p>This sort of testing utilizes our formal solver infrastructure to prove correctness with the solver's internal FP implementation. This is fully described in the solvers documentation.</p> <ol> <li> <p>There are operations for which this is not true. Transcendental ops may\u00a0\u21a9</p> </li> </ol>"},{"location":"fpga_characterization/","title":"FPGA Characterization","text":"<p>Note that right now the in-tree yosys and nextpnr-ice40 builds plugins aren't registering properly, see issue #188. As a result, we have to use out-of-tree <code>yosys</code> and <code>nextpnr-ice40</code> builds for the moment.</p> <pre><code>$ bazel build -c opt //xls/synthesis/yosys:yosys_server_main\n$ ./bazel-bin/xls/synthesis/yosys/yosys_server_main \\\n    --yosys_path $(which yosys) \\\n    --nextpnr_path $(which nextpnr-ice40) \\\n    --synthesis_target=ice40 \\\n    --alsologtostderr\n</code></pre> <p>The above runs a gRPC service, so in another terminal pane, we run the characterization driver:</p> <pre><code>$ bazel run -c opt //xls/synthesis:timing_characterization_client_main\n$ ./bazel-bin/xls/synthesis/timing_characterization_client_main \\\n    &gt; ./xls/delay_model/models/ice40.textproto\n</code></pre> <p>This produces a textual representation of the delay model protobuf.</p>"},{"location":"fpga_characterization/#building-in-tree-binaries","title":"Building In-Tree Binaries","text":"<p>Note that these cannot currently be used for the above characterization flow, see issue #188</p> <p>Build <code>yosys</code> and <code>nextpnr-ice40</code>:</p> <pre><code>$ bazel build -c opt @at_clifford_yosys//:yosys @nextpnr//:nextpnr-ice40\n</code></pre>"},{"location":"fuzzer/","title":"XLS Fuzzer","text":"<ul> <li>XLS Fuzzer<ul> <li>Crashers Directory<ul> <li>Single-file reproducers</li> </ul> </li> <li>IR minimization</li> <li>Summaries</li> <li>Debugging a failing sample<ul> <li>Debugging a tool crash</li> <li>Result miscomparison: unoptimized IR</li> <li>Result miscomparison: optimized IR<ul> <li>Debugging the LLVM JIT<ul> <li>Building LLVM tools</li> <li>Running the LLVM optimization passes</li> <li>Running the Instcombine pass</li> <li>Evaluating LLVM IR</li> <li>Running LLVM code generation</li> </ul> </li> </ul> </li> <li>Result miscomparison: simulated Verilog</li> <li>Filing an LLVM bug</li> </ul> </li> </ul> </li> </ul> <p>To execute the XLS fuzz driver simply run a command line like the following:</p> <pre><code>bazel run -c opt \\\n  //xls/fuzzer:run_fuzz_multiprocess \\\n  -- --crash_path=/tmp/crashers-$(date +'%Y-%m-%d') --seed=0 --duration=8h\n</code></pre> <p>NOTE: The <code>--seed=0</code> flag makes the fuzzer run from a deterministic seed, so the same sequence of examples will be tested each time command line invocation. To run non-deterministically, do not provide the <code>--seed</code> flag.</p> <p>The XLS fuzzer generates a sequence of randomly generated DSLX functions and a set of random inputs to each function often with interesting bit patterns.</p> <p>Given that stimulus, the fuzz driver performs the following actions some of which may be disabled/enabled via flags (run with <code>--help</code> for more details):</p> <ul> <li>Runs the DSLX program through the DSLX interpreter with the batch of     arguments</li> <li>Converts the DSLX program to IR</li> <li>Optimizes the converted IR</li> <li>Interprets the pre-optimized and optimized IR with the batch of arguments</li> <li>Generates the Verilog from the IR with randomly selected codegen options</li> <li>Simulates the generated Verilog using the batch of arguments</li> <li>Performs a multi-way comparison of the DSLX interpreter results, the     pre-optimized IR interpreter results, post-optimized IR interpreter results,     and the simulator results</li> <li>If an issue is observed, the fuzz driver attempts to minimize the IR that     causes an issue to occur.</li> </ul> <p>The above actions are coordinated and run by the SampleRunner class. Many actions are performed by invoking a separate binary which isolates any crashes.</p> <p>When miscompares in results occur or the generated function crashes part of XLS, all artifacts generated by the fuzzer for that sample are written into a uniquely-named subdirectory under the <code>--crash_path</code> given in the command line. The fuzzer also writes a crasher file which is a single file for reproducing the issue. See below for instructions on debugging a failing sample.</p>"},{"location":"fuzzer/#crashers-directory","title":"Crashers Directory","text":"<p>The crashers directory includes a subdirectory created for each failing sample. To avoid collisions the subdirectory is named using a hash of the DSLX code. Each crasher subdirectory has the following contents:</p> <pre><code>$ ls /tmp/crashers-2019-06-25/05adbd50\nargs.txt                   ir_converter_main.stderr   run.sh\ncl.txt                     ir_minimizer.options.json  sample.ir\ncrasher_2020-04-23_9b05.x  ir_minimizer_test.sh       sample.ir.results\neval_ir_main.stderr        options.json               sample.x\nexception.txt              opt_main.stderr            sample.x.results\n</code></pre> <p>The directory includes the problematic DSLX sample (<code>sample.x</code>) and the input arguments (<code>args.txt</code>) as well as all artifacts generated and stderr output emitted by the various utilities invoked to test the sample. Notable files include:</p> <ul> <li><code>options.json</code> : Options used to run the sample.</li> <li><code>sample.ir</code> : Unoptimized IR generated from the DSLX sample.</li> <li><code>sample.opt.ir</code> : IR after optimizations.</li> <li><code>sample.v</code> : Generated Verilog.</li> <li><code>*.results</code> : The results (numeric values) produced by interpreting or     simulating the respective input (DSLX, IR, or Verilog).</li> <li><code>exception.txt</code> : The exception raised when running the sample. Typically     this will indicate either a result miscomparison or a tool return non-zero     status (for example, the IR optimizer crashed).</li> <li><code>crasher_*.x</code>: A single file reproducer which includes the DSLX code,     arguments, and options. See below for details.</li> </ul> <p>Typically the exact nature of the failure can be identified by reading the file <code>exception.txt</code> and possibly the stderr outputs of the various tools.</p> <p>The fuzzer can optionally produce a minimized IR reproduction of the problem. This will be written to <code>minimized.ir</code>. See below for details.</p>"},{"location":"fuzzer/#reproducers","title":"Single-file reproducers","text":"<p>When the fuzzer encounters an issue it will create a single-file reproducer:</p> <pre><code>--- Worker 14 observed an exception, noting\n--- Worker 14 noted crasher #1 for sampleno 42 at /tmp/crashers/095fb405\n</code></pre> <p>Copying that file to the directory <code>//xls/fuzzer/crashers</code> will automatically create a bazel test target for it and add it to the regression suite. Tests can also be added as known failures in <code>//xls/fuzzer/build_defs.bzl</code> as they're being triaged / investigated like so:</p> <pre><code>generate_crasher_regression_tests(\n    srcs = glob([\"crashers/*\"]),\n    prefix = \"xls/fuzzer\",\n    # TODO(xls-team): 2019-06-30 Triage and fix these.\n    failing = [\n        \"crashers/crasher_2019-06-29_129987.x\",\n        \"crashers/crasher_2019-06-29_402110.x\",\n    ],\n)\n</code></pre> <p>Known-failures are marked as manual and excluded from continuous testing.</p> <p>To run the regression suite:</p> <pre><code>bazel test //xls/fuzzer:all\n</code></pre> <p>To run the regression suite including known-failures, run the regression target directly:</p> <pre><code>bazel test //xls/fuzzer:regression_tests\n</code></pre> <p>To reproduce from that single-file reproducer there is a command line tool:</p> <pre><code>bazel run //xls/fuzzer:run_crasher -- \\\n  crasher_2019-06-26_3354.x\n</code></pre>"},{"location":"fuzzer/#minimization","title":"IR minimization","text":"<p>By default the fuzzer attempts to generate a minimal IR reproducer for the problem identified by the DSLX sample. Starting with the unoptimized IR the fuzzer invokes <code>ir_minimizer_main</code> to reduce the size of the input IR. It uses various simplification strategies to minimize the number of nodes in the IR. See the usage description in the tool source code for detailed information.</p> <p>The minimized IR is written to a file <code>minimized.ir</code> in the crasher directory for the sample. Note that minimization is only possible if the problem (crash, result miscomparison, etc.) occurs after conversion from DSLX to XLS IR.</p>"},{"location":"fuzzer/#summaries","title":"Summaries","text":"<p>To monitor progress of the fuzzer and to determine op coverage the fuzzer can optionally (with <code>--summary_path</code>) write summary information to files. The summary files are Protobuf files containing the proto <code>SampleSummaryProto</code> defined in <code>//xls/fuzzer/sample_summary.proto</code>. The summary information about the IR generated from the DSLX sample such as the number and type of each IR op as well as the bit width and number of operands.</p> <p>The summary information also includes a timing breakdown of the various operations performed for each sample (sample generation, IR conversion, etc). This can be used to identify performance bottlenecks in the fuzzer.</p> <p>The summaries can be read with the tool <code>//xls/fuzzer/read_summary_main</code>. See usage description in the code for more details.</p>"},{"location":"fuzzer/#debugging","title":"Debugging a failing sample","text":"<p>A generated sample can fail in one of two ways: a tool crash or a result miscomparison. A tool crash occurred if one of the tools invoked by the fuzzer (e.g., <code>opt_main</code> which optimizes the IR) returned a non-zero status. A result miscomparison occurred if there is not perfect correspondence between the results produced by various ways in which the generated function is evaluated:</p> <ol> <li>Interpreted DSLX</li> <li>Evaluated unoptimized IR</li> <li>Evaluated optimized IR</li> <li>Simulation of the generated (System)Verilog</li> </ol> <p>Generally, the results produced by the interpretation of the DSLX serves are the reference results for comparisons.</p> <p>To identify the underlying cause of the sample failure inspect the <code>exception.txt</code> file in the crasher directory. The file contains the text of the exception raised in <code>SampleRunner</code> which clearly identifies the kind of failure (result miscomparison or tool crash) and details about which evaluation resulted in a miscompare or which tool crashed, respectively. Consult the following sections on how to debug particular kinds of failures.</p>"},{"location":"fuzzer/#debugging-a-tool-crash","title":"Debugging a tool crash","text":"<p>The <code>exception.txt</code> file includes the invocation of the tool for reproducing the failure. Generally, this is a straightforward debugging process.</p> <p>If the failing tool is the IR optimizer binary <code>opt_main</code> the particular pass causing the failure should be in the backtrace. To retrieve the input to this pass, run <code>opt_main</code> with <code>--ir_dump_path</code> to dump the IR between each pass. The last IR file produced (the files are numbered sequentially) is the input to the failing pass.</p>"},{"location":"fuzzer/#result-miscomparison-unoptimized-ir","title":"Result miscomparison: unoptimized IR","text":"<p>The evaluation of the unoptimized IR is the first point at which result comparison occurs (DSLX interpretation versus unoptimized IR evaluation). A miscomparison here can indicate a bug in one of several places:</p> <ol> <li>DSLX interpreter</li> <li>DSLX to IR conversion</li> <li>IR interpreter or IR JIT. The error message in <code>exception.txt</code> indicates     whether the JIT or the interpreter was used.</li> </ol> <p>To help narrow this down, the IR interpreter can be compared against the JIT with the <code>eval_ir_main</code> tool:</p> <pre><code>  eval_ir_main --test_llvm_jit --input_file=args.txt sample.ir\n</code></pre> <p>This runs both the JIT and the interpreter on the unoptimized IR file (<code>sample.ir</code>) using the arguments in <code>args.txt</code> and compares the results. If this is successful, then likely the IR interpreter and the JIT are correct and problem lies earlier in the pipeline (DSLX interpretation or DSLX to IR conversion). Otherwise, there is definitely a bug in either the interpreter or the JIT as their results should always be equal.</p> <p>If a minimized IR file exists (<code>minimized.ir</code>) this may be a better starting point for isolating the failure.</p>"},{"location":"fuzzer/#result-miscomparison-optimized-ir","title":"Result miscomparison: optimized IR","text":"<p>This can indicate a bug in IR evaluation (interpreter or JIT) or in the optimizer. In this case, a comparison of the evaluation of the unoptimized IR against the DSLX interpreter has already succeeds so DSLX interpretation or conversion is unlikely to be the underlying cause.</p> <p>As with miscomparison involving the unoptimized IR, <code>eval_ir_main</code> can be used to compare the JIT results against the interpreter results:</p> <pre><code>  eval_ir_main --test_llvm_jit --input_file=args.txt sample.opt.ir\n</code></pre> <p>If the above invocation fails there is a bug in the JIT or the interpreter. Otherwise, there may be a bug in the optimizer. The tool <code>eval_ir_main</code> can help isolate the problematic optimization pass by running with the options <code>--optimize_ir</code> and <code>--eval_after_each_pass</code>. With these flags, the tool runs the optimization pipeline on the given IR and evaluates the IR after each pass is run. The first pass which results in a miscompare against the unoptimized input IR is flagged. Invocation:</p> <pre><code>  eval_ir_main --input_file=args.txt \\\n    --optimize_ir \\\n    --eval_after_each_pass \\\n    sample.ir\n</code></pre>"},{"location":"fuzzer/#debugging-the-llvm-jit","title":"Debugging the LLVM JIT","text":"<p>To help isolate bugs in the JIT, LLVM's optimization level can be set using the <code>--llvm_opt_level</code> flag:</p> <pre><code>  eval_ir_main --test_llvm_jit \\\n    --llvm_opt_level=0 \\\n    --input_file=args.txt sample.opt.ir\n</code></pre> <p>If the results match (pass) with the optimization level set to zero but fail with the default optimization level of 3, there is likely a bug in the LLVM optimizer or the XLS-generated LLVM program has undefined behavior.</p> <p>Unoptimized and optimized LLVM IR are dumped by the JIT with vlog level of 2 or higher, and the assembly is dumped at level 3 or higher. For example:</p> <pre><code>  eval_ir_main -v=3 --logtostderr --random_inputs=1 sample.opt.ir\n</code></pre> <p>You can also send the optimized or unoptimized IR or asm to a file with the <code>--llvm_jit_ir_output=&lt;file&gt;</code>, <code>--llvm_jit_opt_ir_output=&lt;file&gt;</code> and <code>--llvm_jit_asm_output=&lt;file&gt;</code> flags.</p> <p>For example:</p> <pre><code>  eval_ir_main --random_inputs=1 sample.opt.ir --llvm_jit_ir_output=sample.ll\n</code></pre> <p>A <code>main</code> function which invokes the jitted code on a particular input can be created with the <code>--llvm_jit_main_wrapper_output=&lt;file&gt;</code> flag. If the <code>--llvm_jit_main_wrapper_write_is_linked</code> flag is also given, the result of invoking the xls function will be written to the stdout. This flag calls and writes the results for the inputs in the order they are given. This main wrapper can be <code>llvm-link</code>'d with the ir of the function itself to generate an executable bytecode.</p> <p>Note: LLVM can change significantly and bytecode is not always compatible between versions. If possible, LLVM tools built at the same commit as the JIT should be used to interact with the generated llvm bytecode. This can be done by building the LLVM tools using <code>bazel</code> from the XLS repo.</p> <p>For example:</p> <pre><code>$ eval_ir_main --input 'bits[9]:0x0FA;bits[12]:0xEAB'        \\\n               --llvm_jit_ir_output=sample.ll                \\\n               --llvm_jit_main_wrapper_output=sample_main.ll \\\n               --llvm_jit_main_wrapper_write_is_linked       \\\n               sample.opt.ir\n$ llvm-link -S -o sample_exe.ll sample.ll sample_main.ll\n$ lli sample_exe.ll\n</code></pre>"},{"location":"fuzzer/#building-llvm-tools","title":"Building LLVM tools","text":"<p>The various LLVM tools such as <code>opt</code> and <code>lli</code> can be built with:</p> <pre><code>  bazel build /llvm/llvm-project/llvm:all\n</code></pre> <p>Build in fastbuild mode to get checks and debug features in LLVM.</p>"},{"location":"fuzzer/#running-the-llvm-optimization-passes","title":"Running the LLVM optimization passes","text":"<p>To run the LLVM IR optimizer run the following (starting with the unoptimized IR):</p> <pre><code>  opt sample.ll -O3 -S\n</code></pre> <p>To print the IR before and after each pass:</p> <pre><code> opt sample.ll -S -print-after-all -print-before-all -O3\n</code></pre>"},{"location":"fuzzer/#running-the-instcombine-pass","title":"Running the Instcombine pass","text":"<p>Instcombine is an LLVM optimization pass which is a common source of bugs in code generated from XLS. To run instcombine alone:</p> <pre><code> opt /tmp/bad.ll -S -passes=instcombine\n</code></pre> <p>Instcombine is a large monolithic pass and it can be difficult to isolate the exact transformation which caused the problem. Fortunately, this pass includes a \"compiler fuel\" option which can be used to limit the number of transformations performed by the pass. Example usage (fastbuild of LLVM is required):</p> <pre><code>opt -S --instcombine sample.ll --debug-counter=instcombine-visit-skip=0,instcombine-visit-count=42\n</code></pre>"},{"location":"fuzzer/#evaluating-llvm-ir","title":"Evaluating LLVM IR","text":"<p>The LLVM tool <code>lli</code> evaluates LLVM IR. The tool expects the IR to include a entry function <code>main</code>. This can be generated by <code>eval_ir_main --llvm_jit_main_wrapper_output=&lt;file&gt; --llvm_jit_main_wrapper_write_is_linked</code>. See the description in the tools page for how these flags work.</p> <p>Once both the main wrapper bytecode file and the function bytecode files are created they can be linked to a single file using <code>llvm-link</code>:</p> <pre><code>llvm-link -S -o sample_linked.ll sample.ll sample_main.ll\n</code></pre> <p>The LLVM tool <code>opt</code> optimizes the LLVM IR and can be piped to <code>lli</code> like so:</p> <pre><code>  opt sample_linked.ll --O2 | lli\n</code></pre> <p>The LLVM IR can also compiled to an object file using <code>llc</code> and driven using a generated llvm test wrapper. The directory <code>xls/fuzzer/debug</code> includes a script and example demonstrating how to run JIT-generated LLVM IR in this manner.</p>"},{"location":"fuzzer/#running-llvm-code-generation","title":"Running LLVM code generation","text":"<p>If the bug occurs during LLVM code generation (lowering of LLVM IR to object code) the LLVM tool <code>llc</code> may be used to reproduce the problem. <code>llc</code> takes LLVM IR and produces assembly or object code. Example invocation for producing object code:</p> <pre><code>llc sample_linked.ll -o sample.o --filetype=obj\n</code></pre> <p>The exact output of <code>llc</code> depends on the target machine used during compilation. Logging in the OrcJit (at vlog level 1) will emit the exact <code>llc</code> invocation which uses the same target machine as the JIT.</p>"},{"location":"fuzzer/#result-miscomparison-simulated-verilog","title":"Result miscomparison: simulated Verilog","text":"<p>This can be a bug in codegen, XLS's Verilog testbench code, or the Verilog simulator itself. Running the generated Verilog with different simulators can help isolate the problem:</p> <pre><code>  simulate_module_main --signature_file=module_sig.textproto \\\n    --args_file=args.txt \\\n    --verilog_simulator=iverilog \\\n    sample.v\n\n  simulate_module_main --signature_file=module_sig.textproto \\\n    --args_file=args.txt \\\n    --verilog_simulator=${SIM_2} \\\n    sample.v\n</code></pre> <p>The tool outputs the results of the evaluation to stdout so diffing their outputs is required.</p>"},{"location":"fuzzer/#filing-an-llvm-bug","title":"Filing an LLVM bug","text":"<p>If the fuzzer problem is due to a crash or miscompile by LLVM, file an LLVM bug here. Example LLVM bugs found by the fuzzer: 1, 2.</p> <p>Although the internal Google mirror of LLVM is updated frequently, prior to filing an LLVM bug it's a good idea to verify the failure against LLVM head. Steps to build a debug build of LLVM:</p> <pre><code>git clone https://github.com/llvm/llvm-project.git\ncd llvm-project\nmkdir build\ncd build\ncmake -G Ninja ../llvm -DCMAKE_BUILD_TYPE=Debug -DLLVM_TARGETS_TO_BUILD=X86\ncmake --build . -- opt # or llc or other target.\n</code></pre> <p>Below are instructions to configure LLVM with sanitizers enabled. This can be useful for reproducing issues found with the sanitizer-enabled fuzz tests.</p> <pre><code># Install a version of LLVM which supports necessary sanitizer options.\nsudo apt-get install lld-15 llvm-15 clang-15 libc++1-15\n# In llvm-project directory:\nmkdir build-asan\ncd build-asan\nTOOLBIN=/usr/lib/llvm-15/bin\n# Below enables a particular sanitizer option `sanitize-float-cast-overflow`.\n# `Address` can be used as an option instead of `Undefined` depending on the\n# desired sanitizer check.\ncmake ../llvm -GNinja -DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n  -DCMAKE_CXX_COMPILER=$TOOLBIN/clang++ -DCMAKE_C_COMPILER=$TOOLBIN/clang \\\n  -DLLVM_USE_SANITIZER=Undefined \\\n  -DLLVM_UBSAN_FLAGS='-fsanitize=float-cast-overflow -fsanitize-undefined-trap-on-error' \\\n  -DLLVM_ENABLE_LLD=On -DLLVM_TARGETS_TO_BUILD=X86\n</code></pre> <p>LLVM includes a test case minimizer called <code>bugpoint</code> which tries to reduce the size of an LLVM IR test case. <code>bugpoint</code> has many options but it can operate in a similar manner to the XLS IR minimizer where a user-specified script is used to determine whether the bug exists in the LLVM IR:</p> <pre><code>bugpoint input.ll -compile-custom -compile-command bugpoint_test.sh\n</code></pre> <p>Example bugpoint test script (<code>bugpoint_test.sh</code>):</p> <pre><code>#!/bin/bash\n\n# Create a temporary file for the test command\nlogfile=\"$(mktemp)\"\n\n# Run your test command (and redirect the output messages)\n/path/to/llc \"$@\" -o /tmp/out.o -mcpu=skylake-avx512 --filetype=obj &gt; \"${logfile}\" 2&gt;&amp;1\nret=\"$?\"\n\n# Print messages when error occurs\nif [ \"${ret}\" != 0 ]; then\n  echo \"test failed\"  # must print something on failure\n  cat \"${logfile}\"\nfi\n\n# Cleanup the temporary file\nrm \"${logfile}\"\n\nexit \"${ret}\"\n</code></pre>"},{"location":"ideas_and_projects/","title":"Ideas and Projects","text":"<p>This document lists a few sample ideas, projects, and research ideas, to help get started on contributing to XLS.</p>"},{"location":"ideas_and_projects/#programming-languages","title":"Programming Languages","text":""},{"location":"ideas_and_projects/#xls-new-frontends","title":"XLS New Frontends","text":"<p>One of XLS' primary core focus areas was defining a compiler intermediate representation that is powerful enough to express all required concepts, but minimal enough to make it easy to target from other, new, or modified programming languages. We are focusing on a functional domain-specific language, but others are possible. There is work to target the IR from C++. There are many other research systems out there with their own respective DSLs and other input mechanisms. It would be interesting to connect those to allow comparisons. An embedded DSL in Python could be developed, which is straightforward as the IR's builder interfaces are already exported to Python.</p>"},{"location":"ideas_and_projects/#core-xls","title":"Core XLS","text":""},{"location":"ideas_and_projects/#ml-for-delay-estimation","title":"ML for Delay Estimation","text":"<p>We currently estimate the delay of ops and op combos via benchmarks and the theory of logical efforts. In principle we are trying to guess what the commercial toolchains will do. This is a problem that seems to be just made for ML, especially for new technology nodes or FPGA devices.</p>"},{"location":"ideas_and_projects/#delay-estimation-for-a-variety-of-devices","title":"Delay Estimation for a variety of Devices","text":"<p>We are focusing on only a small number of FPGAs and very specific ASIC Flows. For the lager community out there we should add many more models, improve automation of deriving a delay model, and/or try ML based approaches.</p>"},{"location":"ideas_and_projects/#delay-estimation-for-implicit-broadcasts","title":"Delay Estimation for Implicit Broadcasts","text":"<p>HLS often creates implicit broadcasts (wire load / fan out / wiring congestion) in unrolled loops, deep pipelines, large memory blocks, etc. that lead to frequency bottlenecks. Modeling these broadcasts in the delay model can help mitigate or even completely solve such problems.</p>"},{"location":"ideas_and_projects/#z3","title":"Z3","text":"<p>We use Z3 for our logical equivalence checking, eg., to check that the optimized and unoptimized IR have the same semantics. With a solver like this in place, there are many more opportunities to apply it or make it more practical, for example.</p> <ul> <li>(Automatic) Partitioning of the input IR to reduce per-phase problem space</li> <li>Add / compare with other formal verification tools.</li> <li>There are other alternatives, e.g. Boolector</li> <li>Use cases we haven't thought of yet.</li> </ul>"},{"location":"ideas_and_projects/#design-verification-flows","title":"Design Verification Flows","text":"<p>Provide mechanisms for \"constrained random\" approaches to hardware verification -- either libraries to emulate capabilities of constraint-based vector generation similar to UVM, or more automated approaches provided by QuickCheck/Hypothesis.</p>"},{"location":"ideas_and_projects/#implement-new-blocks-and-functions-in-xls","title":"Implement new blocks and functions in XLS","text":""},{"location":"ideas_and_projects/#extend-xls-standard-library","title":"Extend XLS standard library","text":"<p>XLS has a small standard library which includes some basic utility functions and some limited floating point support. Extending this library and developing more complex functionality would improve the usability of XLS. Ideas:</p> <ul> <li>FP Libraries Implement libraries for important FP operations, modes, widths,     with all the relevant parameters. BFloat libraries</li> <li>Fixed point libraries, similar to above</li> <li>Exploit FPGA hard macros and BRAMs</li> </ul> <p>XLS implementations of common hardware libraries - arbiters - counters - encoders - fifos</p>"},{"location":"ideas_and_projects/#three-stage-risc-v","title":"Three-stage RISC-V","text":"<p>Piccolo is a 3-stage RISC-V core. Implementation appears of reasonable size.</p> <ul> <li>IF: ISA / Instruction Fetch / Decode Unit</li> <li>DM: Data Memory stage</li> <li>WB: Write Back stage</li> <li>ALU</li> <li>add/sub unit, mul unit, shifter unit</li> <li>PLIC / Platform Level Interrupts architecture</li> <li>Cache Hierarchy with various sizes, associativity, and replacement policies</li> <li>MMU and L1-Cache</li> <li>CSRs</li> <li>Register File</li> </ul>"},{"location":"ideas_and_projects/#support-for-systolic-arrays-as-a-parallel-pattern","title":"Support for Systolic Arrays as a \"Parallel Pattern\"","text":"<p>It is clear, especially in the domain of machine learning and signal processing, that HLS will benefit from full support of 2D elements, such as systolic arrays or specialized convolution engines. The idea here is to abstract these properly. For example, one could add constructs to the IR, or we can extend the analysis capabilities and add passes to construct them as needed. We also have to make sure that the mechanisms are fully supported by all optimization and code generation passes. This is a fascinating and wide-open research area and intersects with the current work on concurrent blocks.</p>"},{"location":"ideas_and_projects/#interoperability-with-other-languages-ffi","title":"Interoperability with other languages, FFI","text":"<p>Users may want to reuse building blocks written in other languages that are pre-optimized. Support proper FFI. Instantiate external blocks by specifying their properties in a way the scheduler can understand. Import type definitions from system verilog descriptions e.g. as encoded in protobufs</p>"},{"location":"ideas_and_projects/#xls-tools","title":"XLS Tools","text":""},{"location":"ideas_and_projects/#visualization","title":"Visualization","text":"<p>XLS includes some minimal visualization and exploration tools. There is large potential for tool builders to improve and add information, insights, suggestions, etc.</p>"},{"location":"ideas_and_projects/#source-correlation","title":"Source Correlation","text":"<p>It is always important to maintain a high level of productivity and utility for a toolset like XLS provides. Source correlation, annotations, and many other techniques are always improvable the enhance the debugging, visualization, and also design verification experiences.</p>"},{"location":"ideas_and_projects/#source-analysis","title":"Source Analysis","text":"<p>A linter/style checker for the DSL would be very helpful for users to write high-quality HLS code.</p>"},{"location":"ideas_and_projects/#xls-integration-with-other-tools","title":"XLS Integration with other tools","text":""},{"location":"ideas_and_projects/#add-verilator-support-to-xls","title":"Add Verilator support to XLS","text":"<p>Verilator is a strong open-source (System)Verilog simulator. Currently XLS only supports Icarus Verilog, a Verilog-only simulator. Verilator support in XLS will provide much higher simulation performance and SystemVerilog support to open source users. Unlike other simulators which can run testbench code, Verilator only operates on synthesizable Verilog and emits C++ code for compilation and execution. This unique flow needs to be integrating into XLS's simulation framework.</p>"},{"location":"interpreters/","title":"Interpreters","text":"<p>XLS provides a several interpreters to assist in design validation across our functional stack, from input DSLX down to the netlist level.</p> <ul> <li>Interpreters<ul> <li>DSLX<ul> <li>Execution comparison</li> </ul> </li> <li>IR</li> <li>Netlists</li> </ul> </li> </ul>"},{"location":"interpreters/#dslx","title":"DSLX","text":"<p>The DSLX interpreter (<code>//xls/dslx:interpreter_main</code>) operates on DSLX <code>.x</code> files that contain both the design and unit tests to execute (present as <code>#[test]</code> annotated functions).</p> <p>The adler32 example demonstrates this: the design is encapsulated in the <code>main</code>, <code>adler32_seq</code>, and <code>mod</code> functions, and the samples are present in the test <code>adler32_one_char</code> (note that unit-style tests/interpretations of <code>adler32_seq</code> and <code>mod</code> could also be present).</p> <p>Interpreter targets are automatically generated for <code>dslx_test()</code> targets, so no special declarations are necessary to wrap DSLX code.</p> <p>To invoke these samples, execute the following:</p> <pre><code>bazel build -c opt //xls/examples/adler32:adler32_dslx_test\n./bazel-bin/xls/examples/adler32/adler32_dslx_test\n</code></pre> <p>To execute directly via the interpreter, you can instead run:</p> <pre><code>$ bazel build -c opt //xls/dslx/interpreter_main\n$ ./bazel-bin/xls/dslx/interpreter_main \\\n    ./xls/examples/adler32/adler32.x\n</code></pre> <p>These two methods are equivalent.</p>"},{"location":"interpreters/#execution-comparison","title":"Execution comparison","text":"<p>The DSL interpreter provides a flag, <code>--compare</code>, to implicitly compare its run results to those of the IR-converted DSL functions. This helps \"spot check\" consistency between IR and DSL execution (in addition to other methods used in more generally in XLS, like the fuzzer).</p> <p>The user may compare DSL execution to IR interpreter execution, IR JIT execution, or not perform IR comparison at all.</p> <pre><code>$ ./bazel-bin/xls/dslx/interpreter_main \\\n    ./xls/examples/adler32/adler32.x --compare=jit\n$ ./bazel-bin/xls/dslx/interpreter_main \\\n    ./xls/examples/adler32/adler32.x --compare=interpreter\n$ ./bazel-bin/xls/dslx/interpreter_main \\\n    ./xls/examples/adler32/adler32.x --compare=none\n</code></pre>"},{"location":"interpreters/#ir","title":"IR","text":"<p>XLS provides two means of evaluating IR - interpretation and native host compilation (the JIT). Both are invoked in nearly the same way, via the <code>eval_ir_main</code> tool.</p> <p><code>eval_ir_main</code> supports a wide number of use cases, but the most common end-user case will be to run a sample through a design. To evaluate a sample (1.0 + 2.5) on the add function in floating-point adder, one would run the following:</p> <pre><code>bazel build -c opt //xls/tools:eval_ir_main\n./bazel-bin/xls/tools/eval_ir_main    \\\n--input '(bits[1]: 0x0, bits[8]:0x7F, bits[23]:0x0); (bits[1]: 0x0, bits[8]:0x80, bits[23]:0x200000)'   \\\n./bazel-bin/xls/dslx/stdlib/float32_add.opt.ir\n</code></pre> <p>By default, this runs via the JIT. To use the interpreter, add the <code>--use_llvm_jit=false</code> flag to the invocation.</p> <p><code>eval_ir_main</code> supports a broad set of options and modes of execution. Refer to its [very thorough] <code>--help</code> documentation for full details.</p>"},{"location":"interpreters/#netlists","title":"Netlists","text":"<p>Finally, compiled netlists can also be interpreted against input samples via the aptly-named <code>netlist_interpreter_main</code> tool. This tool currently only supports single sample evaluation (as illustrated in the IR section above):</p> <pre><code>bazel build -c opt //xls/tools:netlist_interpreter_main\n./bazel-bin/xls/tools/netlist_interpreter_main \\\n  --netlist &lt;path to netlist&gt;\n  --module  &lt;module to evaluate&gt;\n  --cell_library[_proto] &lt;path to the module's cell library [proto]&gt;\n  --inputs  &lt;input sample, as above&gt;\n</code></pre> <p>As XLS does not currently provide an sample/example netlist (TODO(rspringer)), concrete values can't [yet] be provided here. The <code>--cell_library</code> flag merits extra discussion, though.</p> <p>During netlist compilation, a cell library is provided to indicate the individual logic cells available for the design, and these cells are referenced in the output netlist. The interpreter needs a description of these cells' behaviors/functions, so the cell library must be provided here, as well. Many cell libraries are very large (&gt; 1GB), and can thus incur significant processing overhead at startup, so we also accept pre-processed cell libraries, as <code>CellLibraryProto</code> messages, that contain much-abridged cell descriptions. The <code>function_extractor_main</code> tool can automatically perform this extraction for Liberty-formatted cell library descriptions.</p>"},{"location":"ir_jit/","title":"IR JIT Compiler","text":"<ul> <li>IR JIT Compiler<ul> <li>Usage<ul> <li>Specialized matching</li> <li>Direct usage</li> </ul> </li> <li>Design<ul> <li>Arg passing</li> <li>ArrayIndex</li> </ul> </li> </ul> </li> </ul> <p>XLS provides a JIT compiler for evaluating functions written in the [XLS] compiler intermediate representation (IR) at native machine speed.</p>"},{"location":"ir_jit/#usage","title":"Usage","text":"<p>Given a DSLX file and build target, one can build and run it through the JIT by:</p> <ol> <li>Declaring a     <code>cc_xls_ir_jit_wrapper</code>     target matching the DSLX build target.</li> <li>Creating a JIT object and calling its <code>Run()</code> method. Using the 2-way     floating-point adder as an example:<pre><code> # include \"xls/modules/fpadd_2x24_jit_wrapper.h\"\n\n absl::StatusOr&lt;Value&gt; foo(Value a, Value b) {\n   ...\n   // Only create this once and re-use it; it's created here just as\n   // an illustration.\n   XLS_ASSIGN_OR_RETURN(Fpadd2x32 adder, Fpadd2x32::Create()); return\n   adder.Run(a, b);\n }\n</code></pre> </li> </ol> <p>The advantages of JIT compilation (or any compilation, for that matter) only come into play when repeatedly using the compiled object, so programs should be structured to create a JIT wrapper once and to reuse it many times, e.g., to test a module across many - or even exhaustively, across all possible - inputs.</p>"},{"location":"ir_jit/#specialized-matching","title":"Specialized matching","text":"<p>In many cases, the types used by DSLX designs map to native types, such as the C/C++ <code>float</code>. In that case, a simplified wrapper call is available:</p> <pre><code>#include \"xls/modules/fpadd_2x24_jit_wrapper.h\"\n\nabsl::StatusOr&lt;float&gt; foo(float a, float b) {\n  ...\n  // Only create this once and re-use it; it's created here just as\n  // an illustration.\n  XLS_ASSIGN_OR_RETURN(Fpadd2x32 adder, Fpadd2x32::Create());\n  return adder.Run(a, b);\n}\n</code></pre> <p>When available, these simplified wrappers should be used for higher performance (~30% in our measured cases). Currently, floats and integral types &gt;= 64 bits in this way. For non-native integral types, the generated wrapper will accept the next larger native type e.g., <code>uint64_t</code> for <code>bits[47]</code>. Proper operation with next-larger types depends on the input value being present in the least-significant bits of the containing type.</p> <p>These are higher performance because they avoid unnecessary marshaling of these types into Views (e.g., a <code>float</code> outside the JIT -&gt; View -&gt; <code>float</code> inside the JIT).</p>"},{"location":"ir_jit/#direct-usage","title":"Direct usage","text":"<p>The JIT is also available as a library with a straightforward interface:</p> <pre><code>absl::StatusOr&lt;Value&gt; RunOnJit(\n    Function* function, absl::Span&lt;const Value&gt; args) {\n  XLS_ASSIGN_OR_RETURN(auto jit, FunctionJit::Create(function));\n  return jit-&gt;Run(args);\n}\n</code></pre> <p>The IR JIT is the default backend for the eval_ir_main tool, which loads IR from disk and runs with args present on either the command line or in a specified file.</p>"},{"location":"ir_jit/#design","title":"Design","text":"<p>Internally, the JIT converts XLS IR to LLVM IR and uses LLVM's ORC infrastructure to convert that into native machine code. The details of compiling an LLVM IR program with ORC are mostly generic and are available online - here are discussed details specific to our usage in XLS.</p> <p>XLS IR is converted to LLVM IR by recursively visiting every node in a function using the DfsVisitor functions. Most nodes have relatively straightforward implementations, e.g., in Concat, we create an empty value with the combined width of the operands, and each is shifted and blitted into that value to produce the result. Some operations, though, merit more discussion.</p>"},{"location":"ir_jit/#arg-passing","title":"Arg passing","text":"<p>When LLVM JIT-compiles a program, the resulting value is simply a function pointer to the requested entry point (that can be called like any function pointer). Calling such a function pointer with concrete-typed arguments, though, is difficult: one must either make heavy [ab]use of C++ templates or \"hide\" the argument types behind an opaque pointer. The latter approach is taken here.</p> <p>When a compiled function is invoked (via <code>FunctionJit::Run()</code>), the typed input args are \"packed\" into an opaque byte buffer which is passed into the new function. Inside there, any references to an argument (via <code>DfsVisitor::HandleParam()</code>) calculate the offset of that param in the opaque buffer and load from there appropriately (this should happen at most once per arg; LLVM and/or XLS should optimize away redundant loads).</p> <p>A special case is for function invocations (inside the JITted function): for these, the arguments already exist inside \"LLVM-space\", so there's no need for unpacking args, so <code>LlvmFunction::getArg()</code> can be used as usual.</p> <p>Results must be handled in a similar way - they could be of any type and will need to be packed inside XLS types before returning, so there's a corresponding argument unpacking phase at function exit.</p> <p>For both packing and unpacking, LLVM's DataLayout must be used to determining where input and output values will be placed, as LLVM will use those conventions when, e.g., loading values from a struct.</p>"},{"location":"ir_jit/#arrayindex","title":"ArrayIndex","text":"<p>IRBuilder provides three means of extracting values from an aggregate type:</p> <ol> <li><code>CreateGEP</code>: these use the getelementptr instruction, which requires a     pointer-typed value (not the same thing as an array!). This requires holding     a value in a specially-created allocation (via <code>CreateAlloca()</code> or in an     input buffer).</li> <li><code>CreateExtractElement</code>: returns the value at a given index in a     vector-typed value.</li> <li><code>CreateExtractValue</code>: returns the value at a given constant index in an     aggregate value.</li> </ol> <p>Unfortunately, #2 doesn't apply, as arrays aren't LLVM vectors, and #3 doesn't apply, as an array index isn't necessarily a constant value. Uniformly managing arrays as allocas doesn't scale well (consider the case of arrays of arrays of tuples...), so for <code>ArrayIndex</code> nodes, we lazily create allocas for only the array of interest and load the requested index from there.</p>"},{"location":"ir_lowering/","title":"XLS: IR Lowering","text":"<ul> <li>XLS: IR Lowering<ul> <li>Flattening<ul> <li>Arrays</li> <li>Tuples</li> <li>Structs</li> </ul> </li> <li>Unrepresented Ops</li> </ul> </li> </ul> <p>As part of codegen, IR constructs are lowered to Verilog/SystemVerilog constructs. In some cases, this lowering is simple and direct, but some IR constructs don't map directly to Verilog constructs.</p>"},{"location":"ir_lowering/#flattening","title":"Flattening","text":"<p>Some XLS types are flattened into simpler types when IR gets lowered to RTL. This flattening is performed here (also see the more-commented header file). The following summarizes how types are flattened.</p>"},{"location":"ir_lowering/#arrays","title":"Arrays","text":"<p>Arrays are flattened such that the last element occupies the most significant bits. Elements are concatenated via the SystemVerilog concatenation operation. This matches the flattening of SystemVerilog packed arrays (e.g. <code>logic [N:0] foo</code>) to unpacked arrays declared like <code>logic bar[N:0]</code>. For example, a 4-element array of 4-bit UInts would be flattened as</p> <pre><code>[0x3, 0x4, 0x5, 0x6] =&gt; 0x6543\n</code></pre>"},{"location":"ir_lowering/#tuples","title":"Tuples","text":"<p>Tuples are flattened by concatenating each leaf element of the tuple. If an element of a tuple is an array, this is equivalent to first flattening the array and treating the flattened array as a leaf element. Concatenation is performed via the SystemVerilog concatenation operation. The zero-th tuple element will end up occupying the most significant bits in the flattened output. For example, a 4-tuple of 4-bit UInts would be flattened as</p> <pre><code>(0x3, 0x4, 0x5, 0x6) =&gt; 0x3456\n</code></pre> <p>and a 2-tuple of length 2 arrays of 4-bit UInts would be flattened as</p> <pre><code>([0x3, 0x4], [0x5, 0x6]) =&gt; 0x4365\n</code></pre>"},{"location":"ir_lowering/#structs","title":"Structs","text":"<p>DSLX structs are lowered to tuples in the IR, so there's no separate handling of structs.</p>"},{"location":"ir_lowering/#unrepresented-ops","title":"Unrepresented Ops","text":"<p>Tokens are not represented in RTL.</p> <p>Entities with zero width are unrepresented in RTL. Where a zero-width value is used, a zero-valued literal can be substituted.</p> <p>Asserts and covers are only represented when producing SystemVerilog, and are unrepresented when producing Verilog.</p>"},{"location":"ir_overview/","title":"XLS: IR Overview","text":"<ul> <li>XLS: IR Overview</li> </ul> <p>Before providing a detailed specification of the IR, in this section we briefly outline the ideas and philosophy behind the IR design and explain how to build, modify, and navigate the IR.</p> <p>The XLS IR is a dataflow-oriented IR that has the static-single-assignment (SSA) property, but is specialized for generating circuitry. It started out as a purely functional IR but over time more and more side-effecting operations had to be introduced. Specifically:</p> <ul> <li>XLS has a single IR representation which is used from the front-end down to     the RTL-level. A single representation throughout the compiler enables     maximal resuse of analysis and transformation components. Often compilers     have different specialized IRs (or \"dialects\") for different levels of     abstraction which can add complexity and inhibit reusability. However, in     XLS this tradeoff between specialization and reusability is unnecessary     because we start with a dataflow representation in the front end and can     smoothly lower the IR down to the RTL-level which is itself dataflow.</li> </ul> <ul> <li>XLS IR is not control-flow graph (CFG) based, as many other compiler     infrastructures. The insight is that the CFG abstraction was developed to     model serial execution on a CPU. In hardware, however, everything happens at     all times and in parallel. A sea-of-nodes (SoN) representation much more     closely resembles this reality, which is why we have chosen it.<p>It further turns out that many optimization passes are rather trivial to   implement in the SoN representation, in particular as it requires no   explicit SSA updates. The SSA property is automatically maintained by the IR   being functional.</p> </li> </ul> <p>TODO: High-level structure, package -&gt; func,proc,block -&gt; sea of nodes</p> <p>TODO: How to navigate</p> <p>TODO: Talk about basic types</p>"},{"location":"ir_semantics/","title":"XLS: IR semantics","text":"<ul> <li>XLS: IR semantics<ul> <li>Data types<ul> <li>Bits</li> <li>Array</li> <li>Tuple</li> <li>Token</li> </ul> </li> <li>Functions, procs, and blocks<ul> <li>Function</li> <li>Proc</li> <li>Block<ul> <li>Port</li> <li>Register</li> <li>Instantiation</li> </ul> </li> </ul> </li> <li>Operations<ul> <li>Unary bitwise operations</li> <li>Variadic bitwise operations</li> <li>Arithmetic unary operations</li> <li>Arithmetic binary operations</li> <li>Comparison operations</li> <li>Shift operations</li> <li>Extension operations<ul> <li>zero_ext</li> <li>sign_ext</li> </ul> </li> <li>Channel operations<ul> <li>receive</li> <li>send</li> </ul> </li> <li>Array operations<ul> <li>array</li> <li>array_index</li> <li>array_update</li> </ul> </li> <li>Tuple operations<ul> <li>tuple</li> <li>tuple_index</li> </ul> </li> <li>Bit-vector operations<ul> <li>bit_slice</li> <li>bit_slice_update</li> <li>dynamic_bit_slice</li> <li>concat</li> <li>reverse</li> <li>decode</li> <li>encode</li> <li>one_hot</li> </ul> </li> <li>Control-oriented operations<ul> <li>param</li> <li>sel</li> <li>one_hot_sel</li> <li>priority_sel</li> <li>invoke</li> <li>map</li> <li>dynamic_counted_for</li> <li>counted_for</li> </ul> </li> <li>Sequencing operations<ul> <li>after_all</li> </ul> </li> <li>Other side-effecting operations<ul> <li>assert</li> <li>cover</li> <li>gate</li> </ul> </li> <li>RTL-level operations<ul> <li>input_port</li> <li>output_port</li> <li>register_read</li> <li>register_write</li> <li>instantiation_input</li> <li>instantiation_output</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>The XLS IR is a pure dataflow-oriented IR that has the static-single-assignment property, but is specialized for generating circuitry. The aim is to create effective circuit designs through a \"lifted\" understanding of the high-level operations and their semantics, instead of trying to reverse all relevant properties via dependence analysis, which often cannot take advantage of high level knowledge that the designer holds in their mind at design time.</p> <p>This document describes the semantics of the XLS intermediate representation (IR) including data types, operations, and textual representation.</p>"},{"location":"ir_semantics/#data-types","title":"Data types","text":""},{"location":"ir_semantics/#bits","title":"Bits","text":"<p>A vector of bits with a fixed width.</p> <p>Type syntax:</p> <p><code>bits[N]</code> where <code>N</code> is the number of bits.</p> <p>Value syntax:</p> <ul> <li>A literal decimal number. Example: <code>42</code>.</li> <li>A binary number prefixed with <code>0b</code>. Example: <code>0b10101</code></li> <li>A hexadecimal number: <code>0x</code>. Example: <code>0xdeadbeef</code></li> </ul> <p>The representation may optionally include the bit width in which case the type is prefixed before the literal: <code>bits[N]:$literal</code>. Example: <code>bits[8]:0xab</code>.</p>"},{"location":"ir_semantics/#array","title":"Array","text":"<p>A one-dimensional array of elements of the same type with a fixed number of elements. An array can contain bits, arrays, or tuples as elements. Empty (zero-element) arrays are not supported.</p> <p>Type syntax:</p> <p><code>$type[N]</code>: an array containing <code>N</code> elements of type <code>$type</code>. Examples:</p> <ul> <li>Two-element array of 8-bit bits type: <code>bits[8][2]</code></li> <li>Three-element array of tuple type: <code>(bits[32], bits[2])[3]</code></li> </ul> <p>Value syntax:</p> <p><code>[$value_1, ... , $value_N]</code> where <code>$value_n</code> is the value of the <code>n</code>-th element. Examples:</p> <ul> <li>Array of bits elements with explicit bit count: <code>[bits[8]:10, bits[8]:30]</code></li> <li>Three-element array consisting of two-element arrays of bits elements: <code>[[1,     2], [3, 4], [5, 6]]]</code></li> </ul>"},{"location":"ir_semantics/#tuple","title":"Tuple","text":"<p>An ordered set of fixed size containing elements with potentially different types. tuples can contain bits, arrays, or tuples as elements. May be empty.</p> <p>Type syntax:</p> <p><code>($type_{0}, ..., $type_{N-1})</code> where <code>N</code> is the number of elements and where <code>$type_n</code> is the type of the <code>n</code>-th element.</p> <p>Value syntax:</p> <p><code>($value_{0}, ..., $value_{N-1})</code> where <code>$value_n</code> is the value of the <code>n</code>-th element. Examples:</p> <ul> <li>Tuple containing two bits elements: <code>(0b100, 0b101)</code></li> <li>A nested tuple containing various element types: <code>((1, 2), 42, [5, 6])</code></li> </ul>"},{"location":"ir_semantics/#token","title":"Token","text":"<p>A type used to enforce ordering between channel operations. The token type has no value and all tokens are identical. A token is purely symbolic / semantic and has no correlate in hardware.</p> <p>Type syntax:</p> <p><code>token</code></p>"},{"location":"ir_semantics/#functions-procs-and-blocks","title":"Functions, procs, and blocks","text":"<p>The XLS IR has three function-level abstractions each which hold a data-flow graph of XLS IR operations: functions, procs, and blocks. Names of function, procs and blocks must be unique among their respective abstractions (functions, procs, and blocks). For example, a block cannot share a name with another block but can share a name with a function.</p>"},{"location":"ir_semantics/#function","title":"Function","text":"<p>A function is a stateless abstraction with a single-output which is computed from zero or more input parameters. May invoke other functions.</p>"},{"location":"ir_semantics/#proc","title":"Proc","text":"<p>A Proc is a stateful abstraction with an arbitrarily-typed recurrent state. Procs can communicate with other procs via channels which (abstractly) are infinite-depth FIFOs with flow control. Channel communication is handled via send and receive IR operations. Procs may invoke functions.</p> <p>TODO(meheff): 2021/11/04 Expand to include more details.</p>"},{"location":"ir_semantics/#block","title":"Block","text":"<p>A Block is an RTL-level abstraction used for code generation. It corresponds to a single Verilog module. Procs and functions are converted to blocks as part of the code generation process. Blocks may \u201cinvoke\u201d other blocks via instantiation. A block includes explicit representations of RTL constructs: ports, registers, and instantiations. The constructs are scoped within the block.</p>"},{"location":"ir_semantics/#port","title":"Port","text":"<p>A port is a representation of an input or output to the block. These correspond to ports on Verilog modules. Ports can be arbitrarily-typed. In the block, each port is represented with a <code>input_port</code> or <code>output_port</code> operation.</p>"},{"location":"ir_semantics/#register","title":"Register","text":"<p>A register is a representation of a hardware register (flop). Registers can be arbitrarily-typed. Each register must have a single <code>register_write</code> and a single <code>register_read</code> operation for writing and reading the register respectively.</p> <p>Each register may optionally specify its reset behavior. The reset can be specified to occur either synchronously or asynchronously and either on the <code>reset</code> signal of the associated <code>register_write</code> being active-high or active-low. If specified the reset value must match the type of the register. If no reset behavior is specified then the <code>reset</code> argument of <code>register_write</code> must be unset.</p>"},{"location":"ir_semantics/#instantiation","title":"Instantiation","text":"<p>An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. The instantiated object can be another block, a FIFO (not yet supported), or a externally defined Verilog module (not yet supported). The instantiation is integrated into the instantiating block with <code>instantiation_input</code> and <code>instantiation_output</code> operations. There is a one-to-one mapping between the instantiation input/output and the ports of the instantiated objects.</p>"},{"location":"ir_semantics/#operations","title":"Operations","text":"<p>Operations share a common syntax and have both positional and keyword arguments \u00e0 la Python. Positional arguments are ordered and must appear first in the argument list. Positional arguments are exclusively the identifiers of the operands. Keyword arguments are unordered and must appear after the positional arguments. Keyword arguments can include arbitrary value types.</p> <pre><code>result = operation(pos_arg_0, ..., pos_arg_N, keyword_0=value0, ..., keyword_M=valueM, ...)\n</code></pre> <p>Common keyword arguments</p> Keyword Type Required Default Description <code>pos</code> <code>SourceLocation</code> no The source location associated with this operation. The syntax is a triplet of comma-separated integer values: <code>Fileno,Lineno,Colno</code>"},{"location":"ir_semantics/#unary-bitwise-operations","title":"Unary bitwise operations","text":"<p>Performs a bit-wise operation on a single bits-typed operand.</p> <p>Syntax</p> <pre><code>result = identity(operand)\nresult = not(operand)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>result</code> <code>bits[N]</code> <p>Operations</p> Operation Opcode Semantics <code>identity</code> <code>Op::kIdentity</code> <code>result = operand</code> <code>not</code> <code>Op::kNot</code> <code>result = ~operand</code>"},{"location":"ir_semantics/#variadic-bitwise-operations","title":"Variadic bitwise operations","text":"<p>Performs a bit-wise operation on one-or-more identically-typed bits operands. If only a single argument is provided the operation is a no-op.</p> <p>Syntax</p> <pre><code>result = and(operand_{0}, ..., operand_{N-1})\nresult = or(operand_{0}, ..., operand_{N-1})\nresult = xor(operand_{0}, ..., operand_{N-1})\n</code></pre> <p>Types</p> Value Type <code>operand_{i}</code> <code>bits[N]</code> <code>result</code> <code>bits[N]</code> <p>Operations</p> Operation Opcode Semantics <code>and</code> <code>Op::kAnd</code> <code>result = lhs &amp; rhs &amp; ...</code> <code>or</code> <code>Op::kOr</code> <code>result = lhs \\| rhs \\| ...</code> <code>xor</code> <code>Op::kXor</code> <code>result = lhs ^ rhs ^ ...</code>"},{"location":"ir_semantics/#arithmetic-unary-operations","title":"Arithmetic unary operations","text":"<p>Performs an arithmetic operation on a single bits-typed operand.</p> <p>Syntax</p> <pre><code>result = neg(operand)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>result</code> <code>bits[N]</code> <p>Operations</p> Operation Opcode Semantics <code>neg</code> <code>Op::kNeg</code> <code>result = -operand</code>"},{"location":"ir_semantics/#arithmetic-binary-operations","title":"Arithmetic binary operations","text":"<p>Performs an arithmetic operation on a pair of bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'.</p> <p>Syntax</p> <pre><code>result = add(lhs, rhs)\nresult = smul(lhs, rhs)\nresult = umul(lhs, rhs)\nresult = sdiv(lhs, rhs)\nresult = smod(lhs, rhs)\nresult = sub(lhs, rhs)\nresult = udiv(lhs, rhs)\nresult = umod(lhs, rhs)\nresult = smulp(lhs, rhs)\nresult = umulp(lhs, rhs)\n</code></pre> <p>Types</p> <p>Currently signed and unsigned multiply, as wells as their partial product variants, support arbitrary width operands and result. For all other arithmetic operations the operands and the result are the same width. The expectation is that all arithmetic operations will eventually support arbitrary widths.</p> <p>Operations</p> Operation Opcode Semantics <code>add</code> <code>Op::kAdd</code> <code>result = lhs + rhs</code> <code>sdiv</code> <code>Op::kSDiv</code> <code>result = $signed(lhs) / $signed(rhs)</code> * ** <code>smod</code> <code>Op::kSMod</code> <code>result = $signed(lhs) % $signed(rhs)</code> * *** <code>smul</code> <code>Op::kSMul</code> <code>result = $signed(lhs) * $signed(rhs)</code> <code>smulp</code> <code>Op::kSMulp</code> <code>result[0] + result[1] = $signed(lhs) * $signed(rhs)</code> **** <code>sub</code> <code>Op::kSub</code> <code>result = lhs - rhs</code> <code>udiv</code> <code>Op::kUDiv</code> <code>result = lhs / rhs</code> * ** <code>umod</code> <code>Op::kUMod</code> <code>result = lhs % rhs</code> * <code>umul</code> <code>Op::kUMul</code> <code>result = lhs * rhs</code> <code>umulp</code> <code>Op::kUMulp</code> <code>result[0] + result[1] = lhs * rhs</code> **** <p>* Synthesizing division or modulus can lead to failing synthesis and/or problems with timing closure. It is usually best not to rely on this Verilog operator in practice, but instead explicitly instantiate a divider of choice.</p> <p>** Division rounds toward zero. For unsigned division this is the same as truncation. If the divisor is zero, unsigned division produces a maximal positive value. For signed division, if the divisor is zero the result is the maximal positive value if the dividend is non-negative or the maximal negative value if the dividend is negative.</p> <p>*** The sign of the result of modulus matches the sign of the left operand. If the right operand is zero the result is zero.</p> <p>**** The partial product multiply variants return a two-element tuple with both elements having the same type. The outputs are not fully constrained; the operations are free to return any values that sum to the product <code>lhs * rhs</code>.</p>"},{"location":"ir_semantics/#comparison-operations","title":"Comparison operations","text":"<p>Performs a comparison on a pair of identically-typed bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Produces a result of bits[1] type.</p> <p>Syntax</p> <pre><code>result = eq(lhs, rhs)\nresult = ne(lhs, rhs)\nresult = sge(lhs, rhs)\nresult = sgt(lhs, rhs)\nresult = sle(lhs, rhs)\nresult = slt(lhs, rhs)\nresult = uge(lhs, rhs)\nresult = ugt(lhs, rhs)\nresult = ule(lhs, rhs)\nresult = ult(lhs, rhs)\n</code></pre> <p>Types</p> Value Type <code>lhs</code> <code>bits[N]</code> <code>rhs</code> <code>bits[N]</code> <code>result</code> <code>bits[1]</code> <p>Operations</p> Operation Opcode Semantics <code>eq</code> <code>Op::kEq</code> <code>result = lhs == rhs</code> <code>ne</code> <code>Op::kNe</code> <code>result = lhs != rhs</code> <code>sge</code> <code>Op::kSGe</code> <code>result = lhs &gt;= rhs</code> <code>sgt</code> <code>Op::kSGt</code> <code>result = lhs &gt; rhs</code> <code>sle</code> <code>Op::kSLe</code> <code>result = lhs &lt;= rhs</code> <code>slt</code> <code>Op::kSLt</code> <code>result = lhs &lt; rhs</code> <code>uge</code> <code>Op::kUGe</code> <code>result = lhs &gt;= rhs</code> <code>ugt</code> <code>Op::kUGt</code> <code>result = lhs &gt; rhs</code> <code>ule</code> <code>Op::kULe</code> <code>result = lhs &lt;= rhs</code> <code>ult</code> <code>Op::kULt</code> <code>result = lhs &lt; rhs</code>"},{"location":"ir_semantics/#shift-operations","title":"Shift operations","text":"<p>Performs an shift operation on an input operand where the shift amount is specified by a second operand.</p> <p>Syntax</p> <pre><code>result = shll(operand, amount)\nresult = shra(operand, amount)\nresult = shrl(operand, amount)\n</code></pre> <p>Types</p> <p>The shifted operand and the result of the shift are the same width. Widths of the shift amount may be arbitrary.</p> <p>Operations</p> Operation Opcode Semantics <code>shll</code> <code>Op::kShll</code> <code>result = lhs &lt;&lt; rhs</code> * <code>shra</code> <code>Op::kShra</code> <code>result = lhs &gt;&gt;&gt; rhs</code> (arithmetic shift right) ** <code>shrl</code> <code>Op::kShrl</code> <code>result = lhs &gt;&gt; rhs</code> * <p>* Logically shifting greater than or equal to the number of bits in the <code>lhs</code> produces a result of zero.</p> <p>** Arithmetic right shifting greater than or equal to the number of bits in the <code>lhs</code> produces a result equal to all of the bits set to the sign of the <code>lhs</code>.</p>"},{"location":"ir_semantics/#extension-operations","title":"Extension operations","text":"<p>Extends a bit value to a new (larger) target bit-length.</p> <p>Syntax</p> <pre><code>result = zero_ext(x, new_bit_count=42)\nresult = sign_ext(x, new_bit_count=42)\n</code></pre> <p>Types</p> Value Type <code>arg</code> <code>bits[N]</code> <code>new_bit_count</code> <code>int64_t</code> <code>result</code> <code>bits[new_bit_count]</code> <p>Note: <code>new_bit_count</code> should be <code>&gt;= N</code> or an error may be raised.</p>"},{"location":"ir_semantics/#zero_ext","title":"<code>zero_ext</code>","text":"<p>Zero-extends a value: turns its bit-length into the new target bit-length by filling zeroes in the most significant bits.</p>"},{"location":"ir_semantics/#sign_ext","title":"<code>sign_ext</code>","text":"<p>Sign-extends a value: turns its bit-length into the new target bit-length by filling in the most significant bits (MSbs) with the following policy:</p> <ul> <li>ones in the MSbs if the MSb of the original value was set, or</li> <li>zeros in the MSbs if the MSb of the original value was unset.</li> </ul>"},{"location":"ir_semantics/#channel-operations","title":"Channel operations","text":"<p>These operations send or receive data over channels. Channels are monomorphic, and each channel supports a fixed set of data types which are sent or received in a single transaction.</p>"},{"location":"ir_semantics/#receive","title":"<code>receive</code>","text":"<p>Receives a data value from a specified channel. The type of the data value is determined by the channel. An optional predicate value conditionally enables the receive operation. An optional <code>blocking</code> attribute determines whether the receive operation is blocking. A blocking receive waits (or blocks) until valid data is present at the channel. Compared to a blocking receive, a non-blocking receive has an additional entry in its return tuple of type <code>bits[1]</code> denoting whether the data read is valid.</p> <pre><code>result = receive(tkn, predicate=&lt;pred&gt;, blocking=&lt;bool&gt;, channel_id=&lt;ch&gt;)\n</code></pre> <p>Types</p> Value Type <code>tkn</code> <code>token</code> <code>pred</code> <code>bits[1]</code> <code>result</code> <code>(token, T)</code> if <code>blocking</code> == <code>true</code> else <code>(token, T, bits[1])</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>predicate</code> <code>bits[1]</code> no A value is received iff <code>predicate</code> is true <code>blocking</code> <code>bool</code> no <code>true</code> Whether the receive is blocking <code>channel_id</code> <code>int64_t</code> yes The ID of the channel to receive data from <p>If the predicate is false the data values in the result are zero-filled.</p>"},{"location":"ir_semantics/#send","title":"<code>send</code>","text":"<p>Sends data to a specified channel. The type of the data values is determined by the channel. An optional predicate value conditionally enables the send operation.</p> <pre><code>result = send(tkn, data, predicate=&lt;pred&gt;, channel_id=&lt;ch&gt;)\n</code></pre> <p>Types</p> Value Type <code>tkn</code> <code>token</code> <code>data</code> <code>T</code> <code>pred</code> <code>bits[1]</code> <code>result</code> <code>token</code> <p>The type of <code>data</code> must match the type supported by the channel.</p> <p>Keyword arguments</p> Keyword Type Required Default Description <code>predicate</code> <code>bits[1]</code> no A value is sent iff <code>predicate</code> is true <code>channel_id</code> <code>int64_t</code> yes The ID of the channel to send data to."},{"location":"ir_semantics/#array-operations","title":"Array operations","text":""},{"location":"ir_semantics/#array_1","title":"<code>array</code>","text":"<p>Constructs an array of its operands.</p> <pre><code>result = array(operand_{0}, ..., operand_{N-1})\n</code></pre> <p>Types</p> Value Type <code>operand_{i}</code> <code>T</code> <code>result</code> <code>T[N]</code> <p>Array can take an arbitrary number of operands including zero (which produces an empty array). The n-th operand becomes the n-th element of the array.</p>"},{"location":"ir_semantics/#array_index","title":"<code>array_index</code>","text":"<p>Returns a single element from an array.</p> <p>Syntax</p> <pre><code>result = array_index(array, indices=[idx_{0}, ... , idx_{N-1}])\n</code></pre> <p>Types</p> Value Type <code>array</code> Array of at least <code>N</code> dimensions <code>idx_{i}</code> Arbitrary bits type <code>result</code> <code>T</code> <p>Returns the element of <code>array</code> indexed by the indices <code>idx_{0} ... idx_{N-1}</code>. The array must have at least as many dimensions as number of index elements <code>N</code>. Each element <code>idx_{i}</code> indexes a dimension of <code>array</code>. The first element <code>idx_{0}</code> indexes the outer most dimension, the second element <code>idx_{1}</code> indexes the second outer most dimension, etc. The result type <code>T</code> is the type of <code>array</code> with the <code>N</code> outer most dimensions removed.</p> <p>Any out-of-bounds indices <code>idx_{i}</code> are clamped to the maximum in bounds index for the respective dimension.</p> <p>The table below shows examples of the result type <code>T</code> and the result expression assuming input array operand <code>A</code>.</p> Indices Array type result type <code>T</code> Result expression <code>{1, 2}</code> <code>bits[3][4][5]</code> <code>bits[3]</code> <code>A[1][2]</code> <code>{10, 2}</code> <code>bits[3][4][5]</code> <code>bits[3]</code> <code>A[4][2]</code> (first index is out-of-bounds and clamped at the maximum index) <code>{1}</code> <code>bits[3][4][5]</code> <code>bits[3][4]</code> <code>A[1]</code> <code>{}</code> <code>bits[3][4][5]</code> <code>bits[3][4][5]</code> <code>A</code> <code>{}</code> <code>bits[32]</code> <code>bits[32]</code> <code>A</code>"},{"location":"ir_semantics/#array_update","title":"<code>array_update</code>","text":"<p>Returns a modified copy of an array.</p> <p>Syntax</p> <pre><code>result = array_update(array, value, indices=[idx_{0}, ... , idx_{N-1}])\n</code></pre> <p>Types</p> Value Type <code>array</code> Array of at least <code>N</code> dimensions <code>value</code> <code>T</code> <code>idx_{i}</code> Arbitrary bits type <code>result</code> Same type as <code>array</code> <p>Returns a copy of the input array with the element at the given indices replaced with the given value. If any index is out of bounds, the result is identical to the input <code>array</code>. The indexing semantics is identical to <code>array_index</code> with the exception of out-of-bounds behavior.</p>"},{"location":"ir_semantics/#tuple-operations","title":"Tuple operations","text":""},{"location":"ir_semantics/#tuple_1","title":"<code>tuple</code>","text":"<p>Constructs a tuple of its operands.</p> <pre><code>result = tuple(operand_{0}, ..., operand_{N-1})\n</code></pre> <p>Types</p> Value Type <code>operand_{i}</code> <code>T_{i}</code> <code>result</code> <code>(T_{0}, ... , T_{N-1})</code> <p>Tuple can take and arbitrary number of operands including zero (which produces an empty tuple).</p>"},{"location":"ir_semantics/#tuple_index","title":"<code>tuple_index</code>","text":"<p>Returns a single element from a tuple-typed operand.</p> <p>Syntax</p> <pre><code>result = tuple_index(operand, index=&lt;index&gt;)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>(T_{0}, ... , T_{N-1})</code> <code>result</code> <code>T_{&lt;index&gt;}</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>index</code> <code>int64_t</code> yes Index of tuple element to produce"},{"location":"ir_semantics/#bit-vector-operations","title":"Bit-vector operations","text":""},{"location":"ir_semantics/#bit_slice","title":"<code>bit_slice</code>","text":"<p>Slices a contiguous range of bits from a bits-typed operand.</p> <p>Syntax</p> <pre><code>result = bit_slice(operand, start=&lt;start&gt;, width=&lt;width&gt;)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>result</code> <code>bits[&lt;width&gt;]</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>start</code> <code>int64_t</code> yes The starting bit of the slice. <code>start</code> is is zero-indexed where zero is the least-significant bit of the operand. <code>width</code> <code>int64_t</code> yes The width of the slice. <p>The bit-width of <code>operand</code> must be greater than or equal to <code>&lt;start&gt;</code> plus <code>&lt;width&gt;</code>.</p>"},{"location":"ir_semantics/#bit_slice_update","title":"<code>bit_slice_update</code>","text":"<p>Replaces a contiguous range of bits in a bits-typed operand at a variable start index with a given value.</p> <p>Syntax</p> <pre><code>result = bit_slice_update(operand, start, update_value)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>start</code> <code>bits[I]</code> <code>update_value</code> <code>bits[M]</code> <code>result</code> <code>bits[N]</code> <p>Evaluates to <code>operand</code> with the contiguous <code>M</code> bits starting at index <code>start</code> replaced with <code>update_value</code>. Out-of-bound bits (which occur if <code>start + M &gt; N</code>) are ignored. Examples:</p> <code>operand</code> <code>start</code> <code>update_value</code> <code>result</code> <code>bits[16]:0xabcd</code> <code>0</code> <code>bits[8]:0xff</code> <code>bits[16]:0xabff</code> <code>bits[16]:0xabcd</code> <code>4</code> <code>bits[8]:0xff</code> <code>bits[16]:0xaffd</code> <code>bits[16]:0xabcd</code> <code>12</code> <code>bits[8]:0xff</code> <code>bits[16]:0xfbcd</code> <code>bits[16]:0xabcd</code> <code>16</code> <code>bits[8]:0xff</code> <code>bits[16]:0xabcd</code>"},{"location":"ir_semantics/#dynamic_bit_slice","title":"<code>dynamic_bit_slice</code>","text":"<p>Slices a contiguous range of bits from a bits-typed operand, with variable starting index but fixed width. Out-of-bounds slicing is supported by treating all out-of-bounds bits as having value 0.</p> <p>Syntax</p> <pre><code>result = dynamic_bit_slice(operand, start, width=&lt;width&gt;)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>start</code> <code>bits[M]</code> <code>result</code> <code>bits[&lt;width&gt;]</code> <p><code>start</code> can be of arbitrary bit width. It will be interpreted as an unsigned integer.</p> <p>Keyword arguments</p> Keyword Type Required Default Description <code>width</code> <code>int64_t</code> yes The width of the slice."},{"location":"ir_semantics/#concat","title":"<code>concat</code>","text":"<p>Concatenates and arbitrary number of bits-typed operands.</p> <pre><code>result = concat(operand{0}, ..., operand{n-1})\n</code></pre> <p>Types</p> Value Type <code>operand_{i}</code> <code>bits[N_{i}]</code> <code>result</code> <code>bits[Sum(N_{i})]</code> <p>This is equivalent to the verilog concat operator: <code>result = {arg0, ..., argN}</code></p>"},{"location":"ir_semantics/#reverse","title":"<code>reverse</code>","text":"<p>Reverses the order of bits of its operand.</p> <pre><code>result = reverse(operand)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>result</code> <code>bits[N]</code>"},{"location":"ir_semantics/#decode","title":"<code>decode</code>","text":"<p>Implements a binary decoder.</p> <pre><code>result = decode(operand, width=&lt;width&gt;)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>result</code> <code>bits[M]</code> <p>The result width <code>M</code> must be less than or equal to 2**<code>N</code> where <code>N</code> is the operand width.</p> <p>Keyword arguments</p> Keyword Type Required Default Description <code>width</code> <code>int64_t</code> yes Width of the result <p><code>decode</code> converts the binary-encoded operand value into a one-hot result. For an operand value of <code>n</code> interpreted as an unsigned number the <code>n</code>-th result bit and only the <code>n</code>-th result bit is set. The width of the <code>decode</code> operation may be less than the maximum value expressible by the input (2**<code>N</code> - 1). If the encoded operand value is larger than the number of bits of the result the result is zero.</p>"},{"location":"ir_semantics/#encode","title":"<code>encode</code>","text":"<p>Implements a binary encoder.</p> <pre><code>result = encode(operand, width=&lt;width&gt;)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>bits[N]</code> <code>result</code> <code>bits[M]</code> <p>The result width <code>M</code> must be equal to \\(\\(\\lceil \\log_{2} N \\rceil\\)\\).</p> <p><code>encode</code> converts the one-hot operand value into a binary-encoded value of the \"hot\" bit of the input. If the <code>n</code>-th bit and only the <code>n</code>-th bit of the operand is set the result is equal the value <code>n</code> as an unsigned number.</p> <p>If multiple bits of the input are set the result is equal to the logical or of the results produced by the input bits individually. For example, if bit 3 and bit 5 of an <code>encode</code> input are set the result is equal to 3 | 5 = 7.</p> <p>If no bits of the input are set the result is zero.</p>"},{"location":"ir_semantics/#one_hot","title":"<code>one_hot</code>","text":"<p>Produces a bits value with exactly one bit set. The index of the set bit depends upon the input value.</p> <p>Syntax</p> <pre><code>result = one_hot(input, lsb_prio=true)\nresult = one_hot(input, lsb_prio=false)\n</code></pre> <p>Types</p> Value Type <code>input</code> <code>bits[N]</code> <code>result</code> <code>bits[N+1]</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>lsb_prio</code> <code>bool</code> yes Whether the least significant bit (LSb) has priority. <p>For <code>lsb_prio=true</code>: result bit <code>i</code> for <code>0 &lt;= i &lt; N</code> is set in <code>result</code> iff bit <code>i</code> is set in the input and all lower bits <code>j</code> for <code>j &lt; i</code> are not set in the input.</p> <p>For <code>lsb_prio=false</code>: result bit <code>i</code> for <code>N-1 &gt;= i &gt;= 0</code> is set in <code>result</code> iff bit <code>i</code> is set in the input and all higher (more significant) bits <code>j</code> for <code>j &gt; i</code> are not set in the input.</p> <p>For both <code>lsb_prio=true</code> and <code>lsb_prio=false</code>, result bit <code>N</code> (the most significant bit in the output) is only set if no bits in the input are set.</p> <p>Examples:</p> <ul> <li><code>one_hot(0b0011, lsb_prio=true)</code> =&gt; <code>0b00001</code> -- note that an extra MSb has     been appended to the output to potentially represent the \"all zeros\" case.</li> <li><code>one_hot(0b0111, lsb_prio=false)</code> =&gt; <code>0b00100</code>.</li> <li><code>one_hot(0b00, lsb_prio=false)</code> =&gt; <code>0b100</code>.</li> <li><code>one_hot(0b00, lsb_prio=true)</code> =&gt; <code>0b100</code> -- note the output for <code>one_hot</code>     is the same for the all-zeros case regardless of whether <code>lsb_prio</code> is true     or false.</li> </ul> <p>This operation is useful for constructing match or switch operation semantics where a condition is matched against an ordered set of cases and the first match is chosen. It is also useful for one-hot canonicalizing, e.g. as a prelude to counting leading/trailing zeros.</p>"},{"location":"ir_semantics/#control-oriented-operations","title":"Control-oriented operations","text":"<p>For context note that, in XLS, operations are evaluated eagerly in a very general sense: all \"branches\" of computation may be evaluated in full before the result is selected via an operation such as <code>one_hot_sel</code> or <code>sel</code>. This model is amenable to pipeline-like hardware execution, where operations tend to be fixed in some spatial area and operations execute a single function, while interconnect is used for reconfiguration purposes.</p> <p>Towards this eager-evaluation-capable model, operations used within a function are generally not Turing-complete: operations such as <code>counted_for</code> require a finite bound so that they could be implemented using a finite amount of pipeline area. Operations such as <code>dynamic_counted_for</code> are an exception, where that operation will only be possible to use in a time-multiplexed code generation mode, such as the XLS sequential emitter, where arbitrary iteration to some dynamic bound is likely to be possible.</p>"},{"location":"ir_semantics/#param","title":"<code>param</code>","text":"<p>A parameter to the current IR function, which can be used as an operand for operations within the function.</p> <p>Syntax</p> <p>Parameters have a special syntactic form distinct from other nodes, where they are listed directly in the function signature with their type.</p> <pre><code>fn f(x: bits[32]) -&gt; bits[32] {\n  ret identity.2 = identity(x, id=2)\n}\n</code></pre> <p>Types</p> Value Type <code>name</code> <code>str</code> <code>type</code> <code>type</code>"},{"location":"ir_semantics/#sel","title":"<code>sel</code>","text":"<p>Selects between operands based on a selector value.</p> <p>This behaves as if the <code>selector</code> indexes into the values given in <code>cases</code>, providing <code>default</code> if it is indexing beyond the given <code>cases</code>.</p> <p>Syntax</p> <pre><code>result = sel(selector, cases=[case_{0}, ... , case_{N-1}], default=&lt;default&gt;)\n</code></pre> <p>A default value must be provided iff the <code>selector</code> is not the correct width for the <code>cases</code> array. That is, if the number of cases is less than \\(2^{bitwidth(selector)}\\) then a default value must be specified (because it must be well defined what happens when the selector takes on values outside the case range). If the selector is exactly the correct bitwidth a default value must not be provided.</p> <p>Types</p> Value Type <code>selector</code> <code>bits[M]</code> <code>case_{i}</code> <code>T</code> <code>default</code> <code>T</code> <code>result</code> <code>T</code>"},{"location":"ir_semantics/#one_hot_sel","title":"<code>one_hot_sel</code>","text":"<p>Selects between operands based on a one-hot selector, <code>OR</code>-ing all selected cases if more than one case is selected.</p> <p>See <code>one_hot</code> for an example of the one-hot selector invariant. Note that when the selector is not one-hot, this operation is still well defined.</p> <p>Note that when <code>one_hot</code> operations are used to precondition the <code>selector</code> operand to <code>one_hot_sel</code>, the XLS optimizer will try to determine when they are unnecessary and subsequently eliminate them.</p> <p>Syntax</p> <pre><code>result = one_hot_sel(selector, cases=[case_{0}, ... , case_{N-1}])\n</code></pre> <p>Types</p> Value Type <code>selector</code> <code>bits[N]</code> <code>case_{i}</code> <code>T</code> <code>result</code> <code>T</code> <p>The result is the logical OR of all cases <code>case_{i}</code> for which the corresponding bit <code>i</code> is set in the selector. When selector is one-hot this performs a select operation.</p>"},{"location":"ir_semantics/#priority_sel","title":"<code>priority_sel</code>","text":"<p>Selects between operands based on a selector, choosing the highest-priority case if more than one case is selected. Each bit in the selector corresponds to a case, with the least significant bit corresponding to the first case and having the highest priority. If there are no bits in the selector set, no case is selected and the default value of 0 is chosen.</p> <p>See <code>one_hot</code> for an example of the one-hot selector invariant. Note that when the selector is not one-hot, this operation is still well defined.</p> <p>Note that when <code>one_hot</code> operations are used to precondition the <code>selector</code> operand to <code>priority_sel</code>, the XLS optimizer will try to determine when they are unnecessary and subsequently eliminate them.</p> <p>Syntax</p> <pre><code>result = priority_sel(selector, cases=[case_{0}, ... , case_{N-1}])\n</code></pre> <p>Types</p> Value Type <code>selector</code> <code>bits[N]</code> <code>case_{i}</code> <code>T</code> <code>result</code> <code>T</code> <p>The result is the first case <code>case_{i}</code> for which the corresponding bit <code>i</code> is set in the selector. If the selector is known to be one-hot, then the <code>priority_sel()</code> operation is equivalent to a <code>one_hot_sel()</code>.</p>"},{"location":"ir_semantics/#invoke","title":"<code>invoke</code>","text":"<p>Invokes a function. The return value for the invoked function is the result value.</p> <p>Syntax</p> <pre><code>result = invoke(operand_{0}, ... , operand_{N-1}, to_apply=&lt;to_apply&gt;)\n</code></pre> <p>Types</p> Value Type <code>init</code> <code>T</code> <code>result</code> <code>T</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>to_apply</code> <code>string</code> yes Name of the function to use as the loop body"},{"location":"ir_semantics/#map","title":"<code>map</code>","text":"<p>Applies a function to the elements of an array and returns the result as an array.</p> <p>Syntax</p> <pre><code>result = map(operand, to_apply=&lt;to_apply&gt;)\n</code></pre> <p>Types</p> Value Type <code>operand</code> <code>array[T]</code> <code>result</code> <code>array[U]</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>to_apply</code> <code>string</code> yes Name of the function to apply to each element of the operand"},{"location":"ir_semantics/#dynamic_counted_for","title":"<code>dynamic_counted_for</code>","text":"<p>Invokes a dynamic-trip count loop.</p> <p>Syntax</p> <pre><code>result = counted_for(init, trip_count, stride, body=&lt;body&gt;, invariant_args=&lt;inv_args&gt;)\n</code></pre> <p>Types</p> Value Type <code>init</code> <code>T</code> <code>trip_count</code> <code>bits[N], treated as unsigned</code> <code>stride</code> <code>bits[M], treated as signed</code>, <code>result</code> <code>T</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>invariant_args</code> array of yes Names of the invariant operands as the loop body <code>body</code> <code>string</code> yes Name of the function to use as the loop body <p><code>dynamic_counted_for</code> invokes the function <code>body</code> <code>trip_count</code> times, passing loop-carried data that starts with value <code>init</code>. The induction variable is incremented by <code>stride</code> after each iteration.</p> <ul> <li>The first argument passed to <code>body</code> is the induction variable -- presently,     the induction variable always starts at zero and increments by <code>stride</code>     after every trip.</li> <li>The second argument passed to <code>body</code> is the loop-carry data. The return type     of <code>body</code> must be the same as the type of the <code>init</code> loop carry data. The     value returned from the last trip is the result of the <code>counted_for</code>     expression.</li> <li>All subsequent arguments passed to <code>body</code> are passed from <code>invariant_args</code>;     e.g. if there are two members in <code>invariant_args</code> those values are passed as     the third and fourth arguments.</li> </ul> <p>Therefore <code>body</code> should have a signature that matches the following:</p> <pre><code>body(i, loop_carry_data, [invariant_arg0, invariant_arg1, ...])\n</code></pre> <p>Note that we currently inspect the <code>body</code> function to see what type of induction variable (<code>i</code> above) it accepts in order to pass an <code>i</code> value of that type. <code>trip_count</code> must have fewer bits than <code>i</code> and <code>stride</code> should have fewer than or equal number of bits to <code>i</code>.</p> <p>Code generation support for <code>dynamic_counted_for</code> is limited because the pipeline generator cannot handle an unknown trip count.</p>"},{"location":"ir_semantics/#counted_for","title":"<code>counted_for</code>","text":"<p>Invokes a fixed-trip count loop.</p> <p>Syntax</p> <pre><code>result = counted_for(init, trip_count=&lt;trip_count&gt;, stride=&lt;stride&gt;, body=&lt;body&gt;, invariant_args=&lt;inv_args&gt;)\n</code></pre> <p>Types</p> Value Type <code>init</code> <code>T</code> <code>result</code> <code>T</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>trip_count</code> <code>int64_t</code> yes Trip count of the loop (number of times that the loop body will be executed) <code>stride</code> <code>int64_t</code> no 1 Stride of the induction variable <code>invariant_args</code> array of yes Names of the invariant operands as the loop body <code>body</code> <code>string</code> yes Name of the function to use as the loop body <p><code>counted_for</code> invokes the function <code>body</code> <code>trip_count</code> times, passing loop-carried data that starts with value <code>init</code>.</p> <ul> <li>The first argument passed to <code>body</code> is the induction variable -- presently,     the induction variable always starts at zero and increments by <code>stride</code>     after every trip.</li> <li>The second argument passed to <code>body</code> is the loop-carry data. The return type     of <code>body</code> must be the same as the type of the <code>init</code> loop carry data. The     value returned from the last trip is the result of the <code>counted_for</code>     expression.</li> <li>All subsequent arguments passed to <code>body</code> are passed from <code>invariant_args</code>;     e.g. if there are two members in <code>invariant_args</code> those values are passed as     the third and fourth arguments.</li> </ul> <p>Therefore <code>body</code> should have a signature that matches the following:</p> <pre><code>body(i, loop_carry_data[, invariant_arg0, invariant_arg1, ...])\n</code></pre> <p>Note that we currently inspect the <code>body</code> function to see what type of induction variable (<code>i</code> above) it accepts in order to pass an <code>i</code> value of that type.</p>"},{"location":"ir_semantics/#sequencing-operations","title":"Sequencing operations","text":"<p>Some operations in XLS IR are sensitive to sequence order, similar to channel operations, but are not themselves channel-related. Tokens are used to determine the possible sequencing of these effects, and <code>after_all</code> can be used to join together tokens as a sequencing merge point for concurrent threads of execution described by different tokens.</p>"},{"location":"ir_semantics/#after_all","title":"<code>after_all</code>","text":"<p>Used to construct partial orderings among channel operations.</p> <pre><code>result = after_all(operand_{0}, ..., operand_{N-1})\n</code></pre> <p>Types</p> Value Type <code>operand_{i}</code> <code>token</code> <code>result</code> <code>token</code> <p><code>after_all</code> can consume an arbitrary number of token operands including zero.</p>"},{"location":"ir_semantics/#other-side-effecting-operations","title":"Other side-effecting operations","text":"<p>Aside from channels operations such as <code>send</code> and <code>receive</code> several other operations have side-effects. Care must be taken when adding, removing, or transforming these operations, e.g., in the optimizer.</p>"},{"location":"ir_semantics/#assert","title":"<code>assert</code>","text":"<p>Raises an error at software run-time (DSLX/IR interpretation, JIT execution, RTL simulation) if the given condition evaluates to false. The operation takes a literal string attribute which is included in the error message. This is a software-only operation and has no representation in the generated hardware. Tokens are used to connect the operation to the graph and order with respect to other side-effecting operations.</p> <pre><code>result = assert(tkn, condition, message=&lt;string&gt;)\nresult = assert(tkn, condition, message=&lt;string&gt;, label=&lt;string&gt;)\n</code></pre> <p>Types</p> Value Type <code>tkn</code> <code>token</code> <code>condition</code> <code>bits[1]</code> <code>result</code> <code>token</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>message</code> <code>string</code> yes Message to include in raised error <code>label</code> <code>optional string</code> yes Label to associate with the assert statement in the generated (System)Verilog"},{"location":"ir_semantics/#cover","title":"<code>cover</code>","text":"<p>Records the number of times the given condition evaluates to true. Just like <code>assert</code>, this is a software-only construct and is not emitted in a final hardware design. Tokens are used to sequence this operation in the graph.</p> <pre><code>result = cover(tkn, condition, label=&lt;string&gt;)\n</code></pre> <p>Types</p> Value Type <code>tkn</code> <code>token</code> <code>condition</code> <code>bits[1]</code> <code>result</code> <code>token</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>label</code> <code>string</code> yes Name associated with the counter."},{"location":"ir_semantics/#gate","title":"<code>gate</code>","text":"<p>Gates an arbitrarily-typed value based on a condition.</p> <p>The result of the operation is the data operand if the condition is true, otherwise the result is a zero value of the type of the data operand (i.e., the value is gated off). A helpful mnemonic is to think of this as analogous to an <code>AND</code> gate: if the condition is <code>true</code>, the value passes through, otherwise it's zeroed.</p> <p>This operation can reduce switching and may be used in power optimizations. This is intended for use in operand gating for power reduction, and the compiler may ultimately use it to perform register-level load-enable gating.</p> <p>The operation is considered side-effecting to prevent removal of the operation when the gated result (condition is false) is not observable. The 'side-effect' of this operation is the effect it can have on power consumption.</p> <p>Despite being 'side-effecting' this operation is special cased to still be eligible for total removal by various passes. This will only be done in cases where the gate is redundant, for example the condition is known to be false or the data is known to be zero.</p> <pre><code>result = gate(condition, data)\n</code></pre> <p>Types</p> Value Type <code>condition</code> <code>bits[1]</code> <code>data</code> <code>T</code> <code>result</code> <code>T</code>"},{"location":"ir_semantics/#rtl-level-operations","title":"RTL-level operations","text":"<p>These IR operations correspond to RTL-level constructs in the emitted Verilog. These operations are added and used in the code generation process and may only appear in blocks (not procs or functions).</p>"},{"location":"ir_semantics/#input_port","title":"<code>input_port</code>","text":"<p>Corresponds to an input port on a Verilog module.</p> <p>Syntax</p> <pre><code>result = input_port()\n</code></pre> <p>Types</p> Value Type <code>result</code> <code>T</code> <p>An input_port operation can be an arbitrary type.</p>"},{"location":"ir_semantics/#output_port","title":"<code>output_port</code>","text":"<p>Corresponds to an output port on a Verilog module. The value sent to the output port is the data operand.</p> <p>Syntax</p> <pre><code>result = output_port(data)\n</code></pre> <p>Types</p> Value Type <code>data</code> <code>T</code> <code>result</code> <code>T</code>"},{"location":"ir_semantics/#register_read","title":"<code>register_read</code>","text":"<p>Reads a value from a register.</p> <p>The register is defined on the block.</p> <p>Syntax</p> <pre><code>result = register_read(register=&lt;register_name&gt;)\n</code></pre> <p>Types</p> Value Type <code>result</code> <code>T</code> <p>The type <code>T</code> of the result of the operation is the type of the register.</p> <p>Keyword arguments</p> Keyword Type Required Default Description <code>register</code> <code>string</code> yes Name of the register to read"},{"location":"ir_semantics/#register_write","title":"<code>register_write</code>","text":"<p>Writes a value to a register.</p> <p>The write to the register may be conditioned upon an optional load-enable and/or reset signal. The register is defined on the block.</p> <p>If <code>reset</code> is given the <code>register</code> associated with this read must have a reset behavior set.</p> <p>If the <code>reset</code> value matches the reset-active value of the register then the <code>reset_value</code> of the register is written and the <code>data</code> is ignored.</p> <p>If the <code>load_enable</code> argument is present the register will only be written if the argument evaluates to <code>1</code>, remaining unchanged otherwise (i.e. if present it is equivalent to <code>register_write.REG(sel(load_enable, {register_read.REG, data}))</code>).</p> <p>The <code>reset</code> and <code>load_enable</code> arguments affect the value written according to the following table.</p> <p>| <code>Register</code> reset      | <code>reset</code> value | <code>load_enable</code> value | new value     | {.sortable} : behavior              :               :                     :               : | --------------------- | ------------- | ------------------- | ------------- | | <code>active_low == false</code> | <code>false</code> / <code>0</code> | not present         | <code>data</code>        | | <code>active_low == false</code> | <code>true</code> / <code>1</code>  | not present         | <code>reset_value</code> | | <code>active_low == true</code>  | <code>false</code> / <code>0</code> | not present         | <code>reset_value</code> | | <code>active_low == true</code>  | <code>true</code> / <code>1</code>  | not present         | <code>data</code>        | | <code>active_low == false</code> | <code>false</code> / <code>0</code> | <code>true</code> / <code>1</code>        | <code>data</code>        | | <code>active_low == false</code> | <code>true</code> / <code>1</code>  | <code>true</code> / <code>1</code>        | <code>reset_value</code> | | <code>active_low == true</code>  | <code>false</code> / <code>0</code> | <code>true</code> / <code>1</code>        | <code>reset_value</code> | | <code>active_low == true</code>  | <code>true</code> / <code>1</code>  | <code>true</code> / <code>1</code>        | <code>data</code>        | | <code>active_low == false</code> | <code>false</code> / <code>0</code> | <code>false</code> / <code>0</code>       | No change     | | <code>active_low == false</code> | <code>true</code> / <code>1</code>  | <code>false</code> / <code>0</code>       | <code>reset_value</code> | | <code>active_low == true</code>  | <code>false</code> / <code>0</code> | <code>false</code> / <code>0</code>       | <code>reset_value</code> | | <code>active_low == true</code>  | <code>true</code> / <code>1</code>  | <code>false</code> / <code>0</code>       | No change     | | not present           | not present   | <code>true</code> / <code>1</code>        | <code>data</code>        | | not present           | not present   | <code>false</code> / <code>0</code>       | No change     |</p> <p>Syntax</p> <pre><code>result = register_write(data, load_enable=&lt;load_enable&gt;, reset=&lt;reset&gt;, register=&lt;register_name&gt;)\n</code></pre> <p>Types</p> Value Type <code>data</code> <code>T</code> <code>load_enable</code> <code>bits[1]</code> (optional) <code>reset</code> <code>bits[1]</code> (optional) <code>result</code> <code>()</code> (empty tuple) <p>Keyword arguments</p> Keyword Type Required Default Description <code>register</code> <code>string</code> yes Name of the register to write <p>The type <code>T</code> of the data operand must be the same as the the type of the register.</p>"},{"location":"ir_semantics/#instantiation_input","title":"<code>instantiation_input</code>","text":"<p>Corresponds to a single input port of an instantiation.</p> <p>An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. Each <code>instantation_input</code> operation corresponds to a particular port of the instantiated object, so generally a single instantiation can have multiple associated <code>instantiation_input</code> operations (one for each input port).</p> <p>Syntax</p> <pre><code>result = instantiation_input(data, instantiation=&lt;instantiation&gt;, port_name=&lt;port_name&gt;)\n</code></pre> <p>Types</p> Value Type <code>data</code> <code>T</code> <code>result</code> <code>()</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>instantiation</code> <code>string</code> yes Name of the instantiation. <code>port_name</code> <code>string</code> yes Name of the associated port of the instantiation. <p>The type <code>T</code> of the data operand must be the same as the the type of the associated input port of the instantiated object.</p>"},{"location":"ir_semantics/#instantiation_output","title":"<code>instantiation_output</code>","text":"<p>Corresponds to a single output port of an instantiation.</p> <p>An instantiation is a block-scoped construct that represents a module instantiation at the Verilog level. Each <code>instantation_output</code> operation corresponds to a output particular port of the instantiated object, so generally a single instantiation can have multiple associated <code>instantiation_output</code> operations (one for each output port).</p> <p>Syntax</p> <pre><code>result = instantiation_output(instantiation=&lt;instantiation&gt;, port_name=&lt;port_name&gt;)\n</code></pre> <p>Types</p> Value Type <code>result</code> <code>T</code> <p>Keyword arguments</p> Keyword Type Required Default Description <code>instantiation</code> <code>string</code> yes Name of the instantiation. <code>port_name</code> <code>string</code> yes Name of the associated port of the instantiation. <p>The type <code>T</code> of the result is type of the associated output port of the instantiated object.</p>"},{"location":"ir_visualization/","title":"IR Visualization","text":"<p>The XLS IR visualization web app presents the IR in text and graphical form side-by-side and enables interactive exploration of the IR.</p>"},{"location":"ir_visualization/#running-the-web-app","title":"Running the web app","text":"<p>To build and launch the IR visualization web app run:</p> <pre><code>bazel run -c opt //xls/visualization/ir_viz:app -- --delay_model=unit\n</code></pre> <p>Then visit http://localhost:5000 in a browser.</p>"},{"location":"ir_visualization/#screenshot","title":"Screenshot","text":"<p>The screenshot below shows a zoomed-in portion of the IR graph for the <code>fp_adder</code> benchmark. The highlighted path in blue is the timing critical path the through the graph.</p> <p></p>"},{"location":"ir_visualization/#usage","title":"Usage","text":""},{"location":"ir_visualization/#text-ir","title":"Text IR","text":"<p>The left hand side of the UI shows the IR in text form in an editable text box. The IR may be entered or loaded in several ways:</p> <ul> <li>Upload from a file on the local file system via the Upload button.</li> </ul> <ul> <li>Enter directly by typing in the text box or cut and pasting.</li> </ul> <ul> <li>Load a pre-compiled benchmark via the Benchmarks button. The IR is from     the benchmark after optimizations.</li> </ul> <p>The text IR is parsed as you type. The result of the parse (OK or an error) appears in an alert at the bottom of the text box. On successful parsing all identifiers in the IR will be shown in bold.</p>"},{"location":"ir_visualization/#ir-graph","title":"IR graph","text":"<p>The right-hand side of the UI shows the IR in graphical form. Clicking on the View Graph button renders the text IR on the left hand side as a graph. The View Graph button is enabled only if the IR is parsed successfully. The graph view may be manipulated as follows:</p> <ul> <li>Zoom The mouse scroll wheel zooms the view of the IR graph.</li> </ul> <ul> <li>Pan Clicking and holding the left mouse button down in the graph panel     (while not on a graph element) and moving the mouse pans the graph.</li> </ul> <ul> <li>Moving nodes Nodes in the graph are moved by clicking and holding on     the node and moving the mouse.</li> </ul> <ul> <li>Focusing on nodes Clicking on a node in the graph while holding down     the control key scrolls the respective definition of the node in the text IR     into view in the text box. Similarly, control clicking on an identifier in     the text IR zooms and centers the graph view on the respective node.</li> </ul>"},{"location":"ir_visualization/#node-colors","title":"Node colors","text":"<p>Every node in the graph is assigned a color on a spectrum from white (<code>#FFFFFF</code>) to red (<code>#FF0000</code>) depending on the modeled latency of the operation. The nodes with the longest latency in the graph are assigned red. Nodes with zero latency are assigned white.</p>"},{"location":"ir_visualization/#hovering-on-ir-elements","title":"Hovering on IR elements","text":"<p>Hovering on nodes and edges in the graph highlights the corresponding element in the text IR and vice versa. In the text IR, the definition and all uses of the IR value are highlighted when a node is highlighted. When a graph edge is highlighted, the definition and corresponding use are highlighted in the text IR.</p> <p>Information about a highlighted node (identifier in text IR) is displayed in a box above the IR graph. This information includes:</p> <ul> <li>The definition of the IR value in text form.</li> </ul> <ul> <li>Estimate of the delay in picoseconds of the corresponding operation. The     delay estimation methodology is described here.</li> </ul> <ul> <li>Any known bits of the value as determined by the query engine     (https://github.com/google/xls/tree/main/xls/passes/query_engine.h).</li> </ul>"},{"location":"ir_visualization/#selecting-nodes","title":"Selecting nodes","text":"<p>Nodes in the graph may be in a selected or deselected state. Clicking on a node in the graph or identifier in the text IR toggles the selection state. A selected node (identifier in IR text) is shown with a blue border. Nodes and edges which are neighbors of selected nodes (the selection frontier) are shown in orange.  Clicking on an empty area of the graph deselects all nodes.</p>"},{"location":"ir_visualization/#showing-only-selected-nodes","title":"Showing only selected nodes","text":"<p>The toggle Show only selected nodes controls whether to show the entire graph or only the selected node and those elements in the selection frontier. Showing only selected nodes can be used to display only a subgraph of interest. For large graphs which are slow to render in their entirety, this mechanism can be used to interactively explore parts of the graph.</p> <p>When showing only selected nodes, the graph maybe be expanded by selecting additional nodes to add to the graph. The graph is re-rendered to include the newly selected node. Similarly, nodes may be removed from the graph by deselecting nodes.</p>"},{"location":"ir_visualization/#selecting-the-critical-path","title":"Selecting the critical path","text":"<p>The button Critical Path selects exactly those nodes which are on the critical path as determined by XLS's timing model. This may be used with the Show only selected nodes toggle to show a graph containing only critical path elements and neighbors. In the screenshot above, the selected critical path is shown in blue.</p>"},{"location":"scheduling/","title":"XLS Pipeline scheduling","text":"<ul> <li>XLS Pipeline scheduling<ul> <li>Scheduling process<ul> <li>Step 1: determine the effective clock period</li> <li>Step 2: schedule to minimize pipeline registers</li> <li>Options for common scheduling objectives</li> </ul> </li> <li>Minimizing pipeline registers via SDC scheduling<ul> <li>Constraints</li> <li>Additional technical details</li> </ul> </li> </ul> </li> </ul> <p>Pipeline scheduling divides the IR nodes of an XLS function or proc into a sequence of stages constituting a feed-forward pipeline. Sequential stages are separated by registers enabling pipeline parallelism. The schedule must satisfy dependency constraints between XLS nodes as well as timing constraints imposed by the target clock frequency. Pipeline scheduling has multiple competing optimization objectives: minimize number of stages (minimize pipeline latency), minimize maximum delay of any stage (maximize clock frequency), and minimize the number of pipeline registers.</p>"},{"location":"scheduling/#scheduling-process","title":"Scheduling process","text":"<p>Pipeline scheduling occurs in two phases:</p> <ol> <li> <p>Determine the effective clock period. This clock period defines the maximum     delay, based on XLS's internal delay model, through     any pipeline stage and limits how many IR operations might be placed in each     stage.</p> </li> <li> <p>Given the constraints of the effective clock period and, optionally, a     user-defined number of pipeline stages, find the schedule which minimizes     the number of pipeline registers. Pipeline registers are required for any IR     operation whose value which is used in a later stage.</p> </li> </ol> <p>The schedule process is controlled via several options defined here. These options are typically passed in as flags to the <code>codegen_main</code> binary but maybe set programmatically. Each is optional though at least one of clock period or pipeline stages must be specified. Different combinations of options result in different strategies as described below.</p> <p>Clock period :   The target clock period.</p> <p>Pipeline stages :   The number of stages in the pipeline.</p> <p>Clock margin percent :   The percentage to reduce the target clock period before scheduling. May only     be specified with clock period. This option is equivalent to specifying     a reduced value for clock period.</p> <p>Clock period relaxation percent :   This is the percentage that the computed minimum clock period, as determined     by the number of pipeline stages, is increased (relaxed) prior to     scheduling. May not be specified with clock period.</p>"},{"location":"scheduling/#step-1-determine-the-effective-clock-period","title":"Step 1: determine the effective clock period","text":"<p>The effective clock period determines the maximum delay through any pipeline stage for the purpose of scheduling. The value is determined in one of two ways depending upon whether the clock period option is specified.</p> <ol> <li> <p>clock period specified</p> <p>The effective clock period is set the clock period value. If clock   margin percent is also specified, then the effective clock period is also   reduced by the given percentage. Example: if clock period is 800ps and   clock margin percent is 20% then the effective clock period is 640ps.</p> </li> <li> <p>clock period not specified</p> <p>In this case, pipeline stages must be specified. The effective clock   period is computed as the minimum clock period in which a schedule may be   found that meets timing with the specified number of pipeline stages. This   is done via a binary search through clock period values, where at each step   of the binary search the scheduler is run in its entirety. If clock period   relaxation percent is specified then the computed effective clock period   is increased by the given percentage. The motivation is that this   relaxation may result in fewer pipeline registers because of increased   scheduling flexibility. Example: if the minimum clock period found by XLS   was 1000ps and clock period relaxation percent is 10% the effective   clock period is 1100ps.</p> </li> </ol>"},{"location":"scheduling/#step-2-schedule-to-minimize-pipeline-registers","title":"Step 2: schedule to minimize pipeline registers","text":"<p>Once an effective clock period is determined, XLS computes a schedule which minimizes the number of registers (see below for details) while satisfying various constraints, including the critical path delay constraints imposed by the effective clock period. The number of stages in the pipeline may be specified by the user via the pipeline stages option. If the number of pipeline stages specified is too small an error such that no feasible schedule can be found then an error is returned. If pipeline stages is not given then the minimum number of stages which meets the delay constraint imposed by the effective clock period is used.</p>"},{"location":"scheduling/#common-options","title":"Options for common scheduling objectives","text":"<p>Different scheduling options result in different optimization strategies for the scheduler. Below are several common scheduling objectives and options which should be set to enable them.</p> <ol> <li> <p>Minimize the number of pipeline registers for a given clock period and given     number of pipeline stages.</p> <p>Specify both clock period and pipeline stages. The scheduler will   attempt to minimize the number of pipeline registers given those   constraints. The option clock margin percent can be swept to search the   local design space (or equivalently, sweep clock period)</p> </li> <li> <p>Minimize the clock period for a given number of pipeline stages</p> <p>Specify only pipeline stages. XLS will find a schedule with minimum   clock period with a secondary objective of minimizing the number of pipeline   registers. Sweeping clock period relaxation percent explores relaxing   the timing constraint which may result in fewer pipeline registers.</p> </li> <li> <p>Minimize the number of pipeline stages for a given clock period</p> <p>Specify only clock period. XLS will find a schedule of the minimum   number of stages with a secondary objective of minimizing the number of   pipeline registers. The option clock margin percent can be swept to   search the local design space (or equivalently, sweep clock period)</p> </li> <li> <p>Minimize the number of pipeline registers for a given clock period</p> <p>Specify only clock period and sweep pipeline stages. Pick the   schedule which produces the minimum number of pipeline registers.</p> </li> <li> <p>Sweep the entire scheduling space</p> <p>The various options directly or indirectly control the two degrees of   freedom within the scheduler: pipeline stages and clock period. Sweeping   these two degrees of freedom is most easily done by sweeping pipeline   stages and clock period relaxation percent. The advantage of sweeping   clock period relaxation percent instead of clock period directly is   that the percent relaxation can be a fixed range (e.g., 0 to 50%) for all   designs and each value will produce a feasible schedule. If clock period   is swept some combinations of pipeline stages* and clock period** values   will result in an error returned because the design point is infeasible.</p> </li> </ol>"},{"location":"scheduling/#sdc","title":"Minimizing pipeline registers via SDC scheduling","text":"<p>For scheduling pipelines, XLS uses a variation on the approach described in SDC-Based Modulo Scheduling for Pipeline Synthesis. The basic principle is to create a set of real-valued variables, each corresponding to the cycle in which a node is scheduled or the lifetime<sup>1</sup> of a node, and then carefully constrain the variables using linear inequality constraints such that minimizing a linear objective always gives an answer with integer values for all the variables. This avoids the need for integer linear programming, which is NP complete, and instead can be solved with linear programming, which is polynomial time in theory and takes roughly cubic time in practice.</p> <p>Prior to the implementation of the SDC scheduler, we used a scheduler based on taking the min-cut of the node graph with Ford-Fulkerson. However, this design proved difficult to extend with needed features like IO constraints, and unlike the SDC algorithm was not optimal in the particular, narrow, sense that it assigns nodes to cycles such that the required register bits are minimized. We found that switching from the min-cut algorithm to SDC resulted in marginal improvements to benchmarks and increased compile times by an small and acceptable amount.</p>"},{"location":"scheduling/#constraints","title":"Constraints","text":"<p>Currently, we generate a variety of constraints:</p> <ul> <li>Causality constraints, i.e.: if node Y uses the output of node X, then the     cycle of node X must be less than or equal to the cycle of node Y.</li> <li>Timing constraints, i.e.: if the critical path between node X and node Y is     greater than the clock period, then the cycle of Y must be strictly greater     than the cycle of X.</li> <li>IO constraints among sends and receives on a given channel (see the codegen     documentation for more details).</li> <li>\"Node in cycle\" constraints, which allow forcing a given node to be     scheduled in a given cycle. This is useful for incremental scheduling in the     scheduling pass pipeline.</li> <li>\"Receives first, sends last\" constraints, which allow accessing the old     behavior in which receives all went into the first cycle and sends all went     into the last cycle.</li> <li>Backedge constraints: when the initiation interval is 1, a state parameter     and its corresponding next state must be scheduled in the same cycle. More     generally, we build up a graph of states where there is an edge between two     states if the output of one affects the input of another, and then compute     the strongly connected components of this graph. All nodes within a strongly     connected component must be in the same cycle.</li> </ul>"},{"location":"scheduling/#additional-technical-details","title":"Additional technical details","text":"<p>The linear inequality constraints can be summarized by a matrix M and a vector y such that Mx \u2264 y. If the linear program has the property described above (that minimizing a linear objective gives an integer answer), then the matrix M is considered to be integral. One class of integral matrices is that of the \"totally unimodular matrices\". The exact definition of this class is out of scope to discuss here, but it suffices to say that it includes constraints of the following form:</p> <ul> <li>Difference constraints between variables with an integer bound: <code>x - y \u2264 k</code>     where <code>k</code> is an integer</li> <li>Constraints of the form <code>x - y - z \u2264 k</code> where <code>k</code> is an integer</li> </ul> <p>In the SDC scheduler, we use <code>x - y \u2264 k</code> constraints to express causality and timing constraints, whereas <code>x - y - z \u2264 k</code> constraints are used to constrain the lifetime variables to be equal to the difference between the max user cycle and the cycle of a given node.</p> <ol> <li> <p>The lifetime of a node is the interval starting at the cycle number assigned to the node and ending at the maximum cycle number of the users of the node.\u00a0\u21a9</p> </li> </ol>"},{"location":"solvers/","title":"XLS Solvers","text":"<p>Programs that are represented as optimized XLS IR are converted into circuits based on boolean logic, and so it is also possible to feed those as logical operations to a theorem prover.</p> <p>We have implemented that conversion with the Z3 theorem prover using its \"bit vector\" type support. As a result, you can conceptually ask Z3 to prove any predicate that can be expressed as XLS, over all possible parameter inputs.</p> <p>See the tools documentation for usage information on related command line tools.</p>"},{"location":"solvers/#applications","title":"Applications","text":"<p>This facility is expected to be useful to augment random testing. While profiling the values in an XLS IR function that is given random stimulus, we may observe bits that result from nodes that appear to be constant (but are not created via a \"literal\" or a \"concat\" of a literal).</p> <p>Example: Say the value resulting from <code>and.1234</code> in the graph appears to be constant zero with all the stimulus provided via a fuzzer thus far -- the solver provides a facility whereby we can ask \"is there a counterexample to <code>and.1234</code> always being zero?\" and the solver will either say \"no, it is always zero\", or it will yield a counterexample, or will not terminate within the allocated deadline.</p> <p>Assuming we can prove useful properties in a reasonable amount of time, we can use this proof capability to help find interesting example inputs that provide unique stimulus.</p>"},{"location":"solvers/#correctness-wrt-reference-32-bit-floating-point-adder","title":"Correctness WRT reference: 32-bit Floating-Point Adder","text":"<p>The full input space for a 32-bit adder is a whopping 64 bits - far more than is possible to exhaustively test for correctness. Proving correctness via Z3, however, is relatively straightforward: at a high level, one simply compares the output from the DSLX (translated into Z3) to the same operation performed solely in Z3.</p> <p>In detail, the steps are:</p> <ol> <li>Translate the DSLX implementation into Z3 via     <code>Z3Translator::CreateAndTranslate()</code>.</li> <li>Create a Z3 implementation of the same addition. This is nearly trivial, as     Z3 helpfully has built-in support for floating-point values and theories.</li> <li>Take the result nodes from each \"branch\" above and create a new node     subtracting the two. This is the absolute error. Note: Usually, one is     interested in relative error when working with FP values, but here, our     target is absolute equivalence, so absolute error sufficies (and is     simpler).</li> <li>Create a Z3 node comparing that error to the maximum bound (here 0.0f).</li> <li>Feed that error node into a Z3 solver, asking it to prove that the error     could be greater than that bound.</li> </ol> <p>If the solver can not satisfy that criterion, then that means the error is never greater than that bound, i.e., that the implementations are equivalent (with our 0.0f bound).</p>"},{"location":"solvers/#ir-transform-validity","title":"IR Transform validity","text":"<p>It's usually not possible (or is merely extremely difficult) to write tests to prove that an optimization/transform is safe across all input IR. By comparing the optimized vs. unoptimized IR in a similar manner as the correctness proof above, we can symbolically prove safety.</p> <p>The only difference between this and the correctness proof is that both the optimized and unoptimized IR need to be fed into the same Z3Translator (the second via <code>Z3Translator::AddFunction()</code>) and the result nodes each are used in the error comparison.</p>"},{"location":"solvers/#ir-to-netlist-logical-equivalence-checking-lec","title":"IR to netlist Logical Equivalence Checking (LEC)","text":"<p>After a user design has been lowered to IR, it is optimized (see the previous section), then Verilog is generated for that optimized IR. That Verilog is then compiled by an external tool, which, if successful, will output a \"netlist\" - a set of standard cells (think AND, OR, NOT, flops, etc.) and wires connecting them that realizes the design.</p> <p>Between the IR level and that netlist, many, many transformations are applied to the design. Before processing the netlist further - and certainly before sending the final design to fabrication - it's a very good idea to ensure that the netlist describes the correct logic!</p> <p>Demonstrating initial design correctness is up to the user, via unit tests or integration tests at the DSLX level. At all stages below that, though, ensuring logical equivalence between forms is XLS' responsibility. To prove equivalence between the IR and netlist forms of a design, XLS uses formal verification via solvers - currently only Z3, above.</p> <p>Performing IR-to-netlist LEC is very similar to the checking above - the source IR is one half of the comparison. Here, the second half is the netlist translated into IR, which only requires a small amount of extra work. Consider the snippet below:</p> <pre><code>FOO p1_and_1 ( .A(p0_i0), .B(p0_i1), .Z(p1_and_1_comb) );\nBAR p1_and_2 ( .A(p0_i2), .B(p0_i3), .Z(p1_and_2_comb) );\n</code></pre> <p>These lines describe, in order:</p> <ul> <li>One cell, called <code>FOO</code>, that takes two inputs, .A and .B, provided by the     wires <code>p0_i0</code> and <code>p0_i1</code>, respectively, and one output, .Z, which will be     assigned to the wire `p1_and_1_comb.</li> <li>One cell, called <code>BAR</code>, that takes two inputs, .A and .B, provided by the     wires <code>p0_i2</code> and <code>p0_i2</code>, respectively, and one output, .Z, which will be     assigned to the wire `p1_and_2_comb.</li> </ul> <p>Note that the values computed by the cells wasn't mentioned - that's because <code>FOO</code> and <code>BAR</code> are defined in the \"cell library\", the list of standard cells used to generate the netlist. Thus, to be able to model these gates in a solver, we need to take that cell library as input to the LEC tool. The netlist describes how cells are laid out, and the cell library indicates what cells actually do. With both of these in hand, preparing the netlist half of a LEC is a [relatively] straightforward matter of parsing a netlist and cell library and converting those together into a description of logic. See z3_netlist_translator.cc for full details.</p>"},{"location":"solvers/#utilities","title":"Utilities","text":"<ul> <li>tools/lec_main.cc:     Driver function for performing IR-to-netlist LEC.</li> </ul>"},{"location":"solvers/#current-limitations","title":"Current Limitations","text":""},{"location":"solvers/#time-to-result","title":"Time-to-result","text":"<p>Under the hood, Z3 (and many other tools in this space) is an SMT solver. At a high level, think of an SMT solver as a SAT solver that has special handling for certain classes of data (bit vectors, floating-point numbers). Many sufficiently complicated problems will reduce to raw SAT solving (especially those involving netlists, which have to implement complex logic at the gate level. Consider what that means for a multiply, for example!). Since SAT scales exponentially with the size of its inputs, execution time can quickly grow past a point of utility for complex operations, notably multiplication. Fortunately, for most designs (without such complex ops), proving equivalence of a single pipeline stage can complete in a small amount of time (O(minutes)).</p>"},{"location":"solvers/#predicate-coverage","title":"Predicate coverage","text":"<p>Hypothetically, any XLS function that computes a predicate (bool) can be fed to Z3 for satisfiability testing. Currently a more limited set of predicates are exposed that can be easily expressed on the command line; however, it should be possible to provide:</p> <ul> <li>an XLS IR file</li> <li>a set of nodes in the entry function</li> <li>a DSLX function that computes a predicate on those nodes</li> </ul> <p>Which would allow the user to compute arbitrary properties of nodes in the function with the concise DSL syntax.</p>"},{"location":"solvers/#subroutines","title":"Subroutines","text":"<p>Z3 doesn't intrinsically have support for subroutines, or as they're called in Z3, \"macros\", instead requiring that all function calls be inlined.</p> <p>There is an extension that adds support for recursive function decls and defs, but in our experience, it doesn't behave the way we'd expect.</p> <p>Consider the following example:</p> <pre><code>package p\n\nfn mapper(value: bits[32]) -&gt; bits[32] {\n  ret value\n}\n\nfn main() -&gt; bits[1] {\n  literal_0: bits[32] = literal(value=0)\n  literal_1: bits[1] = literal(value=1)\n  elem_0: bits[32] = invoke(literal_0, to_apply=mapper)\n  eq_12: bits[1] = eq(literal_0, elem_0)\n  ret and_13: bits[1] = and(eq_12, literal_1)\n}\n</code></pre> <p>Here, it's trivial for a human reader to see that the results are the same; the output should be equal to 1. Z3, however, reports that this is not necessarily the case, suggesting that <code>literal_0</code> and <code>elem_0</code> would not be equal in the case where the input to <code>mapper</code> was 1...which is clearly never the case here.</p> <p>To address this, we require that all subroutines (including those used in maps and counted fors) be inlined before consumption by Z3.</p>"},{"location":"tools/","title":"XLS Tools","text":"<p>An index of XLS developer tools.</p>"},{"location":"tools/#bdd_stats","title":"<code>bdd_stats</code>","text":"<p>Constructs a binary decision diagram (BDD) using a given XLS function and prints various statistics about the BDD. BDD construction can be very slow in pathological cases and this utility is useful for identifying the underlying causes. Accepts arbitrary IR as input or a benchmark specified by name.</p>"},{"location":"tools/#benchmark_main","title":"<code>benchmark_main</code>","text":"<p>Prints numerous metrics and other information about an XLS IR file including: total delay, critical path, codegen information, optimization time, etc. This tool may be run against arbitrary IR not just the fixed set of XLS benchmarks. The output of this tool is scraped by <code>run_benchmarks</code> to construct a table comparing metrics against a mint CL across the benchmark suite.</p>"},{"location":"tools/#booleanify_main","title":"<code>booleanify_main</code>","text":"<p>Rewrites an XLS IR function in terms of its ops' fundamental AND/OR/NOT constituents, i.e., makes all operations boolean, thus it's \"booleanifying\" the function.</p>"},{"location":"tools/#codegen_main","title":"<code>codegen_main</code>","text":"<p>Lowers an XLS IR file into Verilog. Options include emitting a feedforward pipeline or a purely combinational block. Emits both a Verilog file and a module signature which includes metadata about the block. The tool does not run any XLS passes so unoptimized IR may fail if the IR contains constructs not expected by the backend.</p> <p>For a detailed list of codegen options including I/O configurations, please visit the codegen options page.</p>"},{"location":"tools/#delay_info_main","title":"<code>delay_info_main</code>","text":"<p>Dumps delay information about an XLS function including per-node delay information and critical-path.</p>"},{"location":"tools/#eval_ir_main","title":"<code>eval_ir_main</code>","text":"<p>Evaluates an XLS IR file with user-specified or random inputs. Includes features for evaluating the IR before and after optimizations which makes this tool very useful for identifying optimization bugs.</p> <p>This tool accepts two [mutually exclusive] optional args, <code>--input_validator_expr</code> and <code>--input_validator_path</code>, which allow the user to specify an expression to \"filter\" potential input values to discard invalid ones. For both, the filter must be a function, named <code>validator</code>, and must take params of the same layout as the function under test. This function should return true if the inputs are valid for the function and false otherwise. <code>--input_validator_expr</code> lists the function as an inline command-line argument, whereas <code>--input_validator_path</code> holds the path to a .x file containing the validation function.</p>"},{"location":"tools/#jit-inspection","title":"Jit Inspection Flags","text":"<p><code>eval_ir_main</code> provides several flags which can be used to inspect the jit code produced by our function jit. These flags all require that <code>--use_llvm_jit=true</code> (the default). Currently these flags only work with xls-functions. See the fuzzing documentation for more information about how the information these flags reveal can be used.</p> <p>This tool can also be used to generate and inspect the LLVM/ASM programs generated by the jit (for function programs). This can be used to diagnose issues found by the fuzzer.</p> <ul> <li><code>--llvm_jit_ir_output=&lt;file&gt;</code> controls where the unoptimized llvm ir for the     function is saved.</li> <li><code>--llvm_jit_opt_ir_output=&lt;file&gt;</code> does the same but with the IR after LLVM     itself has optimized it.</li> <li><code>--llvm_jit_asm_output=&lt;file&gt;</code> saves the result of converting the opt-ir     into assembly for the current architecture.</li> <li><code>--llvm_jit_main_wrapper_output=&lt;file&gt;</code> writes a bytecode file containing a     <code>main(int argc, char** argv)</code> function which calls the function the jit     creates. This can be     <code>llvm-link</code>d with the     <code>llvm_jit_ir_output</code> or <code>llvm_jit_opt_ir_output</code> to create a program     runnable by <code>lli</code> or other     llvm tools.</li> <li><code>--llvm_jit_main_wrapper_write_is_linked=true</code> (default is <code>false</code>) makes     the main wrapper call the posix <code>write</code> API to send the output to stdout.     The data is written without any processing as a stream of bytes in the order     it appears in memory. This flag is provided (and defaulted to off) because     not all llvm analysis tools are always able to handle extern symbols in a     reasonable way.</li> </ul> <p>Note: LLVM can change significantly and bytecode is not always compatible between versions. If possible, LLVM tools built at the same commit as the JIT should be used to interact with the generated llvm bytecode. This can be done by building the LLVM tools using <code>bazel build</code> from the XLS repo.</p>"},{"location":"tools/#ir_minimizer_main","title":"<code>ir_minimizer_main</code>","text":"<p>Tool for reducing IR to a minimal test case based on an external test.</p>"},{"location":"tools/#ir_stats_main","title":"<code>ir_stats_main</code>","text":"<p>Prints summary information/stats on an IR [Package] file. An example:</p> <pre><code>$ bazel-bin/xls/tools/ir_stats_main bazel-genfiles/xls/modules/fp32_add_2.ir\nPackage \"fp32_add_2\"\n  Function: \"__float32__is_inf\"\n    Signature: ((bits[1], bits[8], bits[23])) -&gt; bits[1]\n    Nodes: 8\n\n  Function: \"__float32__is_nan\"\n    Signature: ((bits[1], bits[8], bits[23])) -&gt; bits[1]\n    Nodes: 8\n\n  Function: \"__fp32_add_2__fp32_add_2\"\n    Signature: ((bits[1], bits[8], bits[23]), (bits[1], bits[8], bits[23])) -&gt; (bits[1], bits[8], bits[23])\n    Nodes: 252\n</code></pre>"},{"location":"tools/#check_ir_equivalence","title":"<code>check_ir_equivalence</code>","text":"<p>Verifies that two IR files (for example, optimized and unoptimized IR from the same source) are logically equivalent.</p>"},{"location":"tools/#opt_main","title":"<code>opt_main</code>","text":"<p>Runs XLS IR through the optimization pipeline.</p> <p>Standard flags include:</p> <ul> <li><code>--top=NAME</code>: Override/set the top function/proc. This is required if a     function/proc is not already marked as <code>top</code> in the IR.</li> <li><code>--opt_level=NUMBER</code>: Change the optimization level. This should be used     with care as the differences between optimization levels are less defined     for xls than they are in tools such as <code>clang</code>. Defaults to <code>3</code>.</li> </ul> <p>Several flags which control the behavior of individual optimizations are also available. Care should be used when modifying the values of these flags.</p> <ul> <li><code>--rm_rewrites_pb=FILE</code>: Used to pass a proto describing the ram rewrites to     be performed.</li> <li><code>--inline_procs=true|false</code>: Whether to enable or disable inlining all procs     into a single mega-proc. Defaults to <code>false</code>.</li> <li><code>--convert_array_index_to_select=NUMBER</code>: Controls the maximum number of     dimensions an array can have to allow xls to convert accesses to the array     into select chains. This can have complicated impacts on the area and delay     of the generated code.</li> <li><code>--use_context_narrowing_analysis=true|false</code>: Controls whether to use     contextual information to optimize range calculations. This can in some     circumstances reveal additional optimization opportunities but it can be     quite slow. Defaults to <code>false</code>.</li> </ul>"},{"location":"tools/#debuggingexperimenting-with-optimizations","title":"Debugging/Experimenting with Optimizations","text":"<p>There are also several flags which are used for debugging and understanding the behavior of the standard optimization pipeline itself and the passes which make up the pipeline. These flags should mostly be used for testing and debugging purposes only.</p> <ul> <li><code>--passes=PIPELINE_SPEC</code>: Allows one to specify an explicit optimization     pipeline to use instead of the standard one. The pipeline is specified by     listing passes using their short-names. Different passes are space     separated. Fixed-point combinations of passes are specified by surrounding     them with square-brackets '<code>[]</code>'. For example, to run the pipeline     'inlining' then 'arith_simp' and 'dce' to fixed-point then 'narrowing' and a     final 'dce' the flag would be set to <code>inlining [ arith_simp dce ] narrowing     dce</code>. This can be used to test odd interactions between specific or single     passes.</li> <li><code>--passes_bisect_limit=NUMBER</code>: Tells <code>opt_main</code> to cease pipeline execution     after running <code>NUMBER</code> passes. This can be used to narrow down misbehaving     passes. This flag works with both custom <code>--passes</code> pipelines and the     standard pipeline.</li> <li><code>--ir_dump_path=FOLDER</code>: Tells <code>opt_main</code> to dump files containing all the     intermediate states of the optimizations IRs into files in that particular     directory. Each file is named so they sort lexicographically in the order     they were created. The names include the pass-number, the pass run and     whether the pass made any changes to the output.</li> <li><code>--skip_passes=NAME1,NAME2,...</code>: Tells <code>opt_main</code> to skip execution of     specific named passes (specified using the short-name of the pass). This     does not otherwise modify the pipeline being used and the pass is considered     to have finished successfully without making any changes. Multiple passes     may be passed at once separated by commas.</li> </ul>"},{"location":"tools/#print_bom","title":"<code>print_bom</code>","text":"<p>Tool to calculate and print a summary of the BOM (bill of materials) elements from <code>ModuleSignatureProto</code> protobuf files produced using the <code>--output_signature_path</code> codegen argument.</p> <p>Features include;</p> <ul> <li>Combining the data from multiple protobuf files.</li> <li>Output in fancy human readable table.</li> <li>Output machine readable CSV (common separate values) file for loading into     other tools (like Google Sheets).</li> <li>Filtering output to a single value type like <code>BOM_KIND_ADDER</code>.</li> </ul> <p>Running the following commands;</p> <pre><code>bazel build //xls/examples/protobuf:varint_encode_sv\nbazel run //xls/tools:print_bom -- --root_path $PWD/bazel-bin/xls/examples/protobuf/\n</code></pre> <p>should produce the following output;</p> <pre><code>Found 1 protobuf files.\n * \"bazel-bin/xls/examples/protobuf/varint_encode_u32.sig.textproto\"\n\n +------------------------+----------------+-------------+--------------+-------+\n |                   Kind |             Op | Input Width | Output Width | Count |\n +------------------------+----------------+-------------+--------------+-------+\n |    BOM_KIND_COMPARISON |             ne |           4 |            1 |     1 |\n |                        |                |           7 |            1 |     2 |\n |                        |                |          11 |            1 |     1 |\n |                        |                |          18 |            1 |     1 |\n |                        |                |          25 |            1 |     1 |\n +------------------------+----------------+-------------+--------------+-------+\n | BOM_KIND_INSIGNIFICANT |          array |           8 |           40 |     1 |\n |                        |      bit_slice |          32 |            4 |     1 |\n |                        |                |          32 |            7 |     4 |\n |                        |                |          32 |           11 |     1 |\n |                        |                |          32 |           18 |     1 |\n |                        |                |          32 |           25 |     1 |\n |                        |         concat |           1 |            2 |     1 |\n |                        |                |           2 |            3 |     1 |\n |                        |                |           4 |            8 |     1 |\n |                        |                |           7 |            8 |     4 |\n |                        |        literal |           0 |            1 |     2 |\n |                        |                |           0 |            2 |     2 |\n |                        |                |           0 |            3 |     2 |\n |                        |                |           0 |            4 |     2 |\n |                        |                |           0 |            7 |     2 |\n |                        |                |           0 |           11 |     1 |\n |                        |                |           0 |           18 |     1 |\n |                        |                |           0 |           25 |     1 |\n |                        |          tuple |          40 |           43 |     1 |\n +------------------------+----------------+-------------+--------------+-------+\n |          BOM_KIND_MISC |     input_port |           0 |           32 |     1 |\n |                        |    output_port |          43 |            0 |     1 |\n |                        |  register_read |           0 |           32 |     1 |\n |                        |                |           0 |           43 |     1 |\n |                        | register_write |          32 |            0 |     1 |\n |                        |                |          43 |            0 |     1 |\n +------------------------+----------------+-------------+--------------+-------+\n |        BOM_KIND_SELECT |            sel |           2 |            2 |     2 |\n |                        |                |           3 |            3 |     2 |\n +------------------------+----------------+-------------+--------------+-------+\n</code></pre> <p>To save the details about the comparison operators to a machine readable CSV file you can do;</p> <pre><code>bazel run //xls/tools:print_bom -- --root_path=$PWD/bazel-bin/xls/examples/protobuf/ --output_as=csv --op_kind=BOM_KIND_COMPARISON &gt; my.csv\n</code></pre> <p>which will produce a CSV file which looks like the following;</p> <pre><code>Kind,Op,Input Width,Output Width,Count\nBOM_KIND_COMPARISON,ne,4,1,1\nBOM_KIND_COMPARISON,ne,7,1,2\nBOM_KIND_COMPARISON,ne,11,1,1\nBOM_KIND_COMPARISON,ne,18,1,1\nBOM_KIND_COMPARISON,ne,25,1,1\n</code></pre>"},{"location":"tools/#dslxinterpreter_main","title":"<code>dslx/interpreter_main</code>","text":"<p>Interpreter for DSLX code and test-runner.</p> <p>When tests are run this also cross-checks that the conversion to IR and JIT compilation of the IR produces the same values.</p> <pre><code>$ bazel run -c opt //xls/dslx:interpreter_main -- $PWD/xls/dslx/stdlib/std.x\n[ RUN UNITTEST  ] sizeof_signed_test\n[            OK ]\n...\n</code></pre> <p>Note that this binary takes a command line argument <code>--dslx_path</code> which indicates where the binary should search for <code>.x</code> files on import (i.e. an import resolution path). Try to use this sparingly, but it is useful for pointing at installation locations where DSLX modules have been placed.</p> <p>In a Bazel environment this binary is encapsulated in an <code>xls_dslx_test</code> target</p>"},{"location":"tools/#dslxprove_quickcheck_test","title":"<code>dslx/prove_quickcheck_test</code>","text":"<p>Command line utility for attempting to prove a quickcheck property via SMT translation. Invoke this tool as:</p> <p><code>prove_quickcheck_main $ENTRY_FILE $QUICKCHECK_NAME</code></p> <p>And it will attempt to prove the given quickcheck property over the entire input domain. Example:</p> <pre><code>$ bazel run -c opt //xls/dslx:prove_quickcheck_main -- $PWD/xls/dslx/stdlib/std.x convert_to_from_bools\nProven! elapsed: 115.419669ms\n</code></pre> <p>NOTE: Currently an error code is returned if it cannot be proven, but it does not dump a counterexample to terminal. A temporary workaround is to use <code>--alsologtostderr --v=1</code> until that functionality is completed.</p>"},{"location":"tools/#dslx_fmt","title":"<code>dslx_fmt</code>","text":"<p>Auto-formatter for DSLX code (i.e. <code>.x</code> files). This is analogous to rustfmt / clang-format.</p> <p>To format a file in-place, use the <code>-i</code> flag:</p> <pre><code>$ bazel build -c opt //xls/dslx:dslx_fmt\n$ echo 'fn f(x:u32)-&gt;u32{x}' &gt; /tmp/my_file.x\n$ ./bazel-bin/xls/dslx/dslx_fmt -i /tmp/my_file.x\n$ cat /tmp/my_file.x\nfn f(x: u32) -&gt; u32 { x }\n</code></pre> <p>Without the <code>-i</code> flag the formatted result is given in the standard output from the tool and the input file path remains unchanged.</p> <p>Note: there is also a Bazel build construct to ensure files remain auto-formatted using the latest <code>dslx_fmt</code> results:</p> <pre><code>load(\"//xls/build_rules:xls_build_defs.bzl\", \"xls_dslx_fmt_test\")\n\nxls_dslx_fmt_test(\n    name = \"my_file_dslx_fmt_test\",\n    src = \"my_file.x\",\n)\n</code></pre> <p>Also see the Bazel rule documentation for <code>xls_dslx_fmt_test</code>.</p>"},{"location":"tools/#proto_to_dslx_main","title":"<code>proto_to_dslx_main</code>","text":"<p>Takes in a proto schema and a textproto instance thereof and outputs a DSLX module containing a DSLX type and constant matching both inputs, respectively.</p> <p>Not all protocol buffer types map to DSLX types, so there are some restrictions or other behaviors requiring explanation:</p> <ol> <li>Only scalar and repeated fields are supported (i.e., no maps or oneofs,     etc.).</li> <li>Only recursively-integral messages are supported, that is to say, a message     may contain submessages, as long as all non-Message fields are integral.</li> <li>Since DSLX doesn't support variable arrays and Protocol Buffers don't     support fixed-length repeated fields. To unify this, all instances of     repeated-field-containing Messages must have the same size of their repeated     members (declared as arrays in DSLX). This size will be calculated as the     maximum size of any instance of that repeated field across all instances in     the input textproto. For example, if a message <code>Foo</code> has a repeated field     <code>bar</code>, and this message is present multiple times in the input textproto,     say as:<pre><code>  foo: {\n    bar: 1\n  }\n  foo: {\n    bar: 1\n    bar: 2\n  }\n  foo: {\n    bar: 1\n    bar: 2\n    bar: 3\n  }\n</code></pre> <p>the DSLX version of <code>Foo</code> will declare <code>bar</code> has a 3-element array. An   accessory field, <code>bar_count</code>, will also be created, which will contain the   number of valid entries in an actual instance of <code>Foo::bar</code>.</p> <p>The \"Fields\" example in   <code>./xls/tools/testdata/proto_to_dslx_main.*</code> demonstrates this   behavior.</p> </li> </ol>"},{"location":"tools/#repl","title":"<code>repl</code>","text":"<p>Allows you to interactively run various parts of the compiler, including parsing/type checking (<code>:reload</code>), lowering/optimization (<code>:ir</code>), Verilog codegen (<code>:verilog [identifier]</code>), and LLVM codegen (<code>:llvm</code>, not yet implemented). You can also inspect the IR types of identifiers with <code>:type</code>, and even imported identifiers can be accessed with <code>:type foo::bar</code>.</p> <p></p>"},{"location":"tools/#simulate_module_main","title":"<code>simulate_module_main</code>","text":"<p>Runs a Verilog block emitted by XLS through a Verilog simulator. Requires both the Verilog text and the module signature which includes metadata about the block.</p>"},{"location":"tools/#smtlib_emitter_main","title":"<code>smtlib_emitter_main</code>","text":"<p>Simple driver for Z3IrTranslator - converts a given IR function into its Z3 representation and outputs that translation as SMTLIB2.</p> <p>First obtain an XLS IR file:</p> <pre><code>$ bazel build -c opt //xls/examples:tiny_adder.opt.ir\n</code></pre> <p>And then feed that XLS IR file into this binary:</p> <pre><code>$ bazel run -c opt //xls/tools:smtlib_emitter_main -- --ir_path \\\n    $PWD/bazel-bin/xls/examples/tiny_adder.opt.ir\n(bvadd (concat #b0 x) (concat #b0 y))\n</code></pre> <p>To turn it into \"gate level\" SMTLib, we can do a pre-pass through the <code>booleanify_main</code> tool:</p> <pre><code>$ bazel run -c opt //xls/tools:booleanify_main -- --ir_path \\\n   $PWD/bazel-bin/xls/examples/tiny_adder.opt.ir \\\n   &gt; /tmp/tiny_adder.boolified.ir\n$ bazel run -c opt //xls/tools:smtlib_emitter_main -- \\\n    --ir_path /tmp/tiny_adder.boolified.ir\n(let ((a!1 (bvand (bvor ((_ extract 0 0) x) ((_ extract 0 0) y))\n                  (bvnot (bvand ((_ extract 0 0) x) ((_ extract 0 0) y))))))\n(let ((a!2 (bvor (bvand (bvor #b0 #b0) (bvnot (bvand #b0 #b0)))\n                 (bvor (bvand ((_ extract 0 0) x) ((_ extract 0 0) y))\n                       (bvand a!1 #b0))))\n      (a!3 (bvand (bvand (bvor #b0 #b0) (bvnot (bvand #b0 #b0)))\n                  (bvor (bvand ((_ extract 0 0) x) ((_ extract 0 0) y))\n                        (bvand a!1 #b0)))))\n  (concat (bvand a!2 (bvnot a!3))\n          (bvand (bvor a!1 #b0) (bvnot (bvand a!1 #b0))))))\n</code></pre>"},{"location":"tools/#solver","title":"<code>solver</code>","text":"<p>Uses a SMT solver (i.e. Z3) to prove properties of an XLS IR program from the command line. Currently the set of \"predicates\" that the solver supports from the command line are limited, but in theory it is capable of solving for arbitrary IR-function-specified predicates.</p> <p>This can be used to uncover opportunities for optimization that were missed, or to prove equivalence of transformed representations with their original version.</p>"},{"location":"tools/#cell_library_extract_formula","title":"<code>cell_library_extract_formula</code>","text":"<p>Parses a cell library \".lib\" file and extracts boolean formulas from it that determine the functionality of cells. This is useful for LEC of the XLS IR against the post-synthesis netlist.</p>"},{"location":"tools/#dslxhighlight_main","title":"<code>dslx/highlight_main</code>","text":"<p>Performs terminal-based color code highlighting of a DSL file.</p>"},{"location":"tools/#dslxtype_systemtypecheck_main","title":"<code>dslx/type_system/typecheck_main</code>","text":"<p>Dumps type information that has been deduced for a given DSL file.</p>"},{"location":"tools/#development-tools","title":"Development Tools","text":""},{"location":"tools/#clang-tidy","title":"clang-tidy","text":"<p>For C++ development, you might need a compilation database to have good support in your IDE. You can create the <code>compile_commands.json</code> by running this script.</p> <pre><code>dev_utils/make-compilation-db.sh\n</code></pre> <p>To run clang-tidy and create a report of things that might be worthwhile fixing, use the following script:</p> <pre><code>dev_utils/run-clang-tidy-cached.sh\n</code></pre> <p>(Note, this will be pretty slow on the first run, but it caches results and will only reprocess changed files in subsequent runs).</p> <p>The output of the clang-tidy runs shows up in the <code>xls_clang-tidy.out</code> file which is formatted just like an output from a compiler. So to quickly work with these, you can use <code>cat xls_clang-tidy.out</code> as your 'compiler invocation' in your IDE (e.g. <code>M-x compile</code> in emacs) and step through next-error locations as usual.</p>"},{"location":"tools/#golden-comparison-files","title":"Golden Comparison Files","text":"<p>To re-generate golden reference files (for all test targets that use golden reference file comparisons), run:</p> <pre><code>dev_utils/rebuild_golden_files.sh\n</code></pre>"},{"location":"tools_quick_start/","title":"XLS Tools Quick Start","text":"<p>This document is a quick start guide through the use of the individual XLS tools, from DSL input to RTL generation.</p> <p>Note: This guide assumes you have set up your system so it can build the XLS tools via Bazel. There is currently no binary tools distribution so building from source is required.</p> <p>Create a file <code>/tmp/simple_add.x</code> with the following contents:</p> <pre><code>fn add(x: u32, y: u32) -&gt; u32 {\n  x + y + u32:0  // Something to optimize.\n}\n\n#[test]\nfn test_add() {\n  assert_eq(add(u32:2, u32:3), u32:5)\n}\n</code></pre> <p>This contains a function, and a unit test of that function.</p>"},{"location":"tools_quick_start/#interpreting-the-dsl-file","title":"Interpreting the DSL file","text":"<p>Now, run it through the DSL interpreter -- the DSL interpreter is useful for interactive development and debugging.</p> <pre><code>$ bazel run -c opt //xls/dslx:interpreter_main -- /tmp/simple_add.x\n[ RUN      ] add\n[       OK ] add\n</code></pre> <p>The DSL interpreter is the execution engine running the test shown.</p> <p>In lieu of using bazel run for the subsequent commands, this document will assume <code>bazel build -c opt //xls/...</code> has been completed so the binaries in <code>./bazel-bin</code> can be used directly:</p> <pre><code>$ ./bazel-bin/xls/dslx/interpreter_main /tmp/simple_add.x\n[ RUN      ] add\n[       OK ] add\n</code></pre>"},{"location":"tools_quick_start/#dsl-to-ir-conversion","title":"DSL to IR conversion","text":"<p>To convert the DSL file to IR, run the following command:</p> <pre><code>$ ./bazel-bin/xls/dslx/ir_convert/ir_converter_main --top=add /tmp/simple_add.x &gt; /tmp/simple_add.ir\n</code></pre>"},{"location":"tools_quick_start/#ir-optimization","title":"IR optimization","text":"<p>To optimize the IR, use the <code>opt_main</code> tool:</p> <pre><code>$ ./bazel-bin/xls/tools/opt_main /tmp/simple_add.ir &gt; /tmp/simple_add.opt.ir\n</code></pre> <p>Check the output of <code>diff -U8 /tmp/simple_add*.ir</code> to see that the optimizer eliminated the useless add-with-zero.</p>"},{"location":"tools_quick_start/#verilog-rtl-generation","title":"Verilog RTL generation","text":"<p>To generate RTL from the optimized IR, use the <code>codegen_main</code> tool:</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main --pipeline_stages=1 --delay_model=unit /tmp/simple_add.opt.ir &gt; /tmp/simple_add.v\n</code></pre>"},{"location":"tools_quick_start/#ir-visualizer","title":"IR visualizer","text":"<p>To get a graphical view of the IR files, use the IR visualization tool:</p> <pre><code>$ ./bazel-bin/xls/visualization/ir_viz/app --delay_model=unit --preload_ir_path=/tmp/simple_add.ir\n</code></pre> <p>This starts a server on localhost port 5000 by default, so you can access it from your machine as <code>http://localhost:5000</code> in a web browser.</p>"},{"location":"vast/","title":"Verilog Abstract Syntax Tree (VAST)","text":"<p>XLS outputs Verilog (or SystemVerilog) for synthesis and simulation. As a lowest common denominator, Verilog output enables XLS generated designs to integrate into existing design flows. To make generation of Verilog easier, XLS includes an abstract representation of Verilog called VAST (Verilog Abstract Syntax Tree). VAST is a C++ library which represents Verilog in a recursive tree data structure which is simple to construct and manipulate programmatically. Verilog source code is emitted directly from the VAST data structure.</p> <p>VAST is intentionally not a complete representation of the Verilog language. VAST is used to emit Verilog for the purposes of code generation within XLS. Given this limited use case, VAST is much smaller and simpler than a complete representation of the entire Verilog language as might be required for a parser, for example.</p>"},{"location":"vast/#vast-overview","title":"VAST Overview","text":"<p>Each supported Verilog construct is represented with a C++ class. These classes form a type hierarchy with the class <code>VastNode</code> at the root. Objects are gathered in tree-shaped structures to represent Verilog constructs. Ownership of all VAST objects is maintained by a <code>VerilogFile</code> object which represents a single file of Verilog source code. References between objects are stored as plain pointers.</p> <p>For example, consider the following Verilog expression:</p> <pre><code>  foo + 8\n</code></pre> <p>In VAST, this is represented with an object of the <code>BinaryInfix</code> class which is derived from the <code>Expression</code> class representing arbitrary Verilog expressions. A <code>BinaryInfix</code> object has three relevant data members:</p> <p><code>std::string op_;</code> :   The string representation of the operation to perform (e.g., <code>+</code>).</p> <p><code>Expression* lhs_;</code> :   The left-hand-side of the expression. In this example, this points to a     <code>LogicRef</code> object (derived from <code>Expression</code> class) referring to a Verilog     <code>reg</code> or <code>wire</code> variable.</p> <p><code>Expression* rhs_;</code> :   The left-hand-side of the expression. In this example, this points to a     <code>Literal</code> object (derived from <code>Expression</code> class) containing the number 8     with unspecified bit width.</p> <p>The <code>BinaryInfix</code> object representing <code>foo + 8</code> might be used within other expressions or statements by referring to the object by pointer. For example, the representation of the statement <code>assign bar = foo + 8</code> would contain an <code>Expression*</code> pointer referring to the <code>foo + 8</code> object for the right-hand-side of the assignment.</p>"},{"location":"vast/#operator-precedence","title":"Operator Precedence","text":"<p>To avoid ambiguity, operators in Verilog follow precedence rules. For example, multiplication is higher precedence than addition so the expression <code>2 + 4 * 10</code> evaluates to <code>42</code> (i.e., <code>2 + (4 * 10)</code>) not <code>60</code> (i.e., <code>(2 + 4) * 10</code>). In VAST, expressions are built as a trees which is evaluated from the leaves to the root. To ensure that the operations are evaluated in the correct order when emitted as Verilog text, VAST automatically adds parentheses where appropriate. For example, the VAST expression consisting of the product (<code>BinaryInfix</code> with operation <code>*</code>) of <code>10</code> and the sum of <code>2</code> and <code>4</code> (<code>BinaryInfix</code> with operation <code>+</code>) will be emitted as <code>10 * (2 + 4)</code>.</p>"},{"location":"vast/#containers","title":"Containers","text":"<p>VAST has a number of classes which hold a sequence of (pointers to) other VAST objects. At the top-level, this includes the <code>VerilogFile</code> class which can hold a sequence of objects such as include statements and modules. Verilog modules themselves are represented with the <code>Module</code> class containing a sequence of statements, declarations, comments, and other constructs. Other containers include always blocks and functions.</p>"},{"location":"vast/#emitting-verilog-text","title":"Emitting Verilog text","text":"<p>VAST classes include an <code>Emit</code> method which returns the represented Verilog construct as a string. Typically, <code>Emit</code> is called on the top-level <code>VerilogFile</code> object to create the text of the entire Verilog source file. Underneath the hood, this method calls the <code>Emit</code> method on all contained VAST objects and assembles the returned strings into the Verilog source code.</p>"},{"location":"vast/#systemverilog-support","title":"SystemVerilog support","text":"<p>XLS can emit either Verilog or SystemVerilog so VAST supports both languages. SystemVerilog constructs are included alongside Verilog constructs in VAST. Examples of SystemVerilog features supported by VAST include:</p> <ul> <li><code>always_ff</code> procedure for modeling sequential logic (VAST <code>AlwaysFlop</code>     class).</li> </ul> <ul> <li>Array assignment pattern (VAST <code>ArrayAssignmentPattern</code> class). Example:     <code>'{foo, bar, baz}</code></li> </ul> <ul> <li>Array declaration using sizes. Example: <code>reg [7:0] foo[42];</code></li> </ul> <p>Within VAST, there is no distinction between the two languages and it is up to the user of VAST to only use the supported features for the target language (Verilog or SystemVerilog).</p>"},{"location":"xls_style/","title":"XLS Style Guide","text":"<p>The Google style guides recommend enforcing local consistency where stylistic choices are not predefined. This file notes some of the choices we make locally in the XLS project, with the relevant Google style guides (C++, Python) as their bases.</p>"},{"location":"xls_style/#c","title":"C++","text":"<ul> <li>Align the pointer or reference modifier token with the type; e.g. <code>Foo&amp;     foo = ...</code> instead of <code>Foo &amp;foo = ...</code>, and <code>Foo* foo = ...</code> instead of <code>Foo     *foo= ...</code>.</li> </ul> <ul> <li>Use <code>/*parameter_name=*/value</code> style comments if you choose to annotate     arguments in a function invocation. <code>clang-tidy</code> recognizes this form, and     provides a Tricorder notification if <code>parameter_name</code> is mismatched against     the parameter name of the callee.</li> </ul> <ul> <li>Prefer <code>int64_t</code> over <code>int</code> to avoid any possibility of overflow.</li> </ul> <ul> <li>Always use <code>Status</code> or <code>StatusOr</code> for any error that a user could encounter.</li> </ul> <ul> <li>Other than user-facing errors, use <code>Status</code> only in exceptional situations.     For example, <code>Status</code> is good to signal that a required file does not exist     but not for signaling that constant folding did not constant fold an     expression.<p>See how heavyweight is StatusOr for more   details on thinking about the costs involved.</p> </li> </ul> <ul> <li>Internal errors for conditions that should never be false can use <code>CHECK</code>,     but may also use <code>Status</code> or <code>StatusOr</code>.</li> </ul> <ul> <li>Prefer to brace single-statement blocks. Because the <code>XLS_ASSIGN_OR_RETURN</code>     macro expands into multiple statements, this can cause problems when using     unbraced single-statement blocks. Instead of XLS developers needing to think     about individual cases of single statement blocks, we brace all single     statement blocks.</li> </ul> <ul> <li>Prefer using <code>XLS_ASSIGN_OR_RETURN</code> / <code>XLS_RETURN_IF_ERROR</code> when     appropriate, but when binding a <code>StatusOr</code> wrapped value prefer to name it     <code>thing_or</code> so that it can be referenced without the wrapper as <code>thing</code>; e.g.<pre><code>absl::StatusOr&lt;Thing&gt; thing_or = f();\nif (!thing_or.ok()) {\n  // ... handling of the status via thing_or.status() and returning ...\n}\nconst Thing&amp; thing = thing_or.value();\n</code></pre> </li> </ul> <ul> <li>Prefer <code>CHECK</code> to <code>DCHECK</code>, except that <code>DCHECK</code> can be used to verify     conditions that it would be too expensive to verify in production, but that     are fast enough to include outside of production.</li> </ul> <ul> <li>Use <code>QCHECK</code> and <code>LOG(QFATAL)</code> during program startup when verifying startup     parameters (i.e., flags); prefer <code>CHECK</code> and <code>LOG(FATAL)</code> in all other     circumstances, as the <code>Q</code> variants suppress <code>atexit</code> handling (including     <code>--cpu_profile</code>).</li> </ul> <ul> <li>Follow the C++ style guide for capitalization guidelines; however, in the     somewhat ambiguous case of I/O (short for Input/Output, which we use often),     the slash counts as internal spacing and therefore the capitalization we use     is <code>IO</code>, as in <code>WrapIO</code> or <code>StreamingIOReader</code>.</li> </ul> <ul> <li>Prefer to use the <code>XLS_FRIEND_TEST</code> macro vs friending manually-mangled test     names.<p>At times it can be useful to test unit test a private/protected member of a   class, and the <code>XLS_FRIEND_TEST</code> macro makes this possible. Note that the   test case must live outside an unnamed namespace in the test file for the   \"friending\" to work properly.</p> </li> </ul> <ul> <li>For simple const accessors, for the sake of consistency in the code base,     and a weak preference towards the benefits of information hiding, prefer to     return view types over the apparent type of the member; e.g.<pre><code>class MyClass {\n public:\n  // This return type is preferrable to `const std::vector&lt;uint64_t&gt;&amp;`.\n  absl::Span&lt;const uint64_t&gt; values() const { return values_; }\n\n private:\n  std::vector&lt;uint64_t&gt; values_;\n};\n</code></pre> </li> </ul> <ul> <li>Follow the     style guide's     decision to avoid RTTI. In practice, this means <code>down_cast&lt;&gt;</code> should be used     instead of <code>dynamic_cast&lt;&gt;</code>. However, the style guide says to avoid     hand-implementing RTTI-like workarounds. The DSLX and XLScc frontends are     places where avoiding RTTI would require implementing workarounds that end     up looking a lot like RTTI, so <code>dynamic_cast&lt;&gt;</code> is common and accepted for     those parts of the codebase. Elsewhere, especially with IR <code>Node</code> types,     <code>down_cast&lt;&gt;</code> should be used instead.</li> </ul> <ul> <li>Prefer <code>std::string_view</code> to <code>absl::string_view</code>. <code>absl::string_view</code> mainly     differs from <code>std::string_view</code> in construction from nullptr, which our     usage/callers do not depend upon. This decision lets us switch over to the     more consistent end-state sooner. Although the style guide recommends we     prefer <code>absl::string_view</code> for now, the rationale for why does not really     apply to us and their target end state is clear.</li> </ul> <ul> <li>XLS code is often written in a functional (i.e. separating functions from     the [ideally immutable] structs they operate on) and layered style, which     leads to <code>_utils.h</code> style translation units that layer on and compose     functionality. Prefer the suffix <code>_utils.h</code> for these, vs <code>_helpers.h</code> or     other alternatives.</li> </ul> <ul> <li>Static member functions should be used sparingly, generally only for     factories that call a private constructor. We prefer to document     implementations with a <code>/* static */</code> comment as a reminder to readers (and     writers that there is no <code>this</code> available). Comments are not an ideal way to     mark this kind of information, but there should be a small number of these     functions and as factories it is unlikely the static qualifier will be     dropped in the future to put the comments out of sync.</li> </ul> <ul> <li>We prefer <code>absl::visit</code> over <code>std::visit</code> as it is reportedly higher     performance.</li> </ul>"},{"location":"xls_style/#functions","title":"Functions","text":"<ul> <li>Short or easily-explained argument lists (as defined by the developer) can     be explained inline with the rest of the function comment. For more complex     argument lists, the following pattern should be used:<pre><code>// &lt;Function description&gt;\n// Args:\n//   arg1: &lt;arg1 description&gt;\n//   arg2: &lt;arg2 description&gt;\n//   ...\n</code></pre> </li> </ul>"},{"location":"xls_style/#ir-nodes","title":"IR nodes","text":""},{"location":"xls_style/#class-hierarchy-and-oop-design","title":"Class Hierarchy and OOP Design","text":"<p>A frequently asked question about XLS's design is how the IR class hierarchy gels with Google style guide recommendations. This section is intended to provide a rationale for \"tagging\" leaf node types and for using <code>node-&gt;Is&lt;NodeType&gt;()</code> and <code>switch (node-&gt;op())</code> to form categories of node types instead of a class inheritance taxonomy.</p> <p>The base type <code>Node</code> encapsulates an element that takes input operands and produces an output, along with some metadata like type, name, and references to source locations. Each IR node (e.g. <code>add</code>, <code>send</code>, <code>concat</code>, etc.) extends directly from <code>Node</code>.</p> <p>Each <code>Node</code> defines <code>op()</code> and <code>Is&lt;NodeType&gt;()</code> methods which are more performant alternatives to C++'s RTTI. For example,</p> <pre><code>if (node-&gt;Is&lt;Send&gt;()) {\n  return node-&gt;As&lt;Send&gt;()-&gt;channel_id();  // As&lt;Send&gt;() is down_cast&lt;Send*&gt;()\n}\n</code></pre> <p>or</p> <pre><code>switch (node-&gt;op()) {\n  case Op::kSend:\n    return down_cast&lt;Send*&gt;(node)-&gt;channel_id();\n  // case Op:: ...\n}\n</code></pre> <p>are common patterns on IR nodes.</p> <p>In many contexts, this code would be a cry for better abstractions- some reasonable ideas include:</p> <ol> <li>A <code>virtual std::optional&lt;int64_t&gt; Node::channel_id()</code> (or     <code>absl::StatusOr&lt;int64_t&gt;</code>) implementation.</li> <li>A subclass or mixin trait like <code>ChannelNode</code> that extends <code>Node</code> for <code>Send</code>     to derive.</li> <li>A visitor that implements similar functionality outside of the class     hierarchy.</li> </ol> <p>These ideas are generally not a good fit for IR nodes. The first idea's main problem is that there are a lot of node types and the base class will become huge if it needs to contain every property of each type of node. Furthermore, the base class will be difficult to reason about without more structure- e.g. does a node with a <code>channel_id</code> sometimes, always, or never also have a <code>predicate</code>?</p> <p>The second idea seems to address the problems of the first, but it is not clear how to design a useful type hierarchy for IR nodes. The subset of nodes we care about is very context dependent. The examples above invite a <code>ChannelNode</code> abstraction, but other places in the code might care about unary vs. n-ary ops, or ops that produce bare values vs. tuples, or some other way to group nodes. We can find ourselves facing the first idea's complexity explosion if we make a mixin trait for each pass, and there aren't good ways to inject mixin traits for each compilation unit.</p> <p>The third idea of using a visitor (or, similarly, a typeclass) is used in the XLS codebase at times, but mostly where there's some well-defined behavior for most kinds of nodes. If you want to pluck out an ad-hoc subset of nodes, you need to make a new kind of visitor for that subset and it ends up being similar to the code above. In the limit, you might need all arbitrary combinations which will lead to too many visitor types to maintain centrally.</p> <p>Using <code>node-&gt;Is&lt;NodeType&gt;()</code> or <code>switch (node-&gt;op())</code> are concise and readable ways for the common task of operating on a new category of nodes. The typical OOP tools we'd often use instead don't map well to the needs of an IR, so we discourage adding to the base type or type hierarchy of IR nodes. We encourage gathering categories that are reused in node_util.h.</p> <p>It's also worth noting that <code>node-&gt;op()</code>, <code>node-&gt;Is&lt;NodeType&gt;()</code>, <code>node-&gt;As&lt;NodeType&gt;()</code>, and <code>down_cast&lt;NodeType*&gt;(node)</code> are more performant than C++ RTTI and <code>dynamic_cast&lt;&gt;</code>. C++ RTTI is not designed to be cheap and if we used <code>dynamic_cast&lt;&gt;</code> instead of our own tags + <code>down_cast&lt;&gt;</code>, we expect that would perform significantly worse. Performance is not the primary rationale for the design decision discussed above, but the knock-on performance effects further support the decision.</p>"},{"location":"xls_style/#passing-node-types","title":"Passing Node Types","text":"<ul> <li>Unlike most data, IR elements should be passed as non-const pointers, even     when expected to be const (which would usually indicate passing them as     const references). Experience has shown that IR elements often develop     non-const usages over time. Consider the case of IR analysis passes - those     passes themselves rarely need to mutate their input data, but they build up     data structures whose users often need to mutate their contents. In     addition, treating elements as pointers makes equality comparisons more     straightforward (avoid taking an address of a reference) and helps avoid     accidental copies (assigning a reference to local, etc.). Non-const pointer     usage propagates outwards such that the few cases where a const reference     could actually be appropriate become odd outliers, so our guidance is that     IR elements should uniformly be passed as non-const pointers.</li> </ul> <ul> <li>A corollary to the above is that <code>nullptr</code> is generally not a valid input to     functions taking IR elements. When an IR element is optional, we recommend     explicitly using <code>std::optional&lt;T*&gt;</code>. We deviate from the style guide here     because for IR elements <code>T*</code> sometimes means <code>const T&amp;</code>, <code>T&amp;</code>, or just <code>T</code>     in addition to <code>T*</code>, but which is not apparent from the signature. However,     using <code>nullptr</code> for IR element types is OK when the usage is fully     encapsulated.</li> </ul>"},{"location":"xls_style/#protocol-buffers","title":"Protocol buffers","text":"<ul> <li>Prefer to use     proto3     specifications in all new protocol buffer files.</li> </ul>"},{"location":"xls_style/#faq","title":"FAQ","text":""},{"location":"xls_style/#how-heavyweight-is-statusor","title":"How heavyweight is <code>StatusOr</code>?","text":"<p>What follows is the general guidance on how absl::StatusOr is used -- it is used extensively throughout the XLS code base as an error-style indicator object wrapper, so it is important to understand the mental model used for its cost.</p> <p>Consider cost wise that: a) creating an ok <code>StatusOr</code> is cheap, b) creating a non-ok <code>StatusOr</code> is expensive (that is, imagine the non-ok <code>Status</code> within a <code>StatusOr</code> is the expensive part).</p> <p>The implication being: if there's an API where \"not found\" is a reasonable outcome, prefer <code>std::optional&lt;&gt;</code> as a return value to indicate that / go with the grain of cost.</p> <p>Something like a filesystem API would be a classic example -- where you shouldn't be rooting around looking for files that aren't there -- so a not-found <code>absl::StatusOr</code> result would be fine to use.</p> <p>A good potential mental model is to imagine the program may run with logging of a traceback for every non-ok status that is created. (This is related to a debugging capability in Google internally called <code>--util_status_save_stack_trace</code> that captures backtraces when error <code>Status</code>es are created.) Ideally, with such a logging flag turned on, the screen wouldn't fill up with \"non error tracebacks\", only tracebacks from events where something really went wrong.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/","title":"Legalize Multiple Channel Ops Per Channel","text":"<ul> <li>Legalize Multiple Channel Ops Per Channel<ul> <li>Context<ul> <li>Objective</li> <li>Background</li> </ul> </li> <li>Design<ul> <li>Overview</li> <li>Minimum Viable Product</li> <li>Details<ul> <li>Single Channel Operation per Tick with True Predicate</li> <li>Multiple Channel Operations per Tick with True Predicate</li> <li>Channel Operations in Different Procs</li> <li>Scheduling</li> </ul> </li> <li>Long-Term Enhancements<ul> <li>Multi-Proc Codegen</li> <li>Optimization for Mutually Exclusive Operations</li> <li>Arbitrary Scheduling of Multiple Channel Operations</li> <li>IR-level testing should run on scheduled IR</li> </ul> </li> </ul> </li> <li>Alternatives considered<ul> <li>Improve Proc Inlining</li> <li>Use a proof assistant that supports induction</li> </ul> </li> </ul> </li> </ul> <p>Written on 2023-05-02.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#context","title":"Context","text":""},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#objective","title":"Objective","text":"<p>Whenever an XLS channel has multiple sends/receives, the XLS compiler currently relies on the mutual exclusion pass to merge these operations. If the operations cannot be proven to be mutually exclusive or if they are not in the same token level, it is an error and many legal programs fail to compile. The goal is to compile more programs that have multiple sends/receives on the same channel.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#background","title":"Background","text":"<p>One common way for the mutual exclusion pass to fail is with a pipelined loop:</p> <pre><code>if (p) {\n  send(chan, value0);\n} else {\n  #pragma hls_pipeline_init_interval 1\n  for (int i = 0; i &lt; 4; i++) {\n    send(chan, value1);\n  }\n}\n</code></pre> <p>The XLSCC frontend<sup>1</sup> will make the for loop a separate proc with channels that pass necessary state in and out. The \"outside\" proc will pause forward progress until the loop proc completes, so the two sends are mutually exclusive. However, after proc inlining, the predicates to the sends will be a function of the proc state that requires induction to prove mutual exclusion. To see why this is, it's helpful to understand how proc inlining works. The proc inlining pass creates a new container proc that allows all the inlined procs to tick simultaneously. This is implemented via an \"activation network\"- a series of boolean values encoded in the proc state that follow the token network of each proc. Each proc has a root activation that corresponds to the proc token parameter- this root activation is initialized as true, and activations flow down the network until something like a waiting blocking receive on an internal channel occurs. When a side-effecting operation does not occur within a container tick, all dependent operations will not be activated and will wait until the next container tick. This allows inlined procs to independently make progress on their execution. It also means that inlined proc ticks are decoupled from container proc ticks.</p> <p>Channel operations in different procs will have activations be a portion of their predicates, but proving properties about these activations in general requires reasoning across multiple proc ticks. The mutual exclusion pass is not able to prove properties over time and will fail to merge the sends in the example above.</p> <p>Currently, XLS treats multiple channel operations on the same channel as an error as a design choice to enforce that full throughput at II=1 is achievable. As a result, compilation will fail. It is important to note that although this problem exists for any channel, it is especially common when working with RAMs, especially single-port RAMs. The example above could come from code like</p> <pre><code>if (p) {\n  mem[x] = value0;\n} else {\n  #pragma hls_pipeline_init_interval 1\n  for (int i = 0; i &lt; 4; i++) {\n    value1 += mem[(x + 1)%MEM_SIZE];\n  }\n}\n</code></pre> <p>Each memory access gets lowered to a pair of send/receives on a request and response channel. For single-port RAMs, read and write channels will also be merged. It is therefore critical for programs using RAMs that multiple sends/receives work.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#design","title":"Design","text":""},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#overview","title":"Overview","text":"<p>Ultimately, we are asking the mutual exclusion pass to do too much. To merge operations like the sends in the example above, analysis must prove arbitrarily complex properties of the program, including those that change over time. Instead of relying on mutual exclusion analysis, it seems better to allow multiple sends and treat the mutually exclusive operations as a special case that allows for optimization. Concretely, the proposal is:</p> <ol> <li>Do not merge mutually exclusive channel operations before scheduling.</li> <li> <p>When multiple channel operations occur on a channel, IR lowering should     replace the channel for those operations with new internal channels. An     adapter proc should be added that accepts multiple channel operations on the     new \"generated by the compiler pass\" side and forwards them to the original     IO channel.</p> <p>1.  One adapter variant will be an arbiter. If multiple channel operations       occur in a cycle, the pipeline stalls until they all complete. Priority       for the adapter will be determined by token level. If operations that       are not ordered with respect to each other activate at the same time, it       will be an assertion failure. Optionally, the compiler can impose an       arbitrary order to unordered operations (following the token ordering       where one exists) via a topo sort.</p> <p>1.  Another adapter variant will be a \"mutex assumed\" variant used only when       the channel operations are mutually exclusive. The XLS\u2192outside signals       will be OR'd and the outside\u2192XLS signals will be fanned out. An       assertion will be added checking that at most one channel operation is       active in a cycle<sup>2</sup>.</p> </li> <li> <p>These adapters should be separate procs and codegen'd as their own Verilog     module. This hierarchy is useful for tracking the cost of the arbiter     variant and facilitating later replacement of arbiter variants with mutex     assumed variants. The MVP can initially inline the adapters until multi-proc     codegen is supported.</p> </li> </ol>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#minimum-viable-product","title":"Minimum Viable Product","text":"<p>The goal is to support the example in the Background section (and similar examples) ASAP. The steps roughly follow:</p> <ol> <li>Write a proc generator for the arbiter variant of adapter.</li> <li> <p>When there are multiple channel operations on the same channel, a scheduling     pass<sup>3</sup> will add new channels and an adapter. The pass will move the     channel operations' channel ids over to the new channels and connect the     adapter's send/recv to the original channel. New channels to send the     predicate before the operation and receive a completion after the operation     will be added and connected.</p> <p>1.  The adapter will arbitrate between multiple operations in priority       order.</p> <p>2.  Priority order will be determined by token dependencies. The adapter       will have runtime checks that all active receives are ordered with       respect to each other. Optionally, impose an arbitrary order to       unordered operations via a topo sort.</p> <p>3.  Channel operations from different procs will be required to be mutually       exclusive, enforced by a runtime check.</p> <p>4.  Initially, use proc inlining on the adapter.</p> </li> <li> <p>After the adapter is added and connected by the pass, scheduling constraints     will only apply to the operation in the adapter. The original send/receives     will be freely scheduled, although after inlining the predicate sends     (before the operation) and completion receives (after the operation) will     implicitly order the operations with respect to the scheduling constraint.</p> </li> </ol>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#details","title":"Details","text":"<p>This proposal suggests extending the current XLS behavior which are, roughly, \"multiple channel operations must be formally proven to be mutually exclusive and on the same token level, and then must be merged into a single operation\". There are trade-offs in choosing how far to extend.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#single-channel-operation-per-tick-with-true-predicate","title":"Single Channel Operation per Tick with True Predicate","text":"<p>One case is to allow multiple channel operations per proc tick, but require that only one of them have a true predicate. The current XLS behavior requires the operations to be on the same token level and to be formally proven to have mutually exclusive predicates. This could be extended to not require formal proof and instead provide a runtime check (assertion). This corresponds to the mutex assumed adapter variant described earlier and is a very incremental extension of the existing behavior. One problem with assuming all channel operations are mutually exclusive (even if you can't prove it) is that they aren't mutually exclusive. We could make it an error in the interpreter and JIT to have multiple channel operations per tick, but it currently isn't an error and seems potentially useful to allow eventually. There it seems like the mutually exclusive case should be an optimization of the more general case where multiple operations fire, and only applied when desired.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#multiple-channel-operations-per-tick-with-true-predicate","title":"Multiple Channel Operations per Tick with True Predicate","text":"<p>When there are multiple channel operations per tick firing, there is a new problem to consider: how to order the operations. There is no ambiguity when there is a total ordering on operations. It would be safe for the MVP to require a total ordering. However, the XLSCC frontend specifies token dependencies as loosely as possible<sup>4</sup>, so it is a normal occurrence for there not to be a total ordering on channel operations.</p> <p>If multiple channel operations aren't totally ordered, you can choose a static or dynamic order. On a subset of operations with no ordering with respect to each other, you could imagine choosing randomly or going round robin. However, choosing a static order seems simplest for MVP, and dynamic options seem of little utility. One way to choose this static order is to use the order given by XLS's <code>TopoSort()</code><sup>5</sup>.</p> <p>An alternative to worrying about the order is to legalize channel operations by adding a new channel per extra operation. This is equivalent to the current approach without the adapter- instead, the adapter (if needed) becomes the responsibility of the external integrator. This option might be useful in cases where the adapter orders things based on the data (for memory traffic, you could imagine performing all writes before all reads, or vice versa, and forwarding data written in the previous cycle if read in the current cycle).</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#channel-operations-in-different-procs","title":"Channel Operations in Different Procs","text":"<p>Channel operations in different procs have no direct token relationship. There is an ordering imposed by channel operations over control channels, but this relationship is indirect. <code>TopoSort()</code> only operates on one proc or function at a time.</p> <p>We could require that multiple sends/receives occur within the same proc, although XLSCC produces channel operations in different procs for the example given at the start and many other interesting examples.</p> <p>We could build a global token graph, where each proc's token parameter has an edge to some root token and each proc's next token feeds into a global next token (analogous to proc inlining). Edges will be added where procs send/receive to each other on internal channels. Note that the analysis may be tricky in cases where multiple parallel send/receives occur on these internal channels. It may be desirable to disallow multiple channel operations on internal channels, or have a variant of channel that disallows multiple channel operations to keep analysis sane.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#scheduling","title":"Scheduling","text":"<p>For MVP, it seems simplest to require all channel operations on a given channel be scheduled in the same cycle. This will sometimes cause scheduling to fail and may interact poorly with II&gt;1 passes, but is a useful starting point. Putting channel ops on the same channel in different cycles could allow for better QoR (especially with II&gt;1) than could be realized if channel ops were required to be merged.</p> <p>If channel operations on the same channel are scheduled in different cycles, the adapters need to delay earlier channel operations until the last cycle containing one of the multiple channel operations<sup>6</sup><sup>7</sup>.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#long-term-enhancements","title":"Long-Term Enhancements","text":""},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#multi-proc-codegen","title":"Multi-Proc Codegen","text":"<p>The adapter procs should ideally be codegen'd as separate Verilog modules, which will require multi-proc codegen.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#optimization-for-mutually-exclusive-operations","title":"Optimization for Mutually Exclusive Operations","text":"<p>Post-codegen analysis may be needed to determine if operations are mutually exclusive. For example, mutual exclusion properties may be data dependent and depend on the context a block is instantiated in. Third party formal tools may be useful in proving these properties.</p> <p>Multi-proc codegen makes it straightforward to switch an adapter from arbitrating to mutex assumed- generate both variants and have a Verilog config set which variant should be used for each instance after the fact.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#arbitrary-scheduling-of-multiple-channel-operations","title":"Arbitrary Scheduling of Multiple Channel Operations","text":"<p>Wavefront matching channel operations across cycles seems to mean that scheduling channel operations in different cycles will not save anything. This is because the extra state you need to delay a channel operation. If analysis can determine that the wavefronts don't need to be matched (in general, this could be data dependent!), then you could remove this extra state.</p> <p>If wavefronts do need to be matched, care needs to be taken to drive channel operations in the correct order in the presence of stalls.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#ir-level-testing-should-run-on-scheduled-ir","title":"IR-level testing should run on scheduled IR","text":"<p>The pass described here is a scheduling pass, and hence performed during codegen. The priority chosen by the pass when there is not a total order on the channel operations will impact program behavior, so ideally it would be consistent when running IR JIT.</p> <p>One way to resolve this is to combine IR opt + codegen into a single driver. That can produce opt IR, scheduled IR, or RTL. Alternatively, you could make scheduling its own distinct step, or move scheduling passes into opt and only do block conversion and RTL conversion in codegen. Regardless, the IR jit should be running the scheduled IR to ensure equivalent program behavior.</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#improve-proc-inlining","title":"Improve Proc Inlining","text":"<p>In practice, many of the failures we've seen in identifying mutually exclusive channel operations are a result of proc inlining. Proc inlining moves channel operation predicates into the proc state, but this doesn't seem necessary for many of the examples that fail to compile.</p> <p>The XLSCC frontend generally produces procs that transfer execution to one proc at a time. They generally look something like:</p> <pre><code>arguments = receive(input_channel, pred=is_init_state);\nstate = WORK_STATE;\n// do stuff\nsend(output_channel, pred=is_done_state);\nif (is_done_state) {\n  state = INIT_STATE;\n}\n</code></pre> <p>In this case, the proc activation state can be condensed into something like a program counter. Each value of the program counter would correspond to a different inlined proc's activation and channel operations would have their predicates be strengthened with <code>p' = p &amp; (pc == MY_PROC_ID)</code>. As a result, channel operations in different inlined procs would be trivially mutually exclusive.</p> <p>The analysis to determine if this optimization can be performed safely could be as simple as trying it and performing a deadlock analysis. However, even if the optimization is safe, it may not be an optimization- multiple procs ticking in parallel may be desired for performance reasons.</p> <p>Analysis to check that execution is transferred could look like:</p> <ol> <li> <p>Find places where procs give control away. These are found by unrolling by 2     and looking for all blocking (send, recv) pairs where the receive is     downstream of the send in the token graph (with nothing in-between). If the     send's predicate implies the receive's predicate, the proc gives control to     another proc on the send's channel until it is returned via the receive's     channel. If this property holds for every proc\u2192proc channel within a proc,     we can say the proc \"always extinguishes\".</p> </li> <li> <p>Find procs that come out of initialization \"extinguished\". There must be at     least one proc (generally, the top proc) where this is not true. Other procs     that begin with a blocking receive with predicate true on initialization are     initialized as \"extinguished\". Every other side-effecting operation should     be downstream of this receive.</p> </li> </ol> <p>The subset of procs that are initialized as extinguished and always extinguish can be inlined as described above.</p> <p>It's unclear that this approach will allow mutual exclusion to succeed for more complicated scenarios, e.g. multiple sends/recvs in nested loops in different orders. It also seems generally unscalable to rely on passes being aware of mutual exclusion.</p> <p>It is worth noting that this may be a generically useful improvement to proc inlining. Having predicates that are easier to reason about could assist optimization, and the state representation of proc activation would be more compact.</p> <p>It is also worth noting that non-blocking channel operations will not be defined as extinguishing. The notion of extinguishing could be generalized to include procs whose state isn't updated observably unless a receive completes. Similarly, in our discussions Remy described some potentially simpler ways of identifying procs that can be run one at a time with a PC. Regardless of what analysis is used to identify situations where proc inlining can execute procs mutually exclusively, the shortcomings of the approach seem to be that 1) proc inlining and other passes need to be aware of how powerful the mutual exclusion pass is, which doesn't scale well; and 2) it\u2019s not clear that this resolves all the cases where mutual exclusion can fail, even in proc inlining (proc inlining adds state to track multiple proc\u2194proc channel operations, so mutual exclusion could fail within an inlined proc if this obfuscates the predicate).</p>"},{"location":"design_docs/legalize_multiple_channel_ops_per_channel/#use-a-proof-assistant-that-supports-induction","title":"Use a proof assistant that supports induction","text":"<p>A more powerful proof assistant has the danger of increasing runtime or timing out unpredictably (Z3 is already pushing our boundaries in this regard). Even though the unrolling required to prove mutual exclusion seems bounded in most cases, it isn\u2019t clear what sort of runtime we should expect.</p> <p>It seems better to treat powerful proof assistants as an infrequent, high-effort final optimization step. For example, in the proposed design the arbiter variants could be replaced with mutex assumed variants if an expensive proof of mutual exclusion can be found.</p> <ol> <li> <p>The example presented here is particular to the XLSCC frontend. DSLX scopes channels to procs, so a similar example in DSLX would not have multiple procs sending/receiving the same channel. Instead, an internal channel would communicate proc\u2192proc and the top proc would forward it to the IO channel. If the top proc manages the send predicate, it is likely that mutual exclusion will be able to merge the channel operations. However, if the child proc manages the predicate, DSLX could have the same problems with proc inlining that XLSCC does.</p> <p>Of course, non-mutually-exclusive multiple sends/receives are not allowed in either frontend currently and this proposal would enable that for both frontends.\u00a0\u21a9</p> </li> <li> <p>In the general case where channel operations are scheduled in different cycles, this will need to be a temporal assertion.\u00a0\u21a9</p> </li> <li> <p>A \"scheduling pass\" is a pass that operates on the IR after scheduling but before block conversion. Importantly, we want this pass to run after the mutual exclusion pass has a chance to run and after scheduling for II&gt;1 has occurred.\u00a0\u21a9</p> </li> <li> <p>As of writing, XLSCC does not specify any token relationships, it emits IR with every side effecting operation taking the proc\u2019s root token and gathers all resulting tokens into an after-all for the next token. The TokenDependencyPass uses data dependencies to add minimal token relationships.\u00a0\u21a9</p> </li> <li> <p>It is often desirable for DSLX interpretation, IR interpretation/JIT, and RTL simulation to all have identical output. One reason is to support fast verification at higher levels of abstraction. Then a LEC chain from IR to RTL can prove that the high-level coverage applies to the RTL. <code>TopoSort()</code> (and many other ways of choosing an order) will be unstable across the different levels of abstraction, which is a potential problem. The DSLX interpreter should have a \"safe\" mode or throw a warning when multiple channel operations happen in a single tick. IR interpretation/JIT should be performed on IR with the token graph reflecting the order chosen by the channel legalization pass.\u00a0\u21a9</p> </li> <li> <p>If you have send(c0) -&gt; recv(c1) -&gt; send(c10) each scheduled in different cycles and recv(c1) blocks until the first send(c0) goes through, you\u2019ll get deadlock.\u00a0\u21a9</p> </li> <li> <p>Like mutual exclusion analysis, a proof that a send in (tick, cycle) (t[n], c[n]) implies the predicate in (t[n-i], c[n+j]) is false could allow for early sends and simpler logic.\u00a0\u21a9</p> </li> </ol>"},{"location":"design_docs/proc_scoped_channels/","title":"Proc-scoped channels in XLS IR","text":"<ul> <li>Proc-scoped channels in XLS IR<ul> <li>Advantages of proc-scoped channels</li> <li>Disadvantages/complexities:</li> <li>Representation in the IR</li> <li>Incremental roll out</li> </ul> </li> </ul> <p>Written on 2023/5/27</p> <p>Channels in the IR should be changed from package-scoped to proc-scoped. Proc-scoped channels have many advantages which outweigh the significant refactoring effort required to make the change.</p> <p>Currently in XLS IR channels are declared globally at the package level. For example:</p> <pre><code>package foo\n\nchan in_ch(bits[32], id=0, kind=streaming, ops=receive_only)\nchan out_ch(bits[32], id=1, kind=streaming, ops=send_only)\nchan a_ch(bits[32], id=2, kind=streaming, ops=send_receive)\n\nproc bar(t: token, s: ()) {\n  ...\n  rcv: (token, bits[32]) = receive(t, channel_id=0)\n  ...\n  snd0: token = send(t, data0, channel_id=2)\n  ...\n  snd1: token = send(t, data1, channel_id=1)\n  ...\n}\n\nproc quux(t: token, s: ()) {\n  ...\n  rcv: (token, bits[32]) = receive(t, channel_id=2)\n  ...\n}\n</code></pre> <p>All channels have a globally unique name and id. These channels are used inside of procs in send and receive nodes. Each channel can be used by at most a single send node and a single receive node.</p> <p>Changing to proc-scoped channels would change the structure of the IR to more closely match DSLX (one of the proposal\u2019s numerous advantages). Specifically, procs would be organized hierarchically where procs spawn other procs. Each proc has zero or more channel parameters with which the proc is spawned. These channel parameters define the interface of the proc.</p> <p>Possible syntax with proc-scoped channels for the proc example above:</p> <pre><code>package foo\n\nproc quux&lt;in_ch: bits[32]&gt;(t: token, st: ()) {\n  ...\n  rcv: (token, bits[32]) = receive(t, channel=in_ch)\n  ...\n}\n\nproc bar&lt;in_ch: bits[32] in,\n         out_ch: bits[32] out&gt;(t: token, st: ()) {\n  // Channel declaration.\n  chan a_ch(bits[32])\n\n  // Spawn a child proc. Naming the spawn statement (\u201cquux_inst\u201d) provides a\n  // mechanism for referring to a particular proc instance via a path of named\n  // instantiations.\n  quux_inst: spawn quux&lt;a_ch&gt;()\n\n  rcv: (token, bits[32]) = receive(t, channel=in_ch)\n  ...\n  snd0: token = send(t, data0, channel=a_ch)\n  ...\n  snd1: token = send(t, data1, channel=out_ch)\n  ...\n}\n</code></pre>"},{"location":"design_docs/proc_scoped_channels/#advantages-of-proc-scoped-channels","title":"Advantages of proc-scoped channels","text":"<ul> <li>In DSLX, an empty top proc can be used to stitch together procs via spawn     statements (<code>next</code> method is empty). Currently this is not representable in     the IR because the empty top proc has no connections to the spawned procs in     the IR. This means some designs supported in DSLX are not representable in     the IR.</li> </ul> <ul> <li>The hierarchical structure of procs naturally maps to the hierarchical     structure of Verilog modules and the structure of procs in the frontend.</li> </ul> <ul> <li>Avoids channel name collisions. Currently whenever a new channel is created     (for example, during IR conversion or transformation) care must be taken to     avoid name collisions resulting in awkward channel names. The DSLX frontend     for example will name channels based on line and column number in the source     code.. With this change channel names can have shorter, more natural names     which translate to shorter more natural Verilog module port names.</li> </ul> <ul> <li>The channel interface of the proc is clearly defined via the spawn     arguments. These channels become the port interface of the Verilog module     generated for the proc. Grouping the proc interface channels in one place     also provides a natural point at which to specify metadata about the     channels. For example, SRAMs have several different kinds of channels: read     request, write request, read response and write response. These could be     specified as metadata on the proc.</li> </ul> <ul> <li>Multiple instantiations of the same proc only require a single IR definition     of the proc. This could extend down to the emitted Verilog resulting in a     more compact, more easily verified RTL.</li> </ul> <ul> <li>Scoping channels to procs makes it easier to hold a channel pointer in send     and receive nodes. Currently send and receive nodes hold a channel ID which     requires translation to a pointer before using. Associating a pointer held     by a node with a package-scoped construct is difficult in the face of     addition, deletion and modification of procs and nodes.</li> </ul> <ul> <li>Supports unused ports on Verilog modules. This is represented with an unused     spawn channel argument. Currently an unused channel is not associated with     any proc which prevents these channels from representing unused ports.</li> </ul>"},{"location":"design_docs/proc_scoped_channels/#disadvantagescomplexities","title":"Disadvantages/complexities:","text":"<ul> <li>This will be a major refactor which will likely require at least a couple     weeks worth of work.</li> </ul> <ul> <li>With proc scoped channels there is no longer a one-to-one correspondence     between the Proc objects and instantiation of procs in the design. The same     is true for channels. This will require changes in the interpreter and JIT     (at least). Specifying a specific channel/proc instance requires a path of     spawn statements.</li> </ul> <ul> <li>Channels can be declared in a couple ways: in the spawn arguments of the     proc where each argument represents half a channel, and channels can be     declared inside of procs (which represents both sides of the channel).     Likely multiple channel representations in the code will be required.     Perhaps the spawn argument is a reference to one half of a channel, and the     channel declaration in a proc creates the channel object itself.</li> </ul>"},{"location":"design_docs/proc_scoped_channels/#representation-in-the-ir","title":"Representation in the IR","text":"<p>The IR data structures will have to change to support proc-scoped channels. A new construct which is a reference to one side of a channel needs to be added. This construct would be used for the channel parameters of the procs. These are bound to actual channels by spawn statements. The channel metadata (fifo depth, etc) would remain attached to the channel. The channel reference would only hold direction and name.</p> <p>Possible C++ implementation:</p> <pre><code>struct ChannelRef {\n  // In (send) or out (receive).\n  Direction direction;\n  std::string name;\n};\n\nclass Proc {\n  ...\n  // List of channels declared inside the proc.\n  std::vector&lt;std::unique_ptr&lt;Channel&gt;&gt; channel_declarations_;\n\n  // Spawn parameters of this proc. For example:\n  //\n  //   proc foo&lt;a: chan&lt;bits[32]&gt;, b: chan&lt;bits[42]&gt;&gt; (...)\n  //\n  // Spawn parameters are `a` and `b`.\n  std::vector&lt;ChannelRef&gt; spawn_parameters_;\n};\n</code></pre> <p>Send and receive nodes in the IR would naturally correspond to a ChannelRef however it\u2019s not clear whether ChannelRef data structures should be explicitly constructed for the send and receive nodes.</p> <p>Proc-scoped channels break the one-to-one correspondence between definitions of channels (or procs) and instantiations of channels (or procs). Some XLS components such as the interpreter and JIT naturally operate on channel/proc instantiations rather than definitions. To construct instantiations, an elaboration process walks the proc hierarchy and creates instantiation objects for instance of channels and procs. Possible instance implementation:</p> <pre><code>class ChannelInstance {\n  // Pointer to the channel construct in the IR.\n  Channel* channel;\n};\n\nclass ProcInstance {\n  Proc* proc;\n  std::vector&lt;ChannelInstance*&gt; spawn_arguments;\n\n  // Channels declared in this proc instance.\n  std::vector&lt;ChannelInstance&gt; channel_instances;\n\n  // Proc instances spawned by this proc instance.\n  std::vector&lt;ProcInstance&gt; spawned_procs;\n\n  // Proc which spawned this instance.\n  ProcInstance* parent;\n};\n</code></pre> <p>The elaboration process would return a ProcInstance for the top-level proc with child instances underneath.</p> <p>The interpreter and JIT would use this elaboration during execution. Each proc instance gets its own continuation and each channel instance gets its own channel queue.</p> <p>The elaboration would be constructed as needed from the IR rather than trying to maintain a persistent elaboration in sync with the IR.</p> <p>One unanswered question is how the interface channels of the top-level proc should be handled. All channels deeper in the proc hierarchy necessarily have a corresponding channel declaration within a proc in which metadata can be stored. However the top-level proc is by definition not spawned by another proc so the interface channels of the top-level proc have no corresponding channel declarations. There are several possible solutions:</p> <ol> <li> <p>Declare top-level channels at the package level along with a spawn statement     of the top-level proc:</p> <pre><code>package the_package\n\nchan x(bits[32], ...);\nchan y(bits[32], ...);\nspawn the_top_proc&lt;x, y&gt;();\n</code></pre> </li> <li> <p>Create a degenerate proc with no interface channels which spawns the     top-level proc:</p> <pre><code>proc __fake_top&lt;&gt;(t: token, s: ()) {\n  chan x(bits[32], ...);\n  chan y(bits[32], ...);\n  spawn the_top_proc&lt;x, y&gt;();\n\n  next (t, s);\n}\n</code></pre> </li> <li> <p>No top-level channels are declared in IR. Instead top-level Channel objects     are created as part of the elaboration. Any necessary metadata (if there is     any) of the top-level procs would be passed as options to the compilation or     evaluation process.</p> </li> </ol> <p>Option (3) seems the cleanest. (1) requires adding channel and spawn support at the package level as a weird special case. (2) is likely a better option than (1) but it does require carrying around a dummy proc which would need to be identified in some way as the \u201cfake top\u201d.</p>"},{"location":"design_docs/proc_scoped_channels/#incremental-roll-out","title":"Incremental roll out","text":"<p>This change is too large to be done in a single change and should be done incrementally. Below are possible steps for an incremental roll out:</p> <ul> <li>Add bit on Proc indicating it is a new style proc and add new fields to proc     as described above which are only set if the bit is set. Initially, only     support a proc being spawned once. This preserves the one-to-one     correspondence between channels/procs and instantiations of channels/procs.</li> </ul> <ul> <li>Add parsing and serialization support (channel declarations, spawn     statements, etc).</li> </ul> <ul> <li>Add interpreter and JIT support.</li> </ul> <ul> <li>Add pass which converts the old-style procs of a package into a hierarchy of     new style procs.</li> </ul> <ul> <li>Add proc conversion pass to end of pipeline and update codegen to support     new style procs. Then incrementally move the pass earlier and earlier in the     pipeline updating passes as the conversion pass is moved.</li> </ul> <ul> <li>After the conversion pass has been moved to the front of the pipeline,     update the frontends.</li> </ul> <ul> <li>Remove support for old style procs.</li> </ul> <ul> <li>Add elaboration and change interpreter and jit to use elaboration results.     This enables spawning a proc multiple times.</li> </ul>"},{"location":"noc/xls_noc_butterfly_topology/","title":"k-ary n-fly Butterfly Topology Types","text":""},{"location":"noc/xls_noc_butterfly_topology/#overview","title":"Overview","text":"<p>A k-ary n-fly butterfly topology type is a multistage logarithm network. It is implemented using n stages of identical routers, where k is the number of channels of a router that connect to the previous and/or to the next stage. For example, a 2-ary 3-tree butterfly topology has three stages composed of routers with two channels connect to the routers of the previous and/or the next stage (see Figure Butterfly_Topology_Example).</p> <p>Each endpoint node and channel has an n-digit radix-k identifier, {d<sub>n\u22121</sub>, d<sub>n\u22122</sub>, ..., d<sub>0</sub>}. The first n\u22121 digits {d<sub>n\u22121</sub>, d<sub>n\u22122</sub>, ..., d<sub>1</sub>} of the identifier corresponds to the router that it is connected to. Each router node has an n\u22121-digit radix-k identifier. To distinguish nodes and channels from different stages, the stage number is appended to their identifier separated by a period. For example, for a 2-ary 4-fly butterfly network: 2.1010<sub>2</sub> is channel 10<sub>10</sub> from stage 2. Butterfly_Ref_0</p> <p>The connection between the stages is a permutation of the channel identifier. The connection of a channel from stage i-1 to stage i swaps digits d<sub>n\u2212i</sub> and d<sub>0</sub> of the channel identifier, with i &gt;= 1 and i &lt; n. For example, for a 2-ary 4-fly butterfly network: channel 9<sub>10</sub> of stage 1 [1.1001<sub>2</sub>] is connected to router 4 [1.1001<sub>2</sub>] of stage 1 and router 6 [2.1100<sub>2</sub>] of stage 2. Butterfly_Ref_0</p> <p></p> <p>Figure Butterfly_Topology_Example. The router nodes of a 2-ary 3-tree butterfly topology.</p> <p>Figure Butterfly_Topology_Example shows the router nodes of a 2-ary 3-tree butterfly topology. The routers are labelled with an S.I format where S is the stage identifier and I is the router identifier. For example, router 0.2 is router 2 from stage 0. Channel 0.6 is connected to router 3 [0.110<sub>2</sub>] of stage 0 and router 1 of stage 1 [1.011<sub>2</sub>], Channel 1.3 is connected to router 1 [1.011<sub>2</sub>] of stage 1 and to router 1 [1.011<sub>2</sub>] of stage 2.</p>"},{"location":"noc/xls_noc_butterfly_topology/#unidirectional-types","title":"Unidirectional Types","text":"<p>For the unidirectional type of the k-ary n-fly, there are endpoints connected to stage 0 and stage n-1. There are two configurations: 1) the endpoints connected to stage 0 send to the network, and the endpoints connected to stage n-1 receive from the network, and 2) the endpoints connected to stage 0 receive from the network, and the endpoints connected to stage n-1 send to the network. A unidirectional k-ary n-fly butterfly topology has at most k<sup>n</sup> endpoints connected to stage 0 and at most k<sup>n</sup> endpoints connected to stage n-1. Typically, the endpoints nodes connected to stage 0 and stage n-1 are the same nodes, resulting in an equal number of nodes connected to stage 0 and stage n-1. Moreover, the endpoints nodes are connected to stage 0 in the same order as stage n-1 (mirrored from the vertical cross section).</p> <p></p> <p>Figure Unidirectional_Butterfly_Topology_Example. A 2-ary 3-fly butterfly with the endpoints connected to stage 0 sending to the network, and the endpoints connected to stage 2 receiving from the network example.</p> <p>Figure Unidirectional_Butterfly_Topology_Example shows a 2-ary 3-fly butterfly with the endpoints connected to stage 0 sending to the network, and the endpoints connected to stage 2 receiving from the network example.</p>"},{"location":"noc/xls_noc_butterfly_topology/#bidirectional-types","title":"Bidirectional Types","text":"<p>In the bidirectional type, there are endpoints connected to stage 0 or stage n-1. A bidirectional k-ary n-fly butterfly topology has at most k<sup>n</sup> endpoints connected to stage 0 or stage n-1. A bidirectional k-ary n-fly butterfly topology with the endpoints connected to stage n-1 is also referred to as the k-ary n-fly fat tree butterfly topology or k-ary n-tree butterfly fat tree topology Fat_Tree_Ref_0.</p> <p>For context, minimal routing between a pair of nodes on a bidirectional k-ary n-tree can be accomplished by sending the message to one of the nearest common ancestors of both source and destination and from there to the destination. That is, each message experiences two phases, an ascending phase to get to a nearest common ancestor, followed by a descending phase. Fat_Tree_Ref_1</p> <p></p> <p>Figure Bidirectional_Butterfly_Topology_Example. A bidirectional 2-ary 3-fly butterfly with the endpoints connected to stage 0 example.</p> <p>Figure Bidirectional_Butterfly_Topology_Example shows a bidirectional 2-ary 3-fly butterfly with the endpoints connected to stage 0 example.</p>"},{"location":"noc/xls_noc_butterfly_topology/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>k-ary n-flies are implemented by using n stages of     identical routers, where k is the number of channels of a router that     connect to the previous and/or to the next stage.</li> <li>For unidirectional k-ary n-fly, there are     endpoints connected to stage 0     and stage n-1. The     communication flows from stage 0 to stage n-1     or from stage n-1 to     stage 0.</li> <li>For bidirectional k-ary n-fly, there are     endpoints connected to stage 0     or stage n-1. The     communication flows from stage 0 to stage n-1     and from stage n-1 to     stage 0.</li> </ul>"},{"location":"noc/xls_noc_butterfly_topology/#variants","title":"Variants","text":"<p>This section describes variants to the k-ary n-fly butterfly topology.</p>"},{"location":"noc/xls_noc_butterfly_topology/#flattened-butterfly","title":"Flattened Butterfly","text":"<p>The flattened butterfly is derived by flattening the routers in each row of a conventional butterfly topology while maintaining the same inter-router connections. Butterfly_Ref_1</p> <p></p> <p>Figure Flattened_Butterfly_Topology_Example. An example of a 2-ary 3-fly butterfly and its corresponding flattened butterfly topology.</p> <p>Figure Flattened_Butterfly_Topology_Example shows an example of a 2-ary 3-fly butterfly and its corresponding flattened butterfly topology. For each row, routers from stage 0, 1 and 2 are flattened into individual routers. For example, router 0.0, 1.0 and 2.0 are flattened into router 0.</p>"},{"location":"noc/xls_noc_butterfly_topology/#endpoint-connections","title":"Endpoint Connections","text":"<p>As mentioned, typically, for the unidirectional butterfly topology, the endpoints nodes are connected to stage 0 in the same order as stage n-1 (mirrored from the vertical cross section). However, different topologies are said to emerge by modifying the order of the endpoint nodes connected to the network. Such a change in connectivity may also change the routing algorithm. Ludovici et al present a Reduced Unidirectional Fat Tree derived from a k-ary n-fly topology. Fat_Tree_Ref_0</p>"},{"location":"noc/xls_noc_butterfly_topology/#references","title":"References","text":"<p>[Butterfly_Ref_0] William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</p> <p>[Fat_Tree_Ref_0] D. Ludovici et al., \"Assessing fat-tree topologies for regular network-on-chip design under nanoscale technology constraints,\" 2009 Design, Automation &amp; Test in Europe Conference &amp; Exhibition, Nice, 2009, pp. 562-565, doi: 10.1109/DATE.2009.5090727.</p> <p>[Fat_Tree_Ref_1] F. Petrini and M. Vanneschi, \"k-ary n-trees: high performance networks for massively parallel architectures,\" Proceedings 11th International Parallel Processing Symposium, Genva, Switzerland, 1997, pp. 87-93, doi: 10.1109/IPPS.1997.580853.</p> <p>[Butterfly_Ref_1] J. Kim, J. Balfour and W. Dally, \"Flattened Butterfly Topology for On-Chip Networks,\" 40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007), Chicago, IL, 2007, pp. 172-182, doi: 10.1109/MICRO.2007.29.</p>"},{"location":"noc/xls_noc_dimension_order_topology/","title":"Dimension-Order Topology Types","text":""},{"location":"noc/xls_noc_dimension_order_topology/#overview","title":"Overview","text":"<p>The dimension-order topology is a topology where the arrangement of the routers are described by their location in a dimensional space. The topology has a hierarchical structure where dimension n is composed of structures from dimension n-1. One or more endpoint nodes are connected to a router.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#k-ary-n-cube","title":"k-ary n-cube","text":"<p>A k-ary n-cube topology consists of k<sup>n</sup> routers arranged in an n-dimensional cube with k routers along each dimension. Each router is assigned an n-digit radix-k address {a<sub>n\u22121</sub>, ..., a<sub>0</sub>} and is connected to all routers with addresses that differ by \u00b11(mod k) in exactly one address digit. Each dimension is constructed using k k-ary (n-1)-cubes. The channels between the routers can be unidirectional or bidirectional. In practice, the channels are bidirectional.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#ring-topology","title":"Ring Topology","text":"<p>Figure Ring_4ary_1cube_example. A ring topology with four routers, also known as a 4-ary 1-cube topology.</p> <p>The ring topology is a special instance of the k-ary n-cube, where n is equal to one. Figure Ring_4ary_1cube_example shows a ring topology with four routers, also known as a 4-ary 1-cube topology. The dotted lines show the connectivity between routers 0 and k-1 for a dimension.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#symmetric-torus-topology","title":"Symmetric Torus Topology","text":"<p>Figure Symmetric_Torus_4ary_2cube_example. A symmetric torus topology with four routers along each dimension, also known as a 4-ary 2-cube topology.</p> <p>The symmetric torus topology is a special instance of the k-ary n-cube, where n is equal to two. Figure Symmetric_Torus_4ary_2cube_example shows a symmetric torus topology with four routers along each dimension, also known as a 4-ary 2-cube topology. Router (3,0) has address 3 in a dimension and 0 in the other dimension. The dotted lines show the connectivity between routers 0 and k-1 for a dimension.</p> <p></p> <p>Figure 4ary_2cube_construction_example. The construction of a symmetric torus (4-ary 2-cube) topology using 4-ary 1-cubes.</p> <p>Figure 4ary_2cube_construction_example shows the construction of a symmetric torus (4-ary 2-cube) topology using 4-ary 1-cubes (ring topologies).</p>"},{"location":"noc/xls_noc_dimension_order_topology/#multi-radix-n-cube","title":"multi-radix n-cube","text":"<p>A multi-radix n-cube is a generalized form of a k-ary n-cube topology where the radix (number of router nodes) for each dimension is explicitly stated. Each router is assigned an n-digit where the digit at index i has the radix of dimension i. A 2,3-ary 2-cube describes a grid-like topology with a radix of 2 in one dimension and radix of 3 in the other dimension.</p> <p></p> <p>Figure 2,3-ary_2cube_example. A 2,3-ary 2-cube topology example.</p> <p>Figure 2,3-ary_2cube_example shows a 2,3-ary 2-cube topology example. The dotted lines show the connectivity between router 0 and router radix<sub>i </sub>- 1 for each dimension.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#k-ary-n-mesh","title":"k-ary n-mesh","text":"<p>A k-ary n-mesh is a k-ary n-cube topology with the connection from address a<sub>k\u22121</sub> to address a<sub>0</sub> omitted in each dimension.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#line","title":"Line","text":"<p>Figure Line_4ary_1mesh_example. A line topology with four routers, also known as a 4-ary 1-mesh topology.</p> <p>The line topology is a special instance of the k-ary n-mesh, where n is equal to one. Figure Line_4ary_1mesh_example shows a line topology with four routers, also known as a 4-ary 1-mesh topology. Compared to the ring topology in Figure Ring_4ary_1cube_example, the connection from router 0 to router 3 is omitted.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#symmetric-mesh","title":"Symmetric Mesh","text":"<p>Figure Symmetric_Mesh_4ary_2mesh_example. A symmetric mesh topology with four routers along each dimension, also known as a 4-ary 2-mesh topology.</p> <p>The symmetric torus mesh is a special instance of the k-ary n-mesh, where n is equal to two. Figure Symmetric_Mesh_4ary_2mesh_example shows a symmetric mesh topology with four routers along each dimension, also known as a 4-ary 2-mesh topology. Router (1,2) has address 1 in a dimension and 2 in the other dimension. Compared to the torus topology in Figure Symmetric_Torus_4ary_2cube_example, the connections between the pair of routers [(0,0), (0,3)], [(1,0), (1,3)], [(2,0), (2,3)], [(3,0), (3,3)], [(0,0), (3,0)], [(0,1), (3,1)], [(0,2), (3,2)], and [(0,3), (3,3)] are omitted.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#multi-radix-n-mesh","title":"multi-radix n-mesh","text":"<p>Similar to the multi-radix n-cube, a multi-radix n-mesh is a generalized form of a k-ary n-mesh topology where the radix (number of router nodes) for each dimension is explicitly stated. Each router is assigned an n-digit where the digit at index i has the radix of dimension i. A 2,3-ary 2-mesh describes a grid-like topology with a radix of 2 in one dimension and radix of 3 in the other dimension.</p> <p></p> <p>Figure 2,3-ary_2mesh_example. A 2,3-ary 2-mesh topology example.</p> <p>Figure 2,3-ary_2mesh_example shows a 2,3-ary 2-mesh topology example. Compared to the multi-radix n-cube topology in Figure 2,3-ary_2cube_example, the connections from the pair of routers [(0,0), (0,2)], [(1,0), (1,2)], [(0,0), (1,0)], [(0,1), (1,1)] and [(0,2), (1,2)] are omitted.</p>"},{"location":"noc/xls_noc_dimension_order_topology/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>A k-ary n-cube topology consists of k<sup>n</sup>     routers arranged in an n-dimensional cube with k router nodes along each     dimension. Each router is assigned an n-digit radix-k address     {a<sub>n\u22121</sub>, ..., a<sub>0</sub>} and is connected to all routers with     addresses that differ by \u00b11 (mod k) in exactly one address digit. Each     dimension is constructed using k k-ary (n-1)-cubes where there are k     routers in the first dimension.</li> <li>A multi-radix n-cube is a generalized form of a     k-ary n-cube topology where the radix (number of routers) for each     dimension is explicitly stated. Each router is assigned an n-digit where     the digit at index i has the radix of dimension i.</li> <li>A k-ary n-mesh is a k-ary n-cube topology with the     connection from address a<sub>k\u22121</sub> to address a<sub>0</sub> omitted in     each dimension.</li> <li>Similar to the multi-radix n-cube, a     multi-radix n-mesh is a generalized form of a k-ary     n-mesh topology where the radix (number of routers) for each dimension is     explicitly stated. Each router is assigned an n-digit where the digit at     index i has the radix of dimension i.</li> <li>Channel direction can be unidirectional or bidirectional.</li> <li>Ring: k-ary 1-cube</li> <li>Symmetric Torus: k-ary 2-cube</li> <li>Line: k-ary 1-mesh</li> <li>Symmetric Mesh: k-ary 2-mesh</li> </ul>"},{"location":"noc/xls_noc_dimension_order_topology/#references","title":"References","text":"<p>William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</p>"},{"location":"noc/xls_noc_fully_connected_topology/","title":"Fully-connected Topology Types","text":"<p>The fully-connected topology is a topology where each router has a bidirectional channel connecting it to each remaining router of the topology. One or more endpoint nodes are connected to a router.</p> <p></p> <p>Figure Fully_Connected_example. A fully-connected topology with six routers.</p> <p>Figure Fully_Connected_example shows a fully-connected topology with six routers.</p>"},{"location":"noc/xls_noc_fully_connected_topology/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>The fully-connected topology is a topology where each     router has a bidirectional connecting it to each remaining router of the     topology.</li> </ul>"},{"location":"noc/xls_noc_glossary/","title":"XLS-NoC Glossary","text":""},{"location":"noc/xls_noc_glossary/#adaptive-routing","title":"Adaptive Routing","text":"<p>An adaptive routing algorithm derives a route using any information about the network\u2019s state.</p>"},{"location":"noc/xls_noc_glossary/#channel","title":"Channel","text":"<p>A channel interconnects a pair of nodes and is the medium that transfers data amongst the pair. A node is either a router input, a router output or an endpoint port. A channel may be unidirectional, transferring the data in a single direction, or bidirectional, transferring the data in both directions.</p>"},{"location":"noc/xls_noc_glossary/#endpoint","title":"Endpoint","text":"<p>A node connected to the network that communicates to other endpoints using the network.</p>"},{"location":"noc/xls_noc_glossary/#endpoint-port","title":"Endpoint Port","text":"<p>An endpoint port is a port that connects to an endpoint. There are two types of endpoint ports: a send port and a receive port. A send port enables an endpoint to send data to the network and a receive port enables an endpoint to receive data from the network. The naming convention for a send port and receive port is from the perspective of the endpoints.</p>"},{"location":"noc/xls_noc_glossary/#deadlock","title":"Deadlock","text":"<p>A deadlock occurs when a set of agents holding resources are waiting on another set of resources such that a cycle of waiting agents is formed, implying that agents are unable to make progress.</p> <p></p> <p>Figure Deadlock_Example. A deadlock example with two agents and two resources.</p> <p>Figure Deadlock_Example shows a deadlock example with two agents and two resources. In the example, agent 0 is waiting for resource B that is assigned to agent 1, and agent 1 is waiting for resource A that is assigned to agent 0. The dependency cycle created with agent 0 and 1 demonstrate that the agents are unable to make progress.</p>"},{"location":"noc/xls_noc_glossary/#flit","title":"Flit","text":"<p>A flow control digit, or flit, is the smallest unit of resource allocation in a router. Variable length packets are divided into one or more fixed length flits to simplify the management and allocation of resources. Flits may be divided further into phits when traversing a router.</p>"},{"location":"noc/xls_noc_glossary/#flow-control","title":"Flow Control","text":"<p>Flow control is the scheduling and allocation of a network\u2019s resources. For example, a virtual channel can have a wormhole flow control.</p>"},{"location":"noc/xls_noc_glossary/#hotspot-hot-spot","title":"Hotspot (Hot-spot)","text":"<p>A hotspot resource is one whose demand is significantly greater than other, similar resources. For example, a particular destination terminal becomes a hotspot in a shared memory multicomputer when many processors are simultaneously reading from the same memory location (for example, a shared lock or data structure). Another example is traffic congestion within an area of the network.</p>"},{"location":"noc/xls_noc_glossary/#livelock","title":"Livelock","text":"<p>Livelock occurs when a packet is not able to make progress in the network and is never delivered to its destination. Unlike deadlock, a livelocked packet continues to move through the network.</p> <p></p> <p>Figure Livelock_Example. A example with livelocked packet P1.</p> <p>Figure Livelock_Example shows a livelock example. In the example, there are two packets, P0 and P1. The destination router for the packets is router R2. R1 uses an adaptive routing algorithm. At timestep 1, P0 and P1 are at router R0. Given the routing algorithm at R0, R0 routes packet P0 to R2 and P1 to R1 (timestep 2). At timestep 3, another instance of P0 is routed to R0 and P1 is routed to R2. At timestep 4, P1 is routed to R0. After the arrival of P1 at R0. The state of R0 is identical to timestep1 where these sequence of events will repeat. In the end, P1 does not arrive to its destination although it makes progress, thus livelocked.</p>"},{"location":"noc/xls_noc_glossary/#network-on-chip-noc","title":"Network-On-Chip (NoC)","text":"<p>At a high level, a Network-On-Chip (NoC) is a network designed for one chip. It is composed of routers, channels and endpoint ports. It transports data between endpoints connected to it. Although the design is intended for a single chip, the logical description can be partitioned across multiple chips.</p>"},{"location":"noc/xls_noc_glossary/#oblivious-routing","title":"Oblivious Routing","text":"<p>An oblivious routing algorithm derives a route without using any information about the network\u2019s state, where, fundamentally, the route is computed using solely the source and the destination.</p>"},{"location":"noc/xls_noc_glossary/#packet","title":"Packet","text":"<p>Packets are the unit of routing within an interconnection network. Messages are broken into one or more variable, but bounded, length packets for processing by the network. All data contained within a packet follow the same route through the network and packets are reassembled into messages at the destination node. A packet is divided further into flits.</p>"},{"location":"noc/xls_noc_glossary/#phit","title":"Phit","text":"<p>A physical digit, or phit, is the smallest unit of data processed (e.g. traversing or accessed) by a router. One or more phits are combined to form a flit.</p>"},{"location":"noc/xls_noc_glossary/#port","title":"Port","text":"<p>A port is a physical gateway to a component (input port) or from a component (output port).</p>"},{"location":"noc/xls_noc_glossary/#router","title":"Router","text":"<p>A router receives packets on its inputs, determines the packets' destination based on the routing algorithm, and forwards the packets to the appropriate output.</p>"},{"location":"noc/xls_noc_glossary/#routing-algorithm","title":"Routing Algorithm","text":"<p>The series of steps for choosing a path for a packet through the network. For a packet, the routing algorithm determines the router's output from its input.</p>"},{"location":"noc/xls_noc_glossary/#topology","title":"Topology","text":"<p>The static arrangement of router nodes, channels, and endpoint ports in a network. The topology affects the routing in the network.</p>"},{"location":"noc/xls_noc_glossary/#virtual-channel","title":"Virtual Channel","text":"<p>A virtual channel (VC) is a logical representation of a channel at a router's input or output. It is composed of flit buffers within the router. In a router that handles virtual channels, a packet or flit is assigned to a virtual channel. Hence, the presence of virtual channels at a router's input or output enables the transfer of multiple packets through a single channel.</p>"},{"location":"noc/xls_noc_glossary/#wormhole-flow-control","title":"Wormhole Flow Control","text":"<p>Wormhole flow control defines the allocation of a resource at the flit granularity. Upon a successful allocation, the transfer of the flit is permitted to commence.</p>"},{"location":"noc/xls_noc_glossary/#references","title":"References","text":"<p>William James Dally and Brian Patrick Towles. 2004. Principles and Practices of Interconnection Networks. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</p>"},{"location":"noc/xls_noc_readme/","title":"XLS-NoC Project","text":"<p>The XLS-NoC subproject is a project that leverages XLS's facilities for optimizing parameterized hardware designs in the realm of NoC generation.</p> <p>A declarative specification drives the NoC generation process for optimization (e.g. of performance/area tradeoffs), traffic simulations and software-level functional verification, and the creation of (System)Verilog RTL artifacts such as routers and network-interfacing adapter components.</p>"},{"location":"noc/xls_noc_star_topology/","title":"Star Topology Types","text":""},{"location":"noc/xls_noc_star_topology/#overview","title":"Overview","text":"<p>The star topology is a topology with a central router node.</p>"},{"location":"noc/xls_noc_star_topology/#hierarchical-star-topology","title":"Hierarchical Star Topology","text":"<p>The hierarchical star topology is a hierarchical topology with a central router node that is only connected to router nodes, and the endpoint nodes are only connected to leaf routers. It is identical to a bidirectional tree topology with the exception that there are no endpoints connected to the root router. The channels are unidirectional or bidirectional. In practice, it is common that all channels of the network be bidirectional.</p> <p></p> <p>Figure Hierarchical_Star_example. A hierarchical star topology example with eight endpoint nodes and five router nodes.</p> <p>Figure Hierarchical_Star_example shows a hierarchical star topology example with eight endpoint nodes and five router nodes. Router 8 is the central router that is only connected to router nodes 9, 10, 11 and 12. The endpoints [0, 1], [2, 3], [4, 5] and [6, 7] are connected to leaf routers 9, 10, 11 and 12 respectively. All the channels of the network are bidirectional.</p>"},{"location":"noc/xls_noc_star_topology/#star-topology","title":"Star Topology","text":"<p>The star topology is a topology with a single router: the central router node. All communication flows through the central router (all traffic traverses through the central router). The channels are bidirectional.</p> <p></p> <p>Figure Star_example. A star topology example with four endpoint nodes and the central router.</p> <p>Figure Star_example shows a star topology example with four endpoint nodes and the central router.</p>"},{"location":"noc/xls_noc_star_topology/#single-router-topology","title":"Single Router Topology","text":"<p>The single router topology is the star topology with the channels being unidirectional. The single router topology has a single router: the central router node. All communication flows through the central router (all traffic traverses through the central router).</p> <p></p> <p>Figure Single_Router_example. A single router topology example with four endpoint nodes and the central router.</p> <p>Figure Single_Router_example shows a single router topology example with four endpoint nodes and the central router. Compared to Figure Star_example, the channels are unidirectional.</p>"},{"location":"noc/xls_noc_star_topology/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>The hierarchical star topology is a hierarchical     topology with a central router node that is only connected to router nodes,     and the endpoint nodes are only connected to leaf routers.</li> <li>The star topology is a topology with a single router: the central     router node. All traffic traverses through the central router. The channels     are bidirectional.</li> <li>The single router topology is a topology with a single     router: the central router node. All traffic traverses through the central     router. The channels are unidirectional.</li> </ul>"},{"location":"noc/xls_noc_topologies/","title":"XLS-NoC Topologies","text":"<p>The network topology is the static arrangement of router nodes, channels, and endpoints in a network. The document will describe the logical representation of common topology types and define the communication flow within the topology. At the physical level, the arrangement of the topology influences the floorplan, layout and packaging.</p> <p>The supported set of topologies types are:</p> <ul> <li>Dimension Order</li> <li>Tree</li> <li>k-ary n-fly Butterfly</li> <li>Fully Connected</li> <li>Star</li> </ul>"},{"location":"noc/xls_noc_tree_topology/","title":"Tree Topology Types","text":""},{"location":"noc/xls_noc_tree_topology/#overview","title":"Overview","text":"<p>The tree topology is a hierarchical topology with a root node and children nodes. The radix of a node defines the number of children nodes for the given node. The nodes represent the router nodes and endpoint nodes of a network. A tree topology has endpoints connected to the root router and leaf routers. It is called a tree topology because the router nodes form a tree data structure.</p> <p></p> <p>Figure Tree_Topology_Example. A tree topology example.</p> <p>Figure Tree_Topology_Example shows an example of a tree topology. In the figure, there are nine nodes: three router nodes and six endpoint nodes. Router 1 is a root router, and routers 2 and 5 are leaf routers. Router 1 has a radix of four, and routers 2 and 5 have a radix of two. Routers 2 and 5 are child routers of router 1. Endpoints 3 and 4 are child endpoints of router 1. Endpoints 6 and 7, and endpoints 8 and 9 are child endpoints of router 2 and router 5 respectively.</p>"},{"location":"noc/xls_noc_tree_topology/#unidirectional-types","title":"Unidirectional Types","text":"<p>A unidirectional tree is a tree topology where the communication flow is unidirectional, thus all channels in the tree are unidirectional. There are two unidirectional tree types: aggregation tree and distribution tree.</p>"},{"location":"noc/xls_noc_tree_topology/#aggregation-tree","title":"Aggregation Tree","text":"<p>In the aggregation tree, the communication flow is from the leaf routers of the tree to the root router of the tree. The endpoints connected to the root router receive from the network, and the endpoints connected to the leaf routers send to the network.</p> <p></p> <p>Figure Aggregation_Tree_Topology_Example. The aggregation tree topology representation of the tree in Figure Tree_Topology_Example.</p> <p>Figure Aggregation_Tree_Topology_Example shows the aggregation tree topology representation of the tree in Figure Tree_Topology_Example. Endpoints 6, 7, 8 and 9 send to the network, and endpoints 3 and 4 receive from the network.</p>"},{"location":"noc/xls_noc_tree_topology/#distribution-tree","title":"Distribution Tree","text":"<p>In the distribution tree, the communication flow is from the root router of the tree to the leaf routers of the tree. The endpoints connected to the root router send to the network, and the endpoints connected to the leaf routers receive from the network.</p> <p></p> <p>Figure Distribution_Tree_Topology_Example. The distribution tree topology representation of the tree in Figure Tree_Topology_Example.</p> <p>Figure Distribution_Tree_Topology_Example shows the distribution tree topology representation of the tree in Figure Tree_Topology_Example. Endpoints 3 and 4 send to the network, and endpoints 6, 7, 8 and 9 receive from the network.</p>"},{"location":"noc/xls_noc_tree_topology/#bidirectional-type","title":"Bidirectional Type","text":"<p>A bidirectional tree is a tree topology where the communication flow is bidirectional. The communication flows: from the root router of the tree to the leaf routers of the tree and from the leaf routers of the tree to the root router of the tree. By definition, a bidirectional tree requires: 1) at least one endpoint connected to the root router that sends to the network, 2) at least one endpoint connected to the root router that receives from the network, 3) at least one endpoint connected to the leaf routers that sends to the network, and 4) at least one endpoint connected to the leaf routers that receives from the network. In practice, it is common to have all endpoints connected to the root router and leaf routers send to and receive from the network.</p> <p></p> <p>Figure Bidirectional_Tree_Topology_Example. The bidirectional tree topology representation of the tree in Figure Tree_Topology_Example.</p> <p>Figure Bidirectional_Tree_Topology_Example shows the bidirectional tree topology representation of the tree in Figure Tree_Topology_Example. All endpoints in the tree send to and receive from the network.</p>"},{"location":"noc/xls_noc_tree_topology/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>A tree topology has endpoints connected to the root router and leaf     routers.</li> <li>Unidirectional Trees<ul> <li>In the aggregation tree, the communication flow is       from the leaf routers of the tree to the root router of the tree.</li> <li>In the distribution tree, the communication flow       is from the root router of the tree to the leaf routers of the tree.</li> </ul> </li> <li>The communication flow of a bidirectional tree is:     from the root router of the tree to the leaf routers of the tree     and from the leaf routers of     the tree to the root router of the tree.</li> </ul>"},{"location":"optimizations/optimizations/","title":"XLS Optimizations","text":"<ul> <li>XLS Optimizations<ul> <li>Traditional compiler optimizations<ul> <li>Dead Code Elimination (DCE)</li> <li>Common Subexpression Elimination (CSE)</li> <li>Constant Folding</li> <li>Assert Cleanup</li> <li>IO Simplifications</li> <li>Reassociation</li> </ul> </li> <li>Narrowing Optimizations<ul> <li>Select operations</li> <li>Arithmetic and shift operations</li> <li>Comparison operations</li> <li>Known-literals and ArrayIndex</li> </ul> </li> <li>Strength Reductions<ul> <li>Arithmetic Comparison Strength Reductions</li> </ul> </li> <li>\"Simple\" Boolean Simplification</li> <li>Bit-slice optimizations<ul> <li>Slicing sign-extended values</li> </ul> </li> <li>Concat optimizations<ul> <li>Hoisting a reverse above a concat</li> <li>Hoisting a bitwise operation above a concat</li> <li>Merging consecutive bit-slices</li> </ul> </li> <li>Select optimizations<ul> <li>Converting chains of Selects to into a single OneHotSelect</li> <li>Specializing select arms</li> <li>Consecutive selects with identical selectors</li> <li>Sparsifying selects with range analysis</li> </ul> </li> <li>Binary Decision Diagram based optimizations<ul> <li>BDD common subexpression elimination</li> <li>OneHot MSB elimination</li> </ul> </li> </ul> </li> </ul>"},{"location":"optimizations/optimizations/#traditional-compiler-optimizations","title":"Traditional compiler optimizations","text":"<p>Many optimizations from traditional compilers targeting CPUs also apply to the optimization of hardware. Common objectives of traditional compiler optimizations include exposing parallelism, reducing latency, and eliminating instructions. Often these translate directly into the primary objectives of hardware optimization of reducing delay and area.</p>"},{"location":"optimizations/optimizations/#dead-code-elimination-dce","title":"Dead Code Elimination (DCE)","text":"<p>Dead Code Elimination (DCE for short) is usually one of the easiest and most straightforward optimization passes in compilers. The same is true for XLS (the implementation is in <code>xls/passes/dce_pass.*</code>). Understanding the pass is also a good way to familiarize yourself with basics of the compiler IR, how to implement a pass, how to iterate over the nodes in the IR, how to query for node properties and so on.</p> <p>In general, DCE removes nodes from the IR that cannot be reached. Nodes can become unreachable by construction, for example, when a developer writes side-effect-free computations in DSLX that are disconnected from the function return ops. Certain optimization passes may also result in dead nodes.</p> <p>Let's look at the structure of the pass. The header file is straightforward, The <code>DeadCodeEliminationPass</code> is a function-level pass and hence derived from <code>OptimizationFunctionBasePass</code>. Every function-level pass must implement the function <code>RunOnFunctionBaseInternal</code> and return a status indicating whether or not the pass made a change to the IR:</p> <pre><code>class DeadCodeEliminationPass : public OptimizationFunctionBasePass {\n public:\n  DeadCodeEliminationPass()\n      : OptimizationFunctionBasePass(\"dce\", \"Dead Code Elimination\") {}\n  ~DeadCodeEliminationPass() override {}\n\n protected:\n  // Iterate all nodes, mark and eliminate the unvisited nodes.\n  absl::StatusOr&lt;bool&gt; RunOnFunctionBaseInternal(\n      FunctionBase* f, const OptimizationPassOptions&amp; options,\n      PassResults* results) const override;\n};\n</code></pre> <p>Now let's look at the implementation (in file <code>xls/passes/dce_pass.cc</code>). After the function declaration:</p> <pre><code>absl::StatusOr&lt;bool&gt; DeadCodeEliminationPass::RunOnFunctionBaseInternal(\n    FunctionBase* f, const OptimizationPassOptions&amp; options, PassResults* results) const {\n</code></pre> <p>There is a little lambda function testing whether a node is deletable or not:</p> <pre><code>  auto is_deletable = [](Node* n) {\n    return !n-&gt;function_base()-&gt;HasImplicitUse(n) &amp;&amp;\n           !OpIsSideEffecting(n-&gt;op());\n  };\n</code></pre> <p>This function tests for two special classes of nodes.</p> <ul> <li>A node with implicit uses (defined in <code>xls/ir/function.h</code>) is a function's     return value. In case of procs, the return is not reached and may appear as     dead. It must not be removed as XLS expects each function to have a return     value.</li> </ul> <ul> <li>There are a number of side-effecting nodes, such as send/receive operations,     asserts, covers, input / output ports, register read / writes, or parameters     (and a few more). Because of their side effects, they must not be eliminated     by DCE.</li> </ul> <p>Next the pass iterates over all nodes in the function and adds deletable nodes with no users to a worklist. Those are leaf nodes, they are the initial candidates for deletion:</p> <pre><code>  std::deque&lt;Node*&gt; worklist;\n  for (Node* n : f-&gt;nodes()) {\n    if (n-&gt;users().empty() &amp;&amp; is_deletable(n)) {\n      worklist.push_back(n);\n    }\n  }\n</code></pre> <p>Now on to the heart of the DCE algorithm. The algorithm iterates over nodes in the worklist until it is empty, popping elements from the front of the list and potentially adding new elements to the list. For example, assume there was a leaf node A with no further users. Further assume that its operand(s) only have node A as user, then the operand will be added to the worklist and visited in the next iteration over the worklist. There is a minor subtlety here - the code has to ensure that operands are only visited once, hence the use of a <code>flat_hash_set&lt;Node*&gt;</code> to check whether an operand has been visited already.</p> <p>After all operands have been visited and potentially added to the worklist, the original leaf node A is being removed and a corresponding logging statement (level 3) is generated.</p> <pre><code>  int64_t removed_count = 0;\n  absl::flat_hash_set&lt;Node*&gt; unique_operands;\n  while (!worklist.empty()) {\n    Node* node = worklist.front();\n    worklist.pop_front();\n\n    // A node may appear more than once as an operand of 'node'. Keep track of\n    // which operands have been handled in a set.\n    unique_operands.clear();\n    for (Node* operand : node-&gt;operands()) {\n      if (unique_operands.insert(operand).second) {\n        if (HasSingleUse(operand) &amp;&amp; is_deletable(operand)) {\n          worklist.push_back(operand);\n        }\n      }\n    }\n    XLS_VLOG(3) &lt;&lt; \"DCE removing \" &lt;&lt; node-&gt;ToString();\n    XLS_RETURN_IF_ERROR(f-&gt;RemoveNode(node));\n    removed_count++;\n  }\n</code></pre> <p>Finally, a pass has to indicate whether or not it made any changes to the IR. For this pass, this amounts to returning whether or not a single IR node has been DCE'ed:</p> <pre><code>  XLS_VLOG(2) &lt;&lt; \"Removed \" &lt;&lt; removed_count &lt;&lt; \" dead nodes\";\n  return removed_count &gt; 0;\n}\n</code></pre>"},{"location":"optimizations/optimizations/#common-subexpression-elimination-cse","title":"Common Subexpression Elimination (CSE)","text":"<p>Common subexpression elimination is another example of a classic compiler optimization that equally applies to high-level synthesis. The heuristics on which specific expressions to commonize may differ, given that commonizing expressions can increase fan-out and connectivity of the IR, complicating place and route. Currently, XLS is greedy and does not apply heuristics. It commonizes any and all common expressions it can find. The CSE implementation can be found in files <code>xls/passes/cse_pass.*</code>.</p> <p>What does CSE actually do? The principles are quite simple and similar in classic control-flow based compilers. Yet, as mentioned, the heuristics may be moderately different. In CFG-based IRs, CSE would look for common expressions and substitute in temporary variables. For example, for code like this with the common expression <code>a+b</code>:</p> <pre><code>   x = a + b + c;\n   if (cond) {\n      ...\n   } else {\n      y = b + a;\n   }\n</code></pre> <p>The compiler would first determine that addition is commutative and that hence the expressions <code>a+b</code> and <code>b+a</code> can be canonicalized, eg., by ordering the operands alphabetically. Then the compiler would introduce a temporary variable for the expression and forward-substitute it into all occurances. For the example, the resulting code would be something like this:</p> <pre><code>   t1 = a + b\n   x = t1 + c;\n   if (cond) {\n      ...\n   } else {\n      y = t1;\n   }\n</code></pre> <p>In effect, an arithmetic operation has been traded against a register (or cache) access. Even in classic compilers, the CSE heuristics may consider factors such as the length and number of live ranges (and corresponding register pressure) to determine whether or not it may be better to just recompute the expression.</p> <p>In XLS's \"sea of nodes\" IR, this transformation is quite simple. Given a graph that contains multiple common subexpressions, for example:</p> <pre><code>   A   B\n    \\ /\n     C1      A   B\n      \\       \\ /\n       ...     C2\n               /\n             op(C2)\n</code></pre> <p>XLS would find that <code>C1</code> and <code>C2</code> compute identical expressions and would simply replace the use of <code>C2</code> with <code>C1</code>, as in this graph (which will result in <code>C2</code> being dead-code eliminated):</p> <pre><code>   A   B\n    \\ /\n     C1      A   B\n      \\       \\ /\n       ...     C2\n         \\\n        op(C1)\n</code></pre> <p>Now let's see how this is implemented in XLS. CSE is a function-level transformation and accordingly the pass is derived from <code>OptimizationFunctionBasePass</code>. In the header file:</p> <pre><code>class CsePass : public OptimizationFunctionBasePass {\n public:\n  CsePass() : OptimizationFunctionBasePass(\"cse\", \"Common subexpression elimination\") {}\n  ~CsePass() override {}\n\n protected:\n  absl::StatusOr&lt;bool&gt; RunOnFunctionBaseInternal(\n      FunctionBase* f, const OptimizationPassOptions&amp; options,\n      PassResults* results) const override;\n};\n</code></pre> <p>Several other optimizations passes expose new CSE opportunities. To make it easy to call CSE from these other passes, we declare a standalone function to call it. It accepts as input the function and returns a map containing the potential replacements of one node with another:</p> <pre><code>absl::StatusOr&lt;bool&gt; RunCse(FunctionBase* f,\n                            absl::flat_hash_map&lt;Node*, Node*&gt;* replacements);\n</code></pre> <p>CSE conceptually has to check for each op and its operands whether or not a similar op exists somewhere else in the IR. To make this more efficient, XLS first computes a 64-bit hash for each node. It combines the nodes' opcode with all operands' IDs into a vector and computes the hash function over this vector.</p> <pre><code>  auto hasher = absl::Hash&lt;std::vector&lt;int64_t&gt;&gt;();\n  auto node_hash = [&amp;](Node* n) {\n    std::vector&lt;int64_t&gt; values_to_hash = {static_cast&lt;int64_t&gt;(n-&gt;op())};\n    std::vector&lt;Node*&gt; span_backing_store;\n    for (Node* operand : GetOperandsForCse(n, &amp;span_backing_store)) {\n      values_to_hash.push_back(operand-&gt;id());\n    }\n    // If this is slow because of many literals, the Literal values could be\n    // combined into the hash. As is, all literals get the same hash value.\n    return hasher(values_to_hash);\n  };\n</code></pre> <p>Note that this procedure uses the function <code>GetOperandsForCse</code> to collect the operands. What does this function do? For nodes to be considered as equivalent, the operands must be in the same order. Commutative operands are agnostic to operand order. So in order to expand the opportunities for CSE, XLS sorts commutative operands by their ID. As an optimization, to avoid having to construct and return a full vector for each node and operands, the function gets a parameter to a vector of nodes to use as storage and returns a <code>absl::Span&lt;Node * const&gt;</code> over this backing store. This may look a bit confusing in the code but is really just a performance optimization:</p> <pre><code>absl::Span&lt;Node* const&gt; GetOperandsForCse(\n    Node* node, std::vector&lt;Node*&gt;* span_backing_store) {\n  CHECK(span_backing_store-&gt;empty());\n  if (!OpIsCommutative(node-&gt;op())) {\n    return node-&gt;operands();\n  }\n  span_backing_store-&gt;insert(span_backing_store-&gt;begin(),\n                             node-&gt;operands().begin(), node-&gt;operands().end());\n  SortByNodeId(span_backing_store);\n  return *span_backing_store;\n}\n</code></pre> <p>Now on to the meat of the optimization pass. As always, we have to maintain whether or not the pass modified the IR:</p> <pre><code>bool changed = false;\n</code></pre> <p>We store each first occurance of an expression in a map which is indexed by the expression's hash value. Since potentially there is no redundancy in the IR, we can pre-allocate this map to the size of function's IR. Note that non-common expressions may result in the same hash value. Because of that, <code>node_buckets</code> is a map from the hash value to a vector of nodes with the same hash value:</p> <pre><code>absl::flat_hash_map&lt;int64_t, std::vector&lt;Node*&gt;&gt; node_buckets;\nnode_buckets.reserve(f-&gt;node_count());\n</code></pre> <p>Now we iterate over the nodes in the IR, ignoring nodes that have side effects:</p> <pre><code>  for (Node* node : TopoSort(f)) {\n    if (OpIsSideEffecting(node-&gt;op())) {\n      continue;\n    }\n</code></pre> <p>First thing to check is whether or not the op represents an expression that we have potentially already seen. If this is the first occurrance of the expression, which is efficient to check via the hash value, we store the op in <code>node_buckets</code> and continue with the next node.</p> <pre><code>    int64_t hash = node_hash(node);\n    if (!node_buckets.contains(hash)) {\n      node_buckets[hash].push_back(node);\n      continue;\n    }\n</code></pre> <p>Now it is getting more interesting. We may have found a node that is common with a previously seen node. We collect the nodes operands (again, this looks a bit complicated because of the performance optimization):</p> <pre><code>    std::vector&lt;Node*&gt; node_span_backing_store;\n    absl::Span&lt;Node* const&gt; node_operands_for_cse =\n        GetOperandsForCse(node, &amp;node_span_backing_store);\n</code></pre> <p>Then we iterate over all previously seen nodes with the same hash value which are stored in <code>node_buckets</code>. Again, the may be multiple expressions with the same hash value, hence we have to iterate over all candidates with that hash value.</p> <p>For each candidate, we collect the operands and then check whether the node is definitely identical to a previously seen node. In this case the node's uses can be replaced:</p> <pre><code>    for (Node* candidate : node_buckets.at(hash)) {\n      std::vector&lt;Node*&gt; candidate_span_backing_store;\n      if (node_operands_for_cse ==\n              GetOperandsForCse(candidate, &amp;candidate_span_backing_store) &amp;&amp;\n          node-&gt;IsDefinitelyEqualTo(candidate)) {\n          [...]\n</code></pre> <p>If it was a match we replace the nodes, fill in the resulting replacement map, and mark the IR as modified. We also note whether a true match was found (via <code>replaced</code>):</p> <pre><code>        XLS_VLOG(3) &lt;&lt; absl::StreamFormat(\n            \"Replacing %s with equivalent node %s\", node-&gt;GetName(),\n            candidate-&gt;GetName());\n        XLS_RETURN_IF_ERROR(node-&gt;ReplaceUsesWith(candidate));\n        if (replacements != nullptr) {\n          (*replacements)[node] = candidate;\n        }\n        changed = true;\n        replaced = true;\n        break;\n      }\n    }\n</code></pre> <p>If, however, it turns out that while the hash value was identical but the ops were not identical, we have to update <code>node_buckets</code> and insert the new candidate.</p> <pre><code>    if (!replaced) {\n      node_buckets[hash].push_back(node);\n    }\n</code></pre> <p>As a final step, we return whether or not the IR was modified:</p> <pre><code>  return changed;\n</code></pre>"},{"location":"optimizations/optimizations/#constant-folding","title":"Constant Folding","text":"<p>Constant folding is another classic compiler optimization that equally applies to high-level synthesis. What does it do? Given an arithmetic expression that as inputs only as constant values, the compiler computes this expression at compile time and replaces it with the result. There are two complexities:</p> <ol> <li> <p>The IR has to be updated after the transformation.</p> </li> <li> <p>More importantly, the evaluation of the expression must match the semantics     of the target architecture!</p> </li> </ol> <p>Both problems are solved elegantly in XLS. The IR update itself is trivial to do with the sea-of-nodes IR. For expression evaluation, XLS simply re-uses the interpreter, which implements the correct semantics. Let's look again at the implementation, which is in file <code>xls/passes/constant_folding_pass.*</code>.</p> <p>We define the pass as usual and maintain whether or not the pass modified the IR in a boolean variable <code>changed</code>:</p> <pre><code>absl::StatusOr&lt;bool&gt; ConstantFoldingPass::RunOnFunctionBaseInternal(\n    FunctionBase* f, const OptimizationPassOptions&amp; options, PassResults* results) const {\n  bool changed = false;\n</code></pre> <p>We now iterate over all nodes in the IR and check whether the node only has literals as operands as well as whether it is safe to replace the node.</p> <pre><code>  for (Node* node : TopoSort(f)) {\n    [...]\n    if (!node-&gt;Is&lt;Literal&gt;() &amp;&amp; !TypeHasToken(node-&gt;GetType()) &amp;&amp;\n        !OpIsSideEffecting(node-&gt;op()) &amp;&amp;\n        std::all_of(node-&gt;operands().begin(), node-&gt;operands().end(),\n                    [](Node* o) { return o-&gt;Is&lt;Literal&gt;(); })) {\n      XLS_VLOG(2) &lt;&lt; \"Folding: \" &lt;&lt; *node;\n</code></pre> <p>Here now comes the fun part. If the condition is true, XLS simply collects the operands in a vector and calls the interpreter to compute the result. Again, the interpreter has to implement the proper semantics, which leads to this exceedingly simple implementation.</p> <pre><code>      std::vector&lt;Value&gt; operand_values;\n      for (Node* operand : node-&gt;operands()) {\n        operand_values.push_back(operand-&gt;As&lt;Literal&gt;()-&gt;value());\n      }\n      XLS_ASSIGN_OR_RETURN(Value result, InterpretNode(node, operand_values));\n      XLS_RETURN_IF_ERROR(node-&gt;ReplaceUsesWithNew&lt;Literal&gt;(result).status());\n      changed = true;\n</code></pre>"},{"location":"optimizations/optimizations/#assert-cleanup","title":"Assert Cleanup","text":"<p>Asserts whose condition is known to be true (represented by a 1-bit value of 1) are removed as they will never trigger. This pass is implemented in file <code>xls/passes/useless_assert_removel_pass.*</code> and is rather trivial. Again, it shows in a simple way how to navigate the IR:</p> <p>https://github.com/google/xls/blob/main/xls/passes/useless_assert_removal_pass.cc#L27-L43</p>"},{"location":"optimizations/optimizations/#io-simplifications","title":"IO Simplifications","text":"<p>Conditional sends and receives that have a condition known to be false are replaced with their input token (in the case of sends) or a tuple containing their input token and a literal representing the zero value of the appropriate channel (in the case of receives). Conditional sends and receives that have a condition known to be true are replaced with unconditional sends and receives. These two transforms are implemented in file <code>xls/passes/useless_io_removal_pass.*</code>.</p>"},{"location":"optimizations/optimizations/#reassociation","title":"Reassociation","text":"<p>Reassociation in XLS uses the associative and commutative property of arithmetic operations (such as adds and multiplies) to rearrange expressions of identical operations to minimize delay and area. Delay is reduced by transforming chains of operations into balanced trees which reduces the critical-path delay. For example, given the following expression:</p> <p></p> <p>This can be reassociated into the following balanced tree:</p> <p></p> <p>The transformation has reduced the critical path through the expression from three adds down to two adds.</p> <p>Reassociation can also create opportunities for constant folding. If an expression contains multiple literal values (constants) the expressions can be reassociated to gather literals into the same subexpression which can then be folded. Generally this requires the operation to be commutative as well as associative. For example, given the following expression:</p> <p></p> <p>This can be reassociated into:</p> <p></p> <p>The right-most add of the two literals can be folded reducing the number of adds in the expression to two.</p>"},{"location":"optimizations/optimizations/#narrowing-optimizations","title":"Narrowing Optimizations","text":"<p>The XLS compiler performs bitwise flow analysis, and so can deduce that certain bits in the output of operations are always-zero or always-one. As a result, after running this bit-level tracking flow analysis, some of the bits on the output of an operation may be \"known\". With known bits in the output of an operation, we can often narrow the operation to only produce the unknown bits (those bits that are not known static constants), which reduces the \"cost\" of the operation (amount of delay through the operation) by reducing its bitwidth.</p>"},{"location":"optimizations/optimizations/#select-operations","title":"Select operations","text":"<p>An example of this is select narrowing, as shown in the following -- beforehand we have three operands, but from our bitwise analysis we know that there are two constant bits in the MSb as well as two constant bits in the LSb being propagated from all of our input operands.</p> <p></p> <p>Recognizing that property, we squeeze the one hot select operation -- in the \"after\" diagram below observe we've narrowed the operation by slicing the known-constant bits out of the one hot select operation, making it cheaper in terms of delay, and propagated the slices up to the input operands -- these slices being presented at the output of the operands may in turn let them narrow their operation and become cheaper, and this can continue transitively):</p> <p></p>"},{"location":"optimizations/optimizations/#arithmetic-and-shift-operations","title":"Arithmetic and shift operations","text":"<p>Most arithmetic ops support mixed bit widths where the operand widths may not be the same width as each other or the result. This provides opportunities for narrowing. Specifically (for multiplies, adds and subtracts):</p> <ol> <li> <p>If the operands are wider than the result, the operand can be truncated to     the result width.</p> </li> <li> <p>If the operation result is wider than the full-precision width of the     operation, the operation result can be narrowed to the full-precision width     then sign- or zero-extended (depending upon the sign of the operation) to     the original width to produce the desired result. The full-precision width     of an add or subtract is one more than the width of the widest operand, and     the full-precision width of a multiply is the sum of the operand widths.</p> </li> <li> <p>If the most-significant bit of an operand are zeros (for unsigned     operations) or the same as the sign-bit value (for signed operations), the     operand can be narrowed to remove these known bits.</p> </li> </ol> <p>As a special case, adds can be narrowed if the least-significant bits of an is all zeros. The add operation is narrowed to exclude this range of least-significant bits. The least-significant bits of the result are simply the least-significant bits of the non-zero operand:</p> <p></p> <p>Similarly, if the most-significant bits of the shift-amount of a shift operation are zero the shift amount can be narrowed.</p>"},{"location":"optimizations/optimizations/#comparison-operations","title":"Comparison operations","text":"<p>Leading and trailing bits can be stripped from the operands of comparison operations if these bits do not affect the result of the comparison. For unsigned comparisons, leading and trailing bits which are identical between the two operands can be stripped which narrows the comparison and reduces the cost of the operation.</p> <p>Signed comparison are more complicated to handle because the sign bit (most-significant bit) affects the interpretation of the value of the remaining bits. Stripping leading bits must preserve the sign bit of the operand.</p>"},{"location":"optimizations/optimizations/#known-literals-and-arrayindex","title":"Known-literals and <code>ArrayIndex</code>","text":"<p>The narrowing pass also converts any node that is determined by range analysis to have a range containing only one value into a literal. As a further extension of this idea, it also converts <code>ArrayIndex</code> nodes that are determined to have a small number of possible indices into select chains.</p> <p>This latter optimization is sometimes harmful, so we currently hide it behind the <code>--convert_array_index_to_select=&lt;n&gt;</code> flag in <code>opt_main</code> and <code>benchmark_main</code>, where <code>&lt;n&gt;</code> controls the number of possible array indices above which the optimization does not fire. The right number to put there is highly contextual, since this optimization relies heavily on later passes to clean up its output.</p>"},{"location":"optimizations/optimizations/#strength-reductions","title":"Strength Reductions","text":""},{"location":"optimizations/optimizations/#arithmetic-comparison-strength-reductions","title":"Arithmetic Comparison Strength Reductions","text":"<p>When arithmetic comparisons occur with respect to constant values, comparisons which are be arithmetic in the general case may be strength reduced to more boolean-analyzeable patterns; for example, comparison with mask constants:</p> <pre><code>u4:0bwxyz &gt; u4:0b0011\n</code></pre> <p>Can be strength reduced -- for the left hand side to be greater one of the <code>wx</code> bits must be set, so we can simply replace this with an or-reduction of <code>wx</code>. Similarly, a trailing-bit and-reduce is possible for less-than comparisons.</p> <p>NOTE These examples highlight a set of optimizations that are not applicable (profitable) on traditional CPUs: the ability to use bit slice values below what we'd traditionally think of as a fundamental comparison \"instruction\", which would nominally take a single cycle.</p>"},{"location":"optimizations/optimizations/#simple-boolean-simplification","title":"\"Simple\" Boolean Simplification","text":"<p>NOTE This optimization is a prelude to a more general optimization we expect to come in the near future that is based on Binary Decision Diagrams. It is documented largely as a historical note of an early / simple optimization approach.</p> <p>With the addition of the <code>one_hot_select</code> and its corresponding optimizations, a larger amount of boolean logic appears in XLS's optimized graphs (e.g. or-ing together bits in the selector to eliminate duplicate <code>one_hot_select</code> operands). Un-simplified boolean operations compound their delay on the critical path; with the process independent constant \\(\\tau\\) a single bit <code>or</code> might be \\(6\\tau\\), which is just to say that having lots of dependent <code>or</code> operations can meaningfully add up in the delay estimation for the critical path. If we can simplify several layers of boolean operations into one operation (say, perhaps with inputs optionally inverted) we could save a meaningful number of tau versus a series of dependent boolean operations.</p> <p>For a simple approach to boolean simplification:</p> <ul> <li>The number of parameters to the boolean function is limited to three inputs</li> <li>The truth table is computed for the aggregate boolean function by flowing     input bit vectors through all the boolean operation nodes.</li> <li>The result bit vectors on the output frontier are matched the resulting     truth table from the flow against one of our standard operations (perhaps     with the input operands inverted).</li> </ul> <p>The algorithm starts by giving <code>x</code> and <code>y</code> their vectors (columns) from the truth table, enumerating all possible bit combinations for those operands. For example, consider two operands and a (bitwise) boolean function, the following is the truth table:</p> <pre><code>X Y | X+~Y\n----+------\n0 0 |  1\n0 1 |  0\n1 0 |  1\n1 1 |  1\n</code></pre> <p>Each column in this table is a representation of the possibilities for a node in the graph to take on, as a vector. After giving the vector <code>[0, 0, 1, 1]</code> to the first input node (which is arbitrarily called X) and the vector <code>[0, 1, 0, 1]</code> to the second input node (which is arbitrarily called Y), and flowing those bit vectors through a network of boolean operations, if you wind up with a vector <code>[1, 0, 1, 1]</code> at the end, it is sound to replace that whole network with the expression <code>X+~Y</code>. Similarly, if the algorithm arrived at the vector <code>[1, 1, 1, 1]</code> at the end of the network, you could replace the result with a literal <code>1</code>, because it has been proven for all input operand possibilities the result is always <code>1</code> in every bit. Effectively, this method works by brute force enumerating all the possibilities for input bits and operating on all of those possibilities at the same time. In the end, the algorithm arrives at a composite boolean function that can be pattern matched against XLS's set of \"simple boolean functions\".</p> <p>In the following example there are two nodes on the \"input frontier\" for the boolean operations (<code>sub</code> and <code>add</code>, which we \"rename\" to <code>x</code> and <code>y</code> for the purposes of analysis).</p> <p></p> <p>As shown in the picture, the algorithm starts flowing the bit vector, which represents all possible input values for <code>x</code> and <code>y</code>. You can see that the <code>not</code> which produces \\(\\bar{x}\\) (marked with a red star) simply inverts all the entries in the vector and corresponds to the \\(\\bar{x}\\) column in the truth table. Similarly the <code>and</code> operation joins the two vector with the binary <code>&amp;</code> operation, and finally we end up with the blue-starred bit vector on the \"output frontier\", feeding the dependent <code>one_hot_select</code> (marked as <code>ohs</code> in the diagram).</p> <p>When we resolve that final result bit vector with the blue star against our table of known function substitutions, we see that the final result can be replaced with a node that is simply <code>or(x, y)</code>, saving two unnecessary levels of logic, and reducing the critical path delay in this example from something like \\(13\\tau\\) to something like \\(6\\tau\\).</p> <p>This basic procedure is then extended to permit three variables on the input frontier to the boolean expression nodes, and the \"known function\" table is extended to include all of our supported logical operators (i.e. <code>nand</code>, <code>nor</code>, <code>xor</code>, <code>and</code>, <code>or</code>) with bit vectors for all combinations of inputs being present, and when present, either asserted, or their inversions (e.g. we can find \\(nand(\\bar{X}, Y)\\) even though X is inverted).</p>"},{"location":"optimizations/optimizations/#bit-slice-optimizations","title":"Bit-slice optimizations","text":"<p>Bit-slice operations narrow values by selecting a contiguous subset of bits from their operand. Bit-slices are zero-cost operations as no computation is performed. However, optimization of bit-slices can be beneficial as bit-slices can interfere with optimizations and hoisting bit-slices can narrow other operations reducing their computation cost.</p>"},{"location":"optimizations/optimizations/#slicing-sign-extended-values","title":"Slicing sign-extended values","text":"<p>A bit-slice of a sign-extended value is a widening operation followed by a narrowing operation and can be optimized. The details of the transformation depends upon relative position of the slice and the sign bit of the original value. Let <code>sssssssXXXXXXXX</code> be the sign-extended value where <code>s</code> is the sign bit and <code>X</code> represents the bits of the original value. There are three possible cases:</p> <ol> <li> <p>Slice is entirely within the sign-extend operand.</p> <p></p> <p>Transformation: replace the bit-slice of the sign-extended value with a   bit-slice of the original value.</p> </li> <li> <p>Slice spans the sign bit of the sign-extend operand.</p> <p></p> <p>Transformation: slice the most significant bits from the original value   and sign-extend the result.</p> </li> <li> <p>Slice is entirely within the sign-extended bits.</p> <p></p> <p>Transformation: slice the sign bit from the original value and   sign-extend it.</p> </li> </ol> <p>To avoid introducing additional sign-extension operations cases (2) and (3) should only be performed if the bit-slice is the only user of the sign-extension.</p>"},{"location":"optimizations/optimizations/#concat-optimizations","title":"Concat optimizations","text":"<p><code>Concat</code> (short for concatenation) operations join their operands into a single word. Like <code>BitSlice</code>s, <code>Concat</code> operations have no cost since since they simply create a new label to refer to a set of bits, performing no actual computation. However, <code>Concat</code> optimizations can still provide benefit by reducing the number of IR nodes (increases human readability) or by refactoring the IR in a way that allows other optimizations to be applied. Several <code>Concat</code> optimizations involve hoisting an operation on one or more <code>Concat</code>s to above the <code>Concat</code> such that the operation is applied on the <code>Concat</code> operands directly. This may provide opportunities for optimization by bringing operations which actually perform logic closer to other operations performing logic.</p>"},{"location":"optimizations/optimizations/#hoisting-a-reverse-above-a-concat","title":"Hoisting a reverse above a concat","text":"<p>A <code>Reverse</code> operation reverses the order of the bits of the input operand. If a <code>Concat</code> is reversed and the <code>Concat</code> has no other consumers except for reduction operations (which are not sensitive to bit order), we hoist the <code>Reverse</code> above the <code>Concat</code>. In the modified IR, the <code>Concat</code> input operands are <code>Reverse</code>'d and then concatenated in reverse order, e.g. :</p> <pre><code>Reverse(Concat(a, b, c))\n=&gt; Concat(Reverse(c), Reverse(b), Reverse(a))\n</code></pre>"},{"location":"optimizations/optimizations/#hoisting-a-bitwise-operation-above-a-concat","title":"Hoisting a bitwise operation above a concat","text":"<p>If the output of multiple <code>Concat</code> operations are combined with a bitwise operation, the bitwise operation is hoisted above the <code>Concat</code>s. In the modified IR, we have a single <code>Concat</code> whose operands are bitwise'd <code>BitSlice</code>s of the original <code>Concat</code>s, e.g. :</p> <pre><code>Or(Concat(A, B), Concat(C, D)), where A,B,C, and D are 1-bit values\n=&gt; Concat(Or(Concat(A, B)[1], Concat(C, D)[1]), Or(Concat(A, B)[0], Concat(C, D)[0]))\n</code></pre> <p>In the case that an added <code>BitSlice</code> exactly aligns with an original <code>Concat</code> operand, other optimizations (bit slice simplification, constant folding, dead code elimination) will replace the <code>BitSlice</code> with the operand, e.g. for the above example:</p> <pre><code>=&gt; Concat(Or(A, C), Or(B, D))\n</code></pre>"},{"location":"optimizations/optimizations/#merging-consecutive-bit-slices","title":"Merging consecutive bit-slices","text":"<p>If consecutive <code>Concat</code> operands are consecutive <code>BitSlice</code>s, we create a new, merged <code>BitSlice</code> spanning the range of the consecutive <code>BitSlice</code>s. Then, we create a new <code>Concat</code> that includes this <code>BitSlice</code> as an operand, e.g.</p> <pre><code>Concat(A[3:2], A[1:0], B)\n=&gt; Concat(A[3:0], B)\n</code></pre> <p>This optimization is sometimes helpful and sometimes harmful, e.g. in the case that there are other consumers of the original <code>BitSlice</code>s, we may only end up adding more IR nodes since the original <code>BitSlice</code>s will not be removed by DCE once they are replaced in the <code>Concat</code>. Some adjustments might be able to help with this issue. An initial attempt at this limited the application of this optimization to cases where the given<code>Concat</code> is the only consumer of the consecutive <code>BitSlice</code>s. This limited the more harmful applications of this optimization, but also reduced instances in which the optimization was beneficially applied (e.g. the same consecutive <code>BitSlice</code>s could be merged in multiple <code>Concat</code>s).</p>"},{"location":"optimizations/optimizations/#select-optimizations","title":"Select optimizations","text":"<p>XLS supports two types of select operations:</p> <ol> <li> <p><code>Select</code> (opcode <code>Op::kSel</code>) is a traditional multiplexer. An <code>n</code>-bit     binary-encoded selector chooses among 2**<code>n</code> inputs.</p> </li> <li> <p><code>OneHotSelect</code> (opcode <code>Op::kOneHotSel</code>) has one bit in the selector for     each input. The output of the operation is equal to the logical-or reduction     of the inputs corresponding to the set bits of the selector. Generally, a     <code>OneHotSelect</code> is lower latency and area than a <code>Select</code> as a <code>Select</code> is     effectively a decode operation followed by a <code>OneHotSelect</code>.</p> </li> </ol>"},{"location":"optimizations/optimizations/#converting-chains-of-selects-to-into-a-single-onehotselect","title":"Converting chains of <code>Select</code>s to into a single <code>OneHotSelect</code>","text":"<p>A linear chain of binary <code>Select</code> operations may be produced by the front end to select amongst a number of different values. This is equivalent to nested ternary operators in C++.</p> <p></p> <p>A chain of <code>Select</code>s has high latency, but latency may be reduced by converting the <code>Select</code> chain into a single <code>OneHotSelect</code>. This may only be performed if the single-bit selectors of the <code>Select</code> instructions are one-hot (at most one selector is set at one time). In this case, the single-bit <code>Select</code> selectors are concatenated together to produce the selector for the one hot.</p> <p></p> <p>This effectively turns a serial operation into a lower latency parallel one. If the selectors of the original <code>Select</code> instructions can be all zero (but still at most one selector is asserted) the transformation is slightly modified. An additional selector which is the logical NOR of all of the original <code>Select</code> selector bits is appended to the <code>OneHotSelect</code> selector and the respective case is the value selected when all selector bits are zero (<code>case_0</code> in the diagram).</p>"},{"location":"optimizations/optimizations/#specializing-select-arms","title":"Specializing select arms","text":"<p>Within any given arm of a <code>Select</code> multiplexer, we can assume that the selector has the specific value required to select that arm. This assumption is safe because in the event that the selector has a different value, the arm is dead.</p> <p></p> <p>This makes it possible to try specialize the arms based on the selector value. In the above example, a LHS of <code>Selector + x</code> could be simplified to <code>0 + x</code>.</p> <p>The current optimization simply substitutes any usages of the selector in a <code>Select</code> arm with its known value in that arm. Future improvements could use range based analysis or other techniques to narrow down the possible values of any variables within the selector, and use that information to optimize the select arms.</p>"},{"location":"optimizations/optimizations/#consecutive-selects-with-identical-selectors","title":"Consecutive selects with identical selectors","text":"<p>Two consecutive two-way selects which use the same selector can be compacted into a single select statement. The selector only has two states so only two of the three different cases may be selected. The third is dead. Visually, the transformation looks like:</p> <p></p> <p>The specific cases which remain in the new select instruction depends on whether the upper select feeds the true or false input of the lower select.</p>"},{"location":"optimizations/optimizations/#sparsifying-selects-with-range-analysis","title":"Sparsifying selects with range analysis","text":"<p>If range analysis determines that the selector of a select has fewer possible values than the number of cases, we can do a form of dead code elimination to remove the impossible cases.</p> <p>Currently, we do this in the following way:</p> <pre><code>// Suppose bar has interval set {[1, 4], [6, 7], [10, 13]}\nfoo = sel(bar, cases=[a1, ..., a16])\n// Sparsification will lead to the following code:\nfoo = sel((bar &gt;= 1) &amp;&amp; (bar &lt;= 4), cases=[\n  sel((bar &gt;= 6) &amp;&amp; (bar &lt;= 7), cases=[\n    sel(bar - 10, cases=[a11, ..., a14], default=0),\n    sel(bar - 6, cases=[a7, a8], default=0)\n  ]),\n  sel(bar - 1, cases=[a2, ..., a5], default=0)\n])\n</code></pre> <p>This adds a little bit of code for the comparisons and subtractions but is generally worth it since eliminating a case branch can be a big win.</p>"},{"location":"optimizations/optimizations/#binary-decision-diagram-based-optimizations","title":"Binary Decision Diagram based optimizations","text":"<p>A binary decision diagram (BDD) is a data structure that can represent arbitrary boolean expressions. Properties of the BDD enable easy determination of relationships between different expressions (equality, implication, etc.) which makes them useful for optimization and analysis</p>"},{"location":"optimizations/optimizations/#bdd-common-subexpression-elimination","title":"BDD common subexpression elimination","text":"<p>Determining whether two expression are equivalent is trivial using BDDs. This can be used to identify operations in the graph which produce identical results. BDD CSE is an optimization pass which commons these equivalent operations.</p>"},{"location":"optimizations/optimizations/#onehot-msb-elimination","title":"OneHot MSB elimination","text":"<p>A <code>OneHot</code> instruction returns a bitmask with exactly one bit equal to 1. If the input is not all-0 bits, this is the first 1 bit encountered in the input going from least to most significant bit or vice-versa depending on the priority specified. If the input is all-0 bits, the most significant bit of the <code>OneHot</code> output is set to 1. The semantics of <code>OneHot</code> are described in detail here. If the MSB of a <code>OneHot</code> does not affect the functionality of a program, we replace the MSB with a 0-bit, e.g.</p> <pre><code>OneHot(A) such that the MSB has no effect\n\u21d2 Concat(0, OneHot(A)[all bits except MSB]))\n</code></pre> <p>This can open up opportunities for further optimization. To determine if a <code>OneHot</code>\u2019s MSB has any effect on a function, we iterate over the <code>OneHot</code>\u2019s post-dominators. We use the BDD to test if setting the <code>OneHot</code>\u2019s MSB to 0 or to 1 (other bits are 0 in both cases) implies the same value for the post-dominator node. If so, we know the value of the MSB cannot possibly affect the function output, so the MSB can safely be replaced with a 0 bit. Note: This approach assumes that IR nodes do not have any side effects. When IR nodes with side effects are introduced (i.e. channels) the analysis for this optimization will have to be adjusted slightly to account for this.</p>"},{"location":"tutorials/","title":"XLS Tutorials","text":"<p>The XLS team has written several tutorials to help explain language features and design techniques. Here they are, grouped by topic:</p>"},{"location":"tutorials/#dslx","title":"DSLX","text":"<ul> <li>Hello, XLS! : A brief introduction to writing and evaluating     your first design in DSLX.</li> <li>Float-to-int conversion : A guide to writing \"real\" logic     in DSLX, demonstrated by creating an IEEE-754 binary32, i.e., C <code>float</code> to     <code>int32_t</code> converter.</li> <li>Intro to parametrics : A demonstration on how     functions and types can be parameterized to allow a single implementation to     apply to many different data layouts.</li> <li><code>for</code> expressions : Explains how to understand and write looping     constructs in DSLX.</li> <li><code>enumerate</code> and <code>match</code> expressions : Explains how to use     <code>enumerate()</code> expressions to control loop iteration and how to use the     <code>match</code> pattern-matching expression for selecting between alternatives.</li> <li>Intro to procs (communicating sequential processes) :     Provides a basic introduction to writing stateful and communicating modules,     i.e., procs.</li> </ul>"},{"location":"tutorials/crc32/","title":"Tutorial: <code>for</code> expressions","text":"<p>In this document we explain in detail the implementation of routine to compute a CRC32 checksum on a single 8-bit input. We don't discuss the algorithm here, only the language features necessary to implement the algorithm.</p> <p>Refer to the full implementation while following this document.</p>"},{"location":"tutorials/crc32/#function-prototype","title":"Function Prototype","text":"<p>The signature and first line of this function should look familiar enough now, but the <code>for</code> construct is new: DSLX provides a means of iterating a fixed number of times within a function.</p> <p>A DSLX <code>for</code> loop has the following structure:</p> <ol> <li>The loop signature: this consists of three elements:     1.  An (index, ) tuple. The index holds the current         iteration number, and the accumulator vars are user-specified data         carried into the current iteration.     2.  The type specification for the index/accumulators tuple. Note that the         index type can be controlled by the user (i.e., doesn't have to be u32,         but it should be able to hold all possible loop index values).     3.  An         iterable,         either the <code>range()</code> or <code>enumerate()</code> expressions, either of which         dictates the number of iterations of the loop to complete. <li>The loop body: this has the same general form as a DSLX function.     Particularly noteworthy is that the loop body ends by stating the \"return\"     value. In a <code>for</code> loop, this \"return\" value is either used as the input to     the next iteration of the loop (for non-terminal iterations) or as the     result of the entire expression (for the terminal iteration).</li> <p>For this specific for loop, the index variable and accumulator are <code>i</code> and <code>crc</code>, both of type <code>u32</code>. The iterable range expression specifies that the loop should execute 8 times.</p> <pre><code>  // 8 rounds of updates.\n  for (i, crc): (u32, u32) in range(u32:8) {\n</code></pre> <p>At the end of the loop, the calculated value is being assigned to the accumulator <code>crc</code> - the last expression in the loop body is assigned to the accumulator:</p> <pre><code>    let mask: u32 = -(crc &amp; u32:1);\n    (crc &gt;&gt; u32:1) ^ (polynomial &amp; mask)\n</code></pre> <p>Finally, the accumulator's initial value is being passed to the <code>for</code> expression as a parameter. This can be confusing, especially when compared to other languages, where the init value typically is provided at or near the top of a loop.</p> <pre><code>}(crc)\n</code></pre> <p>Since the <code>for</code> loop is the last expression in the function, it's also the function's return value, but in other contexts, it could be assigned to a variable and used elsewhere. In general, the result of a <code>for</code> expression can be used in the same manner as any other expression's result.</p>"},{"location":"tutorials/float_to_int/","title":"Tutorial: basic logic","text":"<p>This tutorial demonstrates how to use XLS to create a simple combinational module, in this case one that performs floating-point-to-integer conversion.</p> <p>The first task is to define the full semantics of the module. We wish for the module to accept a IEEE-754 floating-point number and to output the integer representing the same. All fractional elements will be discarded. Overflow, NaN, and infinite values will be clamped to the maximum or minimum representable integer value with the sign of the input number.</p> <ul> <li>Tutorial: basic logic<ul> <li>1. Bootstrapping</li> <li>2. Simple logic</li> <li>3. Conclusion</li> </ul> </li> </ul>"},{"location":"tutorials/float_to_int/#1-bootstrapping","title":"1. Bootstrapping","text":"<p>When creating a module, one first needs to define the signature and skeleton of the entry function. If you recall, a IEEE binary32 (the C <code>float</code> type) has 1 sign bit, 8 [biased] exponent bits, and 23 fractional bits. These values can be packed into a tuple, and so, the signature of our function can be defined as:</p> <pre><code>pub fn float_to_int(\n    x: (u1, u8, u23))\n    -&gt; s32 {\n  s32:0xbeef\n}\n</code></pre> <p>DSLX syntax is intended to follow Rust syntax as much as possible, so this may look familiar if you're a Rustacean. In any case, let's walk through this code, line-by-line:</p> <ol> <li>This line declares a public function (<code>pub fn</code>), named <code>float_to_int</code>. Since     this function is \"public\", it can be referenced from other modules (i.e.,     files) if     imported     therein.</li> <li>Function parameter declarations! This function only takes one parameter,     <code>x</code>, whose type follows its name. In this case, it's a <code>tuple</code>: a grouping     of potentially disparate elements into a single quantity. A tuple is     specified by listing a set of types in parentheses, as here. Our tuple has a     1-bit element for the sign, an 8-bit element for the biased     exponent, and a 23-bit element for the fractional part. In what is the     complete opposite of a coincidence, these fields match those of an IEEE     float32 number. If a function takes more than one argument, they'll be     comma-separated.     -   u1, u8, and u23 are all shortcuts for the type uN[1], uN[8], and uN[23].         The uN[X] construct declares an X-bit wide unsigned type. There is also         sN[X], which declares an X-bit wide signed type.     -   Other type shortcuts exist such as bitsX, bool (alias         for uN[1]), and u[1-64] and s[1-64], being aliases for uN[1] through         uN[64] and sN[1] through sN[64].</li> <li>Function return type. This function returns a signed 32-bit type, matching     the intentions of float-to-int conversion (since floats are signed).</li> <li>Finally, the last line: the final statement in a function is its return     value. Here, we're unconditionally returning a signed 32-bit number with     the value <code>48879</code>. This is only temporary to     make the function syntactically valid - we're still learning the basics!     Gimme a second!</li> </ol> <p>As an aside, it's a good idea to keep a bookmark to the DSLX language reference handy. It has the full details on language features and syntax and even we XLS devs frequently reference it.</p> <p>Anyway...the tuple representation of our input is a bit cumbersome, so let's define our floating-point number as a struct instead:</p> <pre><code>pub struct float32 {\n  sign: u1,\n  bexp: u8,\n  fraction: u23,\n}\n\npub fn float_to_int(x: float32) -&gt; s32 {\n  s32:0xbeef\n}\n</code></pre> <p>Finally, let's write a quick test to make sure things work. Add the following code to your file.</p> <pre><code>#[test]\nfn float_to_int_test() {\n  // 0xbeef in float32.\n  let test_input = float32 {\n    sign: u1:0x0,\n    bexp: u8:0x8e,\n    fraction: u23:0x3eef00\n  };\n  assert_eq(s32:0xbeef, float_to_int(test_input))\n}\n</code></pre> <p>Now run the test through the DSLX interpreter:</p> <pre><code>$ ./bazel-bin/xls/dslx/interpreter_main float_to_int.x\n</code></pre> <p>You should see something like the following:</p> <pre><code>[ RUN UNITTEST  ] float_to_int_test\n[            OK ]\n[===============] 1 test(s) ran; 0 failed; 0 skipped.\n</code></pre> <p>If so, then congrats! You've written - and tested - your first DSLX module! Next up: let's make it do something more interesting.</p>"},{"location":"tutorials/float_to_int/#2-simple-logic","title":"2. Simple logic","text":"<p>After getting the trivial module up and running, the next step is to add real logic to the implementation. Recall that a floating-point number's fractional part has an implicit leading <code>1</code>, so a floating-point number is representable as an integer if its exponent is &lt; 30, that is to say, if its value is between [-2^31, 2^31).</p> <p>To get that exponent, we need to unbias it. In its binary representation, a valid floating-point number's exponent is a value from 0 to 254, being an 8-bit value (an exponent of 255 indicates either NaN or an infinity). The range of a floating-point number's exponent is from -128 to 127, though, so we need to subtract 127 from that value to get the actual exponent. Let's write a function to do just that:</p> <pre><code>fn unbias_exponent(exp: u8) -&gt; s9 {\n  exp as s9 - s9:127\n}\n</code></pre> <p>Notice that we need to expand the exponent to add on the sign bit before the subtraction!</p> <p>Note: The repeated <code>s9</code> type specifications are a bit redundant. They're needed because we've not yet fully built out DSLX' type inference capabilities, but this is an area targeted for improvement.</p> <p>Now that we can get the proper exponent, we can code up the rest of the simple in-bound cases. To do that, we need to prepend that leading <code>1</code> and shift the fractional part into its proper location in the final integer. Here's what that looks like when we add that to our original function:</p> <pre><code>pub struct float32 {\n  sign: u1,\n  bexp: u8,\n  fraction: u23,\n}\n\nfn unbias_exponent(exp: u8) -&gt; s9 {\n  exp as s9 - s9:127\n}\n\npub fn float_to_int(x: float32) -&gt; s32 {\n  let exp = unbias_exponent(x.bexp);\n\n  // Add the implicit leading one.\n  // Note that we need to add one bit to the fraction to hold it.\n  let fraction = u33:1 &lt;&lt; 23 | (x.fraction as u33);\n\n  // Shift the result to the right if the exponent is less than 23.\n  let fraction =\n      if (exp as u8) &lt; u8:23 { fraction &gt;&gt; (u8:23 - (exp as u8)) }\n      else { fraction };\n\n  // Shift the result to the left if the exponent is greater than 23.\n  let fraction =\n      if (exp as u8) &gt; u8:23 { fraction &lt;&lt; ((exp as u8) - u8:23) }\n      else { fraction };\n\n  let result = fraction as s32;\n  let result = if x.sign { -result } else { result };\n  result\n}\n</code></pre> <p>If we run this function with our original test case, it still works! Of course, one should run additional test cases to see what happens with other inputs, particularly because this implementation will fail for some important values.</p> <p>Try adding tests on your own to find these cases - and to fix them! If you're stumped, hints (and answers) are hidden below:</p> Missing case 1 What if the input is 0.0? What should the result be?  To fix this, add a specific check for a zero exponent and fractional part.  Missing case 2 Are NaNs or infinite numbers handled correctly?  To fix, add a special check for NaN or infinities at function end. Consider making `is_inf` and `is_nan` functions!"},{"location":"tutorials/float_to_int/#3-conclusion","title":"3. Conclusion","text":"<p>I hope that these examples help you get a better grasp on DSLX and writing modules in them. While our float-to-int function correctly handles <code>float</code>-to-<code>int32_t</code> conversions, what if we wanted to convert <code>double</code> to <code>int64_t</code>? Or even <code>float</code> to <code>int64_t</code>? Even worse, will we have to write separate floating-point operators for every floating-point type we (or our users) wish to support?</p> <p>Fortunately, the answer is no! The next tutorial covers type parameterization, and demonstrates how we can write a single int-to-float routine that covers all our possible conversions. See you there!</p>"},{"location":"tutorials/hello_xls/","title":"Tutorial: Hello, XLS!","text":"<p>So, you're interested in learning more about XLS and DSLX! Super! This tutorial is aimed at the very basics of getting started with XLS: getting your execution environment set up and running the most trivial of DSLX examples: printing the standard \"Hello, world!\" message to the terminal.</p> <p>Yes, even though XLS is a hardware synthesis language, it still needs basic printing and string support, if only for debugging!</p>"},{"location":"tutorials/hello_xls/#1-installation-and-building","title":"1. Installation and building","text":"<p>First things first: if you haven't yet done so, download the XLS sources from Github:</p> <pre><code>git clone https://github.com/google/xls.git xls\n</code></pre> <p>Next, build the project tree. XLS includes several dependencies that can take a while to build, so the first build may take a while; subsequent builds will be much shorter.</p> <p>NOTE: If you don't have Bazel installed, install it: check the Bazel website for instructions. The other prerequisites are a C++20-compliant compiler toolchain and a Python3 interpreter; check with your distribution for installation instructions for both.</p> <p>Start the XLS build by running:</p> <pre><code>bazel build -c opt xls/...\n</code></pre> <p>Then go get a cup of coffee. LLVM and Z3 are big projects, and will take a while to compile (but only the first time). Binary releases are coming soon: they'll avoid the need for long local compiles.</p>"},{"location":"tutorials/hello_xls/#2-create-your-module","title":"2. Create your module","text":"<p>With your toolchain built, let's get to coding! Open up an editor and create a file called <code>hello_xls.x</code> in your XLS checkout root directory. Populate it with the following:</p> <pre><code>fn hello_xls(hello_string: u8[11]) {\n  trace!(hello_string);\n}\n</code></pre> <p>Let's go over this, line-by-line:</p> <ol> <li>This first line declares a fn (<code>fn</code>) named \"<code>hello_xls</code>\". This function     accepts an array of eleven characters (u8) called <code>hello_string</code>, and     returns no value (the return type would be specified after the argument     list's closing parenthesis and before the function-opening curly brace, if     the function returned a value).</li> <li>This second line invokes the built-in <code>trace!</code> directive, passing it the     function's input string, and throws away the result.</li> </ol>"},{"location":"tutorials/hello_xls/#3-say-hello-xls","title":"3. Say hello, XLS!","text":"<p>Let's run (and test) our code!</p> <p>First thing, though, we should make sure our module parses and passes type checking. The fastest way to do that is via the DSLX \"repl\", conveniently called <code>repl</code>. You can run it against the above example with the command:</p> <pre><code>$ ./bazel-bin/xls/tools/repl hello_xls.x\n</code></pre> <p>This tool first examines the specified module for language correctness, and will print an <code>INVALID_ARGUMENT</code> error if it fails to parse or typecheck. In that case, fix the errors and type <code>:reload</code> to try again. <code>repl</code> supports other features (IR, Verilog, and LLVM code examination), but those are outside the scope of this tutorial.</p> <p>Once you have a parsing DSLX file, the best way to \"smoke test\" a module is via the DSLX interpreter. First, though, we need a test case for it to execute. Add the following to the end of your <code>hello_xls.x</code> file:</p> <pre><code>#[test]\nfn hello_test() {\n  hello_xls(\"Hello, XLS!\")\n}\n</code></pre> <p>Again, going line-by-line:</p> <ol> <li>This directive tells the interpreter that the next function is a test     function, meaning that it shouldn't be passed down the synthesis chain and     that it should be executed by the interpreter.</li> <li>This line declares the [test] function <code>hello_test</code>, which takes no args and     returns no value.</li> <li>The only line in this function invokes the <code>hello_xls</code> function and passes     it a chipper greeting.</li> </ol> <p>With both the function and its corresponding test/driver in place, let's fire it up! Open a terminal and execute the following in the XLS checkout root directory:</p> <pre><code>$ ./bazel-bin/xls/dslx/interpreter_main hello_xls.x\n</code></pre> <p>You should see the following output:</p> <pre><code>[ RUN UNITTEST  ] hello_test\ntrace of hello_string @ hello.x:4:17-4:31: [72, 101, 108, 108, 111, 44, 32, 88, 76, 83, 33]\n[            OK ]\n[===============] 1 test(s) ran; 0 failed; 0 skipped.\n</code></pre> <p>Perfect! While this may not be what you initially expected, examine the output elements carefully: they correspond to the ASCII codes of the characters in \"Hello, XLS!\" When designing and debugging hardware, signals are more often numbers than strings, which is why they're represented as numbers here.</p> <p>Congrats! You've written your first piece of hardware in DSLX! It might be more satisfying, though, if your hardware actually did anything. For that, see the next tutorial, float-to-int conversion.</p>"},{"location":"tutorials/intro_to_parametrics/","title":"Tutorial: parametric types and functions","text":"<p>This tutorial demonstrates how types and functions can be parameterized to enable them to work on data of different formats and layouts, e.g., for a function <code>foo</code> to work on both u16 and u32 data types, and anywhere in between.</p> <p>It's recommended that you're familiar with the concepts in the previous tutorial, \"float-to-int conversion\" before following this tutorial.</p>"},{"location":"tutorials/intro_to_parametrics/#simple-parametrics","title":"Simple parametrics","text":"<p>Consider the simple example of the <code>umax</code> function in the DSLX standard library:</p> <pre><code>pub fn umax&lt;N: u32&gt;(x: uN[N], y: uN[N]) -&gt; uN[N] {\n  if x &gt; y { x } else { y }\n}\n</code></pre> <p>Most of this function looks like other DSLX functions you may have seen, except for the new-style parameter, <code>N</code>. The declaration of <code>N</code> inside angle brackets denotes that <code>N</code> is a parametric value whose value is a build-time invariant that will be specified by the caller. In other words, changing regular function parameters pumps different values through the circuit, while changing parametric values changes the circuit itself.</p> <p>Here, <code>N</code> is used to define the widths of the input and output types. It's plain to see, then, that specifying <code>N = 16</code> would calculate the maximum of two 16-bit numbers, whereas <code>N = 273</code> would calculate the maximum of two 273-bit numbers. That being said, the smaller the circuit, the faster, smaller, and lower-power it will be, so <code>N</code> should be as small as possible (but no smaller!).</p> <p>There are two ways invoke a parametric function: the first is to explicitly specify all parametric values, and the second is to rely on the language to infer them:</p> <p>Explicit specification:</p> <pre><code>import std;\n\nfn foo(a: u32, b: u16) -&gt; u64 {\n  std::umax&lt;u32:64&gt;(a as u64, b as u64)\n}\n</code></pre> <p>Here, the user has directly told the language what the values of all parametrics are.</p> <p>Parametric inference:</p> <pre><code>import std;\n\nfn foo(a: u32, b: u16) -&gt; u64 {\n  std::umax(a as u64, b as u64)\n}\n</code></pre> <p>Here, though, the language is able to determine that <code>N</code> is 64, since that matches the types of the arguments to <code>umax</code>, and since both arg types agree. There may be times where inference isn't possible - for example, when there exist parametrics that aren't referenced in the argument list:</p> <pre><code>fn my_parametric_sum&lt;N: u32&gt;(a: u32, b: u32) -&gt; uN[N] {\n  let a_mod = a as uN[N];\n  let b_mod = a as uN[N];\n  a_mod + b_mod\n}\n</code></pre> <p>To invoke this function, explicit specification is required.</p>"},{"location":"tutorials/intro_to_parametrics/#derived-parametrics","title":"Derived parametrics","text":"<p>It's common, when using parametric types, to need types similar, but not identical to the parametric type. Consider calculating the unbiased floating-point exponent (from the previous tutorial): while the biased exponent was 8 bits wide, the calculated unbiased exponent was 9 bits wide due to the additional sign bit. In this situation, if <code>EXP_SZ</code> was 8, then it'd be handy to also have a <code>SIGNED_EXP_SZ</code> symbol that was equal to 9. This can be done as follows:</p> <pre><code>fn unbias_exponent&lt;EXP_SZ: u32, SIGNED_EXP_SZ: u32 = EXP_SZ + u32:1&gt;(\n    exp: uN[EXP_SZ]) -&gt; sN[SIGNED_EXP_SZ] {\n  exp as sN[SIGNED_EXP_SZ] - sN[SIGNED_EXP_SZ]:???\n}\n</code></pre> <p>Oh no! Specifying parametrics in this way has revealed a problem! If we parameterize types, then in some situations, we'll need to also parameterize values!</p> <p>Of course, we'd not be writing this tutorial if that wasn't possible. DSLX supports \"constexpr\"-style evaluation, whereby constant expressions can be evaluated at interpretation or compilation time. In this case, we just need an expression that can calculate the correct bias adjustment: <code>(sN[SIGNED_EXP_SZ]:1 &lt;&lt; (EXP_SZ - u32:1)) - sN[SIGNED_EXP_SZ]:1</code></p> <p>This is a bit unwieldy in practice, so we can wrap it in a function:</p> <pre><code>fn bias_scaler&lt;N: u32, WIDE_N: u32 = {N + u32:1}&gt;() -&gt; sN[WIDE_N] {\n  (sN[WIDE_N]:1 &lt;&lt; (N - u32:1)) - sN[WIDE_N]:1\n}\n\nfn unbias_exponent&lt;EXP_SZ: u32, SIGNED_EXP_SZ: u32 = {EXP_SZ + u32:1}&gt;(\n    exp: uN[EXP_SZ]) -&gt; sN[SIGNED_EXP_SZ] {\n  exp as sN[SIGNED_EXP_SZ] - bias_scaler&lt;EXP_SZ&gt;()\n}\n</code></pre>"},{"location":"tutorials/intro_to_parametrics/#parameterized-float-to-int","title":"Parameterized float-to-int","text":"<p>Finally, consider the 32-bit float-to-int program from the previous tutorial. That program was restricted to converting from one specific type to another. If, however, we wanted to convert from, say a <code>double</code> to an <code>int32_t</code>, we'd have to write a new function, even though the basic logic would be the same.</p> <p>Instead, armed with parametrics, we can write a single function to handle all such conversions - even to floating-point formats we haven't considered!</p> <p>The first step in such a parameterization is to have a working single-typed example, which we take from the previous codelab:</p> <pre><code>pub struct float32 {\n  sign: u1,\n  bexp: u8,\n  fraction: u23,\n}\n\nfn unbias_exponent(exp: u8) -&gt; s9 {\n  exp as s9 - s9:127\n}\n\npub fn float_to_int(x: float32) -&gt; s32 {\n  let exp = unbias_exponent(x.bexp);\n\n  // Add the implicit leading one.\n  // Note that we need to add one bit to the fraction to hold it.\n  let fraction = u33:1 &lt;&lt; 23 | (x.fraction as u33);\n\n  // Shift the result to the right if the exponent is less than 23.\n  let fraction =\n      if (exp as u8) &lt; u8:23 { fraction &gt;&gt; (u8:23 - (exp as u8)) }\n      else { fraction };\n\n  // Shift the result to the left if the exponent is greater than 23.\n  let fraction =\n      if (exp as u8) &gt; u8:23 { fraction &lt;&lt; ((exp as u8) - u8:23) }\n      else { fraction };\n\n  let result = fraction as s32;\n  let result = if x.sign { -result } else { result };\n  result\n}\n</code></pre> <p>Next is to identify all types needing parameterization, here being the intended size of the result and the layout of the floating-point type itself; all other types flow from that base definition:</p> <ul> <li><code>exp:</code>float32::bexp` size + 1 sign bit</li> <li><code>fraction:</code>float32::fraction` size + 1 implicit leading bit</li> </ul> <p>Thus, the struct declaration and function signature will be:</p> <pre><code>pub struct float&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt; {\n  sign: u1,\n  bexp: uN[EXP_SZ],\n  fraction: uN[FRACTION_SZ],\n}\n\npub fn float_to_int&lt;EXP_SZ: u32, FRACTION_SZ: u32, RESULT_SZ: u32&gt;(\n    x: float&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; sN[RESULT_SZ] {\n  ...\n}\n</code></pre> <p>From there, the rest of the function can be populated by replacing the types in the original implementation with the parameterized ones in the signature:</p> <pre><code>pub struct float&lt;EXP_SZ: u32, FRACTION_SZ: u32&gt; {\n  sign: u1,\n  bexp: uN[EXP_SZ],\n  fraction: uN[FRACTION_SZ],\n}\n\nfn bias_scaler&lt;N: u32, WIDE_N: u32 = {N + u32:1}&gt;() -&gt; sN[WIDE_N] {\n  (sN[WIDE_N]:1 &lt;&lt; (N - u32:1)) - sN[WIDE_N]:1\n}\n\nfn unbias_exponent&lt;EXP_SZ: u32, SIGNED_EXP_SZ: u32 = {EXP_SZ + u32:1}&gt;(\n    exp: uN[EXP_SZ]) -&gt; sN[SIGNED_EXP_SZ] {\n  exp as sN[SIGNED_EXP_SZ] - bias_scaler&lt;EXP_SZ&gt;()\n}\n\npub fn float_to_int&lt;\n    EXP_SZ: u32, FRACTION_SZ: u32, RESULT_SZ: u32,\n    WIDE_EXP_SZ: u32 = {EXP_SZ + u32:1},\n    WIDE_FRACTION_SZ: u32 = {FRACTION_SZ + u32:1}&gt;(\n    x: float&lt;EXP_SZ, FRACTION_SZ&gt;) -&gt; sN[RESULT_SZ] {\n  let exp = unbias_exponent(x.bexp);\n\n  let fraction = uN[WIDE_FRACTION_SZ]:1 &lt;&lt; FRACTION_SZ |\n      (x.fraction as uN[WIDE_FRACTION_SZ]);\n\n  let fraction =\n      if (exp as u32) &lt; FRACTION_SZ { fraction &gt;&gt; (FRACTION_SZ - (exp as u32)) }\n      else { fraction };\n\n  let fraction =\n      if (exp as u32) &gt; FRACTION_SZ { fraction &lt;&lt; ((exp as u32) - FRACTION_SZ) }\n      else { fraction };\n\n  let result = fraction as sN[RESULT_SZ];\n  let result = if x.sign { -result } else { result };\n  result\n}\n</code></pre> <p>Note that <code>unbias_exponent()</code> didn't need type specification, since the type could be inferred from the argument! (Also note that this implementation doesn't contain the fixes from the missing cases from the previous tutorial. Exercise to the reader: apply those fixes here, too!)</p> <p>This technique underlies all of XLS' floating-point libraries. Common operations are defined in common files, such as apfloat.x (general utilities). Specializations of the above are then available in, e.g., float32.x to hide internal implementation details from end users.</p> <p>With this technique, you can write single implementations of functionality that can be applicable across all sorts of hardware configurations for minimal additional cost. Try it out! Create an <code>0xbeef</code>-bit wide floating-point adder!</p>"},{"location":"tutorials/intro_to_procs/","title":"DSLX Tutorial: Intro to procs","text":"<p>Up to this point, our tutorials have described stateless, non-communicating, combinational modules. To add state or communication with other actors, we need to venture into the exciting land of procs!</p> <p>Procs, short for \"communicating sequential processes\", are the means by which DSLX models sequential and stateful modules. A proc contains:</p> <ul> <li>A <code>config</code> function that initializes constant proc state and spawns any     other dependent/child procs needed for execution.</li> <li>A recurrent (i.e., infinitely looping) <code>next</code> function that contains the     actual logic to be executed by the proc.</li> </ul> <p>A critical component of procs is communication: every proc needs a means to share data with other procs, or else it'd just be spinning dead code. These means are channels: entities into which data can be sent and from which data can be received. Each channel has a send and a receive endpoint: data inserted into a channel by a <code>send</code> op can be pulled out by a <code>recv</code> op.</p>"},{"location":"tutorials/intro_to_procs/#example-proc","title":"Example proc","text":"<p>Many concepts here are more easily explained via example, so here's a possible DSLX implementation of FMAC (fused multiply-accumulate), which computes <code>C = A * B + C</code>:</p> <pre><code>import float32;\n\ntype F32 = float32::F32;\n\nproc Fmac {\n  input_a_consumer: chan&lt;F32&gt; in;\n  input_b_consumer: chan&lt;F32&gt; in;\n  output_producer: chan&lt;F32&gt; out;\n\n  config(input_a_consumer: chan&lt;F32&gt; in, input_b_consumer: chan&lt;F32&gt; in,\n         output_producer: chan&lt;F32&gt; out) {\n    (input_a_consumer, input_b_consumer, output_producer)\n  }\n\n  next(tok: token, state: F32) {\n    let (tok_a, input_a) = recv(tok, input_a_consumer);\n    let (tok_b, input_b) = recv(tok, input_b_consumer);\n    let result = float32::fma(input_a, input_b, state);\n    let tok = join(tok_a, tok_b);\n    let tok = send(tok, output_producer, result);\n    (result,)\n  }\n}\n</code></pre> <p>(In practice, a FMAC unit would want a reset signal, but we've leaving that out for simplicity.)</p> <p>There's a lot to unpack here, so we'll walk through the example.</p> <p>The first part of interest is the declaration of the proc member values: the three channels. Procs can have \"member\" state, similar to class data members in software. In DSLX, proc members are constant values, and are set by the output of the <code>config</code> function. Member values can be referred to inside the <code>next</code> function in the same way as locally-declared data.</p> <p>Next up is the <code>config</code> function itself. When a proc is \"spawned\", its given two sets of values: one set for the <code>config</code> function and one set to initialize <code>next</code> (following this example, we'll show how procs are spawned). Inside a <code>config</code> function, any constant values can be computed, any necessary procs can be spawned, and finally member values are set by the return value. Member values are assigned in declaration order: the first element of the return tuple corresponds to the first-declared member, and so on.</p> <p>After that, we encounter <code>next</code>. This function serves as the real \"body\" of the Proc. The <code>next</code> function maintains and evolves the proc's recurrent state and is responsible for communicating with the outside world, as well. In our example, the first two lines are that communication, receiving the input values for the computation. The token elements are used to sequence events: since the receives can happen in parallel, they can share the same token, but since sending the output must happen after that, their result tokens are \"joined\" (think joining two threads of execution in software), and the result is used to sequence the send. Between the communication routines is the actual computation.</p> <p>At the end of the proc, we terminate with the <code>result</code> value. This final value becomes the input state for the next iteration. This is how recurrent state is managed by procs: a state value is provided to the <code>next</code> function, and the result of that function is used as the next iteration's state input. Procs can have several state elements: in that case, there will be several input state args, e.g., <code>next(tok: token, state_a: F32, state_b: F32)</code>, and the output will be a tuple of values, corresponding in order to the input state values. Regardless of that, a <code>next</code> function will always take a token as the first parameter.</p>"},{"location":"tutorials/intro_to_procs/#spawning-procs","title":"Spawning procs","text":"<p>In any real design, procs will form a network<sup>1</sup>, where one proc will spawn any number of child procs, which themselves might spawn other procs. (The \"root\" proc will be instantiated by some outside component in the outside RTL environment.) Procs may only be spawned in <code>config</code> functions, as they're part of statically configuring the hardware network to construct.</p> <p>As an example, spawning a couple of our procs above would look as follows:</p> <pre><code>proc Spawner {\n  fmac_1_a_producer = chan&lt;F32&gt; out;\n  fmac_1_b_producer = chan&lt;F32&gt; out;\n  fmac_1_output_consumer = chan&lt;F32&gt; in;\n  fmac_2_a_producer = chan&lt;F32&gt; out;\n  fmac_2_b_producer = chan&lt;F32&gt; out;\n  fmac_2_output_consumer = chan&lt;F32&gt; in;\n\n  config() {\n    let (fmac_1_a_p, fmac_1_a_c) = chan&lt;F32&gt;;\n    let (fmac_1_b_p, fmac_1_b_c) = chan&lt;F32&gt;;\n    let (fmac_1_output_p, fmac_1_output_c) = chan&lt;F32&gt;;\n    spawn fmac(fmac_1_a_c, fmac_1_b_c, fmac_1_output_p)(float32::zero(false));\n\n    let (fmac_2_a_p, fmac_2_a_c) = chan&lt;F32&gt;;\n    let (fmac_2_b_p, fmac_2_b_c) = chan&lt;F32&gt;;\n    let (fmac_2_output_p, fmac_2_output_c) = chan&lt;F32&gt;;\n    spawn fmac(fmac_2_a_c, fmac_2_b_c, fmac_2_output_p)(float32::zero(false));\n\n    (fmac_1_a_p, fmac_1_b_p, fmac_1_output_c,\n     fmac_2_a_p, fmac_2_b_p, fmac_2_output_c)\n  }\n}\n</code></pre> <p>For each child proc, we first declare the necessary channels (each channel declaration produces a producer and consumer channel, respectively), then we actually spawn it. The first set of arguments is passed to the child's config function and the second is the initial state for the child proc. A spawn produces no value, hence no <code>let</code> on the left-hand side.</p>"},{"location":"tutorials/intro_to_procs/#advanced-features","title":"Advanced features","text":""},{"location":"tutorials/intro_to_procs/#channel-arrays-and-loop-based-spawning","title":"Channel arrays and loop-based spawning","text":"<p>Note: This feature is currently WIP and is not yet available.</p> <p>Many hardware layouts have regular arrays of components, such as systolic arrays, vector units, etc. Individually specifying these quickly grows cumbersome, so users can instead declare channels of arrays and spawn procs inside <code>for</code> loops. This looks as follows:</p> <pre><code>proc Spawner4x4 {\n  input_producers: chan&lt;F32&gt; out[4][4];\n  output_consumers: chan&lt;F32&gt; out[4][4];\n\n  config() {\n    let (input_producers, input_consumers) = chan[4][4] F32;\n    let (output_producers, output_consumers) = chan[4][4] F32;\n\n    for (i) : (u32) in range(0, 4) {\n      for (j) : (u32) in range(0, 4) {\n        spawn Node(input_consumers[i][j],\n                   output_producers[i][j])(float32::zero(false))\n      }()\n    }()\n\n    (input_producers, output_consumers)\n  }\n}\n</code></pre>"},{"location":"tutorials/intro_to_procs/#parametric-procs","title":"Parametric procs","text":"<p>Just as with other DSLX constructs, Procs can be parameterized. Parametrics must be specified at the proc level, and not at the component function level (i.e., not on <code>config</code> or <code>next</code>). Building off of the previous example, this looks as follows:</p> <pre><code>proc Parametric&lt;N: u32, M: u32&gt; {\n  input_producers: chan&lt;F32&gt; out[N][M];\n  output_consumers: chan&lt;F32&gt; out[N][M];\n\n  config() {\n    let (input_producers, input_consumers) = chan[N][M] F32;\n    let (output_producers, output_consumers) = chan[N][M] F32;\n\n    for (i) : (u32) in range(0, N) {\n      for (j) : (u32) in range(0, M) {\n        spawn Node(input_consumers[i][j],\n                   output_producers[i][j])(float32::zero(false))\n      }()\n    }()\n\n    (input_producers, output_consumers)\n  }\n}\n</code></pre> <p>These two features are very powerful together: users can specify a broad variety of designs simply by adjusting a few parametric values.</p>"},{"location":"tutorials/intro_to_procs/#proc-testing","title":"Proc testing","text":"<p>The DSLX interpreter supports testing procs via the test_proc construct. A test proc is very similar to a normal proc with the following changes:</p> <ul> <li>A test proc is preceded by the <code>#[test_proc()]</code> directive. This directive,     as one might expect, notifies the interpreter that the following proc is a     test proc. Any initial state needed by the test proc should go inside the     parentheses.</li> <li>A test proc's <code>config</code> function must accept a single argument: a boolean     input channel for terminating interpretation. When the test is complete, the     proc should send the test's status (<code>true</code> on success, <code>false</code> on failure)     on that channel (commonly called the \"terminator\" channel).</li> </ul> <p>A skeletal example:</p> <pre><code>#[test_proc(u32:0)]\nproc Tester {\n  terminator: chan&lt;bool&gt; out;\n\n  config(terminator: chan&lt;bool&gt; out) {\n    spawn proc_under_test(...)(...);\n    (terminator,)\n  }\n\n  next(tok: token, state: u32) {\n    new_state = ...\n    (new_state,)\n  }\n}\n</code></pre> <p>The FP32 fmac module has a more complete proc test that may be used for reference.</p>"},{"location":"tutorials/intro_to_procs/#scheduling-constraints","title":"Scheduling constraints","text":"<p>If you want to interface with something in the outside world that is latency sensitive (for example, an SRAM -- though we have separate infrastructure for making SRAMs work that builds on top of this feature), you can create external channels representing the interface you want to use, e.g.:</p> <pre><code>proc main {\n  req: chan&lt;u32&gt; out;\n  resp: chan&lt;u32&gt; in;\n\n  init { u32: 0 }\n\n  config(req: chan&lt;u32&gt; out, resp: chan&lt;u32&gt; in) {\n    (req, resp)\n  }\n\n  next(tok: token, state: u32) {\n    let request = state * state;\n    let tok = send(tok, req, request);\n    let (tok, response) = recv(tok, resp);\n    state + u32:1\n  }\n}\n</code></pre> <p>where the fact that <code>req</code> and <code>resp</code> are parameters of <code>config</code>, and <code>main</code> is the top proc during IR conversion, is what makes them \"external\".</p> <p>Then when you codegen this, you can pass in <code>--io_constraints=foo__req:send:foo__resp:recv:2:2</code> where <code>foo__req</code> is the mangled name of the channel, which you can see by examining the generated IR prior to codegen. That constraint means \"a send on any channel named <code>req</code> must occur exactly two cycles before a receive on any channel named <code>resp</code>\"; the <code>2</code> is specified twice because it is possible to give a range of allowed cycle differences.</p> <p>For more details on <code>--io_constraints</code>, check out the docs. For a complete example, see <code>//xls/examples:constraint_sv</code> and associated build targets; the target you'd build to get the mangled channel names is <code>:constraint_ir</code>.</p> <ol> <li> <p>The proc network itself will form a tree, but channels may make point-to-point connections between any two procs.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/prefix_scan/","title":"DSLX Tutorial: <code>enumerate</code> and <code>match</code> expressions","text":"<p>In this document we explain in detail the implementation of a 8 byte prefix scan computation. In order to understand the implementation, it is useful to understand the intended functionality first.</p> <p>For a given input of 8 bytes, the scan iterates from left to right over the input and produces an output of the same size. Each element in the output contains the count of duplicate values seen so far in the input. The counter resets to 0 if a new value is found.</p> <p>For example, for this input:</p> <pre><code>  let input = u32[8]:[0, ...]\n</code></pre> <p>the code should produce this output:</p> <pre><code>  u3[8]:[0, 1, 2, 3, 4, 5, 6, 7])\n</code></pre> <p>At index 0 it has not yet found any value, so it assigns a counter value of <code>0</code>.</p> <p>At index 1 it finds the second occurrence of the value '0' (which is the 1st duplicate) and therefore adds a 1 to the counter from index 0.</p> <p>At index 2 it finds the third occurrence of the value '0' (which is the 2nd duplicate) and therefore adds a 1 to the counter from index 1. And so on.</p> <p>Correspondingly, for this input:</p> <pre><code>  let input = u32[8]:[0, 0, 1, 1, 2, 2, 3, 3]\n</code></pre> <p>it should produce:</p> <pre><code>  assert_eq(result, u3[8]:[0, 1, 0, 1, 0, 1, 0, 1])\n</code></pre> <p>The full listing is in <code>examples/dslx_intro/prefix_scan_equality.x</code>.</p>"},{"location":"tutorials/prefix_scan/#function-prefix_scan_eq","title":"Function <code>prefix_scan_eq</code>","text":"<p>The implementation displays a few interesting language features.</p> <p>The function prototype is straight-forward. Input is an array of 8 values of type <code>u32</code>. Output is an array of size 8 holding 3-bit values (the maximum resulting count can only be 7, which fits in 3 bits).</p> <pre><code>fn prefix_scan_eq(x: u32[8]) -&gt; u3[8] {\n</code></pre> <p>The first let expression produces a tuple of 3 values. It only cares about the last value <code>result</code>, so it stubs out the other two elements via the 'ignore' placeholder <code>_</code>.</p> <pre><code>  let (_, _, result) =\n</code></pre> <p>Why a 3-Tuple? Because he following loop has tuple of three values as the accumulator. The return type of the loop is the type of the accumulator, so above let needs to be of the same type.</p>"},{"location":"tutorials/prefix_scan/#enumerated-loop","title":"Enumerated Loop","text":"<p>Using tuples as the accumulator is a convenient way to model multiple loop-carried values:</p> <pre><code>    for ((i, elem), (prior, count, result)): ((u32, u32), (u32, u3, u3[8]))\n          in enumerate(x) {\n</code></pre> <p>The iterable of this loop is <code>enumerate(x)</code>. On each iteration, this construct delivers a tuple consisting of current index and current element. This is represented as the tuple <code>(i, elem)</code> in the <code>for</code> construct.</p> <p>The loop next specifies the accumulator, which is a 3-tuple consisting of the values named <code>prior</code>, <code>count</code>, and <code>result</code>.</p> <p>The types of the iterable and accumulator are specified next. The iterable is a tuple consisting of two <code>u32</code> values. The accumulator is more interesting, it is a tuple consisting of a <code>u32</code> value (<code>prior</code>), a <code>u3</code> value (<code>count</code>), and an array type <code>u3[8]</code>, which is an array holding 8 elements of bit-width 3. This is the type of <code>result</code> in the accumulator.</p> <p>Looping back to the prior <code>let</code> statement, it ignores the <code>prior</code> and <code>count</code> members of the tuple and will only return the <code>result</code> part.</p>"},{"location":"tutorials/prefix_scan/#a-match-expression","title":"A Match Expression","text":"<p>The next expression is an interesting <code>match</code> expression. The let expression binds the tuple <code>(to_place, new_count): (u3, u3)</code> to the result of the following match expression:</p> <pre><code>let (to_place, new_count): (u3, u3) = match (i == u32:0, prior == elem) {\n</code></pre> <p><code>to_place</code> will hold the value that is to be written at a given index. <code>new_count</code> will contain the updated counter value.</p> <p>The <code>match</code> expression evaluates two conditions in parallel:</p> <ul> <li>is <code>i</code> == 0?</li> <li>is the <code>prior</code> element the same as the current <code>elem</code></li> </ul> <p>Two tests mean there are four possible cases, which are all handled in the following four cases:</p> <pre><code>      // i == 0 (no matter whether prior == elem or not):\n      //    we set position 0 to 0 and update the new_counter to 1\n      (true, true) =&gt; (u3:0, u3:1),\n      (true, false) =&gt; (u3:0, u3:1),\n\n      // if i != 0 - if the current element is the same as pior,\n      //    set to_place to the value of the current count\n      //    update new_counter with the increased counter value\n      (false, true) =&gt; (count, count + u3:1),\n\n      // if i != 0 - if current element is different from prior,\n      //     set to_place back to 0\n      //     set new_counter back to 1\n      (false, false) =&gt; (u3:0, u3:1),\n    };\n</code></pre> <p>To update the result, we set index <code>i</code> in the <code>result</code> array to the value <code>to_place</code> via the built-in <code>update</code> function, which produces a new value <code>new_result</code>):</p> <pre><code>    let new_result: u3[8] = update(result, i, to_place);\n</code></pre> <p>Finally the updated accumulator value is constructed, it is the last expression in the loop:</p> <pre><code>    (elem, new_count, new_result)\n</code></pre> <p>Following the loop body, as an argument to the loop, we initialize the accumulator in the following way.</p> <ul> <li>set element <code>prior</code> to -1, in order to not match any other value.</li> <li>set element <code>count</code> to 0.</li> <li>set element <code>result</code> to 8 0's of size <code>u3</code>. (Note that the <code>...</code> syntax     is short for \"fill in the rest of the elements with the last value     specified\".)</li> </ul> <pre><code>}((u32:-1, u3:0, u3[8]:[0, ...]));\n</code></pre> <p>And, finally, the function simply returns <code>result</code>:</p> <pre><code>  result\n}\n</code></pre>"},{"location":"tutorials/prefix_scan/#testing","title":"Testing","text":"<p>To test the two cases we've described above, we add the following two test cases right to this implementation file:</p> <pre><code>#[test]\nfn test_prefix_scan_eq_all_zero() {\n  let input = u32[8]:[0, ...];\n  let result = prefix_scan_eq(input);\n  assert_eq(result, u3[8]:[0, 1, 2, 3, 4, 5, 6, 7])\n}\n\n#[test]\nfn test_prefix_scan_eq_doubles() {\n  let input = u32[8]:[0, 0, 1, 1, 2, 2, 3, 3];\n  let result = prefix_scan_eq(input);\n  assert_eq(result, u3[8]:[0, 1, 0, 1, 0, 1, 0, 1])\n}\n</code></pre> <p>To run tests against the file present in the repository:</p> <pre><code>xls$ bazel run -c opt //xls/dslx:interpreter_main -- \\\n       $PWD/xls/examples/dslx_intro/prefix_scan_equality.x \\\n       --compare=none\n[ RUN UNITTEST  ] prefix_scan_eq_all_zero_test\n[            OK ]\n[ RUN UNITTEST  ] prefix_scan_eq_doubles_test\n[            OK ]\n[===============] 2 test(s) ran; 0 failed; 0 skipped.\n</code></pre> <p>(Note that <code>--compare=none</code> is currently required because <code>enumerate</code> ranges are not currently convertable to IR, otherwise running the DSLX interpreter would do implicit comparison to IR interpreter -- see google/xls#164.)</p>"},{"location":"tutorials/xlscc_channels/","title":"Tutorial: XLS[cc] channels.","text":"<ul> <li>Tutorial: XLS[cc] channels.<ul> <li>Introduction to channels.</li> <li>Translate into optimized XLS IR.</li> <li>Note the metadata output</li> <li>Perform code-generation into a pipelined Verilog block.</li> <li>Additional XLS[cc] examples.</li> </ul> </li> </ul> <p>This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a sequential C++ block containing channels.</p>"},{"location":"tutorials/xlscc_channels/#introduction-to-channels","title":"Introduction to channels.","text":"<p>XLS implements channels via a FIFO-based (ready/valid/data) interface.</p> <p>In C++, these channels are provided by a built-in template class called <code>__xls_channel</code> supporting the two methods: <code>read()</code> and <code>write(val)</code>.</p> <p>It can be aliased to the desired name like this:</p> <pre><code>template&lt;typename T&gt;\nusing OutputChannel = __xls_channel&lt;T, __xls_channel_dir_Out&gt;;\n\ntemplate&lt;typename T&gt;\nusing InputChannel = __xls_channel&lt;T, __xls_channel_dir_In&gt;;\n</code></pre> <p>An example of a usage is below, which reads an integer on the input channel, multiplies it by 3, and writes it to the output channel.</p> <pre><code>class TestBlock {\npublic:\n    InputChannel&lt;int&gt; in;\n    OutputChannel&lt;int&gt; out;\n\n    #pragma hls_top\n    void Run() {\n        auto x = in.read();\n        out.write(3*x);\n    }\n};\n</code></pre>"},{"location":"tutorials/xlscc_channels/#translate-into-optimized-xls-ir","title":"Translate into optimized XLS IR.","text":"<p>With the above setup complete, XLS IR can now be generated using a sequence of <code>xlscc</code> and <code>opt_main</code>.</p> <pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test_channels.cc \\\n  --block_from_class TestBlock --block_pb meta.pb &gt; test_channels.ir\n$ ./bazel-bin/xls/tools/opt_main test_channels.ir &gt; test_channels.opt.ir\n</code></pre> <p>Below is a quick summary of the options. 1. <code>--block_from_class TestBlock</code> - tells XLS[cc] which class is the top block. 1. <code>--block_pb</code> - tells XLS[cc] where to write the block's metadata description. This must be specified with <code>--block_from_class</code>.</p> <p>Note that unlike in the prior tutorial, XLS[cc] is used to generate XLS procs rather than functions. This is to support the additional interface requirements of channels.</p>"},{"location":"tutorials/xlscc_channels/#note-the-metadata-output","title":"Note the metadata output","text":"<p>The file <code>meta.pb</code> now contains a description of the block which can be useful for integration. In this example, the result is:</p> <pre><code>channels {\n  name: \"in\"\n  is_input: true\n  type: FIFO\n  width_in_bits: 32\n}\nchannels {\n  name: \"out\"\n  is_input: false\n  type: FIFO\n  width_in_bits: 32\n}\nname: \"TestBlock\"\n</code></pre>"},{"location":"tutorials/xlscc_channels/#perform-code-generation-into-a-pipelined-verilog-block","title":"Perform code-generation into a pipelined Verilog block.","text":"<p>With the same IR, you can either generate a combinational block or a clocked pipelined block with the <code>codegen_main</code> tool. In this section, we'll demonstrate how to generate a pipelined block using the above C++ code.</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main test_channels.opt.ir \\\n  --generator=pipeline \\\n  --delay_model=\"sky130\" \\\n  --output_verilog_path=test_channels.v \\\n  --module_name=xls_test \\\n  --top=TestBlock_proc \\\n  --reset=rst \\\n  --reset_active_low=false \\\n  --reset_asynchronous=false \\\n  --reset_data_path=true \\\n  --pipeline_stages=5 \\\n  --flop_inputs=true \\\n  --flop_outputs=true \\\n  --flop_inputs_kind=skid \\\n  --flop_outputs_kind=skid\n</code></pre> <p>Below is a quick summary of the options.</p> <ol> <li><code>--delay_model=\"sky130\"</code> - use the sky130 delay model.</li> <li><code>--top=TestBlock_proc</code> - the proc that is the top-level is named     <code>TestBlock_proc</code>. This should be the block class name given to XLS[cc] with     a <code>_proc</code> suffix appended.</li> <li><code>--flop_inputs_kind=skid</code> and <code>--flop_outputs_kind=skid</code> - control what type     of I/O buffering is used. In this case, we configure a skid buffer at both     the input and output.</li> </ol>"},{"location":"tutorials/xlscc_channels/#additional-xlscc-examples","title":"Additional XLS[cc] examples.","text":"<p>For developers, it is possible to check if a specific feature is supported by checking translator_io_test.cc translator_proc_test.cc for unit tests.</p>"},{"location":"tutorials/xlscc_integers/","title":"Tutorial: XLS[cc] arbitrary width integers.","text":"<ul> <li>Tutorial: XLS[cc] arbitrary width integers.<ul> <li>C++ Source</li> <li>Introduction to fixed-width integers.</li> <li>Configuring XLS[cc] for fixed-width integers.</li> <li>Translate into optimized XLS IR.</li> <li>Additional XLS[cc] examples.</li> </ul> </li> </ul> <p>This tutorial is aimed at walking you through usage of arbitrary width integers using the XlsInt class.</p> <p>Note that an <code>ac_datatypes</code> compatibility layer is also provided so that the same C++ code can be used for both simulation and XLS[cc] synthesis with just a change in the include paths: ac_compat.</p>"},{"location":"tutorials/xlscc_integers/#c-source","title":"C++ Source","text":""},{"location":"tutorials/xlscc_integers/#introduction-to-fixed-width-integers","title":"Introduction to fixed-width integers.","text":"<p>XLS also provides a template class for fixed-width integers. These are declared using the template class <code>XlsInt&lt;int Width, bool Signed = true&gt;</code>.</p> <p>To utilize fixed with integer types, include xls_int.h.</p> <p>Create a <code>test.cc</code> with the following contents for the rest of this tutorial.</p> <pre><code>#include \"xls_int.h\"\n\n#pragma hls_top\nXlsInt&lt;55, true&gt; foo(XlsInt&lt;17, false&gt; x, XlsInt&lt;5, false&gt; y) {\n    return x+y;\n}\n</code></pre>"},{"location":"tutorials/xlscc_integers/#configuring-xlscc-for-fixed-width-integers","title":"Configuring XLS[cc] for fixed-width integers.","text":"<p>XLS[cc] fixed-width integers have a dependency on the <code>ac_datatypes</code> library. Clone the repository (https://github.com/hlslibs) into a directory named <code>ac_datatypes</code>.</p> <pre><code>git clone https://github.com/hlslibs/ac_types.git ac_datatypes\n</code></pre> <p>Then create the a <code>clang.args</code> file with the following contents to configure the include paths and pre-define the <code>__SYNTHESIS__</code> name as a macro.</p> <pre><code>-D__SYNTHESIS__\n-I/path/to/your/xls/contrib/xlscc/synth_only\n-I/path/containing/ac_datatypes/..\n</code></pre>"},{"location":"tutorials/xlscc_integers/#translate-into-optimized-xls-ir","title":"Translate into optimized XLS IR.","text":"<p>Now that the C++ function has been created, <code>xlscc</code> can be used to translate the C++ into XLS IR. <code>opt_main</code> is used afterwards to optimize and transform the IR into a form more easily synthesized into verilog.</p> <pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test.cc --clang_args_file clang.args &gt; test.ir\n$ ./bazel-bin/xls/tools/opt_main test.ir &gt; test.opt.ir\n</code></pre> <p>The resulting <code>test.opt.ir</code> file should look something like the following</p> <pre><code>package my_package\n\nfile_number 1 \"/usr/local/google/home/seanhaskell/tmp/tutorial_int.cc\"\nfile_number 2 \"/usr/local/google/home/seanhaskell/xls/xls/contrib/xlscc/synth_only/xls_int.h\"\n\ntop fn foo(x: bits[17], y: bits[5]) -&gt; bits[55] {\n  literal.132: bits[1] = literal(value=0, id=132, pos=[(1,6,2)])\n  literal.133: bits[13] = literal(value=0, id=133, pos=[(1,6,2)])\n  xid4: bits[18] = concat(literal.132, x, id=134, pos=[(1,6,2)])\n  yid6__1: bits[18] = concat(literal.133, y, id=135, pos=[(1,6,2)])\n  literal.141: bits[37] = literal(value=0, id=141, pos=[(1,6,2)])\n  xid4id8: bits[18] = add(xid4, yid6__1, id=140, pos=[(1,6,2)])\n  ret xid4id8id2: bits[55] = concat(literal.141, xid4id8, id=143, pos=[(1,6,2)])\n}\n</code></pre>"},{"location":"tutorials/xlscc_integers/#additional-xlscc-examples","title":"Additional XLS[cc] examples.","text":"<p>For developers, it is possible to check if a specific feature is supported by checking xls_int_test.cc for unit tests.</p>"},{"location":"tutorials/xlscc_memory/","title":"Tutorial: XLS[cc] memories.","text":"<ul> <li>Tutorial: XLS[cc] memories.<ul> <li>C++ Source</li> <li>Translate to IR</li> <li>Examine the metadata</li> <li>Optimize the IR with RAM rewrites</li> <li>Generate Verilog with IO constraints</li> <li>Additional XLS[cc] examples.</li> </ul> </li> </ul> <p>This tutorial is aimed at walking you through usage of memories. Memories in this context refer to externally implemented SRAMs. The example given will be usage of a dual port \"1 read 1 write\" memory.</p> <p>The channels tutorial should be followed as a prerequisite.</p>"},{"location":"tutorials/xlscc_memory/#c-source","title":"C++ Source","text":"<p>Create a source file <code>test_memory.cc</code></p> <pre><code>template&lt;typename T&gt;\nusing InputChannel = __xls_channel&lt;T, __xls_channel_dir_In&gt;;\ntemplate&lt;typename T&gt;\nusing OutputChannel = __xls_channel&lt;T, __xls_channel_dir_Out&gt;;\ntemplate&lt;typename T, int Size&gt;\nusing Memory = __xls_memory&lt;T, Size&gt;;\n\nclass TestBlock {\npublic:\n    InputChannel&lt;int&gt; in;\n    OutputChannel&lt;int&gt; out;\n    Memory&lt;short, 32&gt; store;\n\n    int addr = 0;\n\n    #pragma hls_top\n    void Run() {\n        const int next_addr = (addr + 1) &amp; 0b11111;\n        store[addr] = in.read();\n        out.write(store[next_addr]);\n        addr = next_addr;\n    }\n};\n</code></pre>"},{"location":"tutorials/xlscc_memory/#translate-to-ir","title":"Translate to IR","text":"<pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test_memory.cc \\\n  --block_from_class TestBlock --block_pb block.pb \\\n  &gt; test_memory.ir\n</code></pre>"},{"location":"tutorials/xlscc_memory/#examine-the-metadata","title":"Examine the metadata","text":"<p>The memory should appear in the resulting block <code>block.pb</code>:</p> <pre><code>channels {\n  name: \"in\"\n  is_input: true\n  type: FIFO\n  width_in_bits: 32\n}\nchannels {\n  name: \"out\"\n  is_input: false\n  type: FIFO\n  width_in_bits: 32\n}\nchannels {\n  name: \"store\"\n  type: MEMORY\n  width_in_bits: 16\n  depth: 32\n}\nname: \"TestBlock\"\n</code></pre> <p>This block description information can be used to generate the RAM rewrites and IO constraints specified in later steps.</p>"},{"location":"tutorials/xlscc_memory/#optimize-the-ir-with-ram-rewrites","title":"Optimize the IR with RAM rewrites","text":"<p>Create <code>rewrites.textproto</code> with the following contents:</p> <pre><code>rewrites {\n  from_config {\n    kind: RAM_ABSTRACT\n    depth: 32\n  }\n  to_config {\n    kind: RAM_1R1W\n    depth: 32\n  }\n  from_channels_logical_to_physical: {\n    key: \"abstract_read_req\"\n    value: \"store__read_request\"\n  }\n  from_channels_logical_to_physical: {\n    key: \"abstract_read_resp\"\n    value: \"store__read_response\"\n  }\n  from_channels_logical_to_physical: {\n    key: \"abstract_write_req\"\n    value: \"store__write_request\"\n  }\n  from_channels_logical_to_physical: {\n    key: \"write_completion\"\n    value: \"store__write_response\"\n  }\n  to_name_prefix: \"store_\"\n}\n</code></pre> <p>Then run <code>opt</code> with awareness of the RAM:</p> <pre><code>$ ./bazel-bin/xls/tools/opt_main test_memory.ir --ram_rewrites_pb rewrites.textproto &gt; test_memory.opt.ir\n</code></pre>"},{"location":"tutorials/xlscc_memory/#generate-verilog-with-io-constraints","title":"Generate Verilog with IO constraints","text":"<p>For this memory, we are assuming a fixed 1-cycle latency for reads and writes. As such, we need to add these constraints to the <code>codegen</code> command, in addition to making it aware of the RAM rewrites, so that the correct ports are generated.</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main test_memory.opt.ir \\\n  --generator=pipeline \\\n  --delay_model=\"sky130\" \\\n  --output_verilog_path=memory_test.v \\\n  --module_name=memory_test \\\n  --top=TestBlock_proc \\\n  --reset=rst \\\n  --reset_active_low=false \\\n  --reset_asynchronous=false \\\n  --reset_data_path=true \\\n  --pipeline_stages=2  \\\n  --flop_inputs=false \\\n  --flop_outputs=false \\\n  --io_constraints=store__read_req:send:store__read_resp:recv:1:1,store__write_req:send:store__write_completion:recv:1:1 \\\n  --ram_configurations=ram:1R1W:store__read_req:store__read_resp:store__write_req:store__write_completion\n</code></pre> <p>Below is a quick summary of the options.</p> <ol> <li><code>--io_constraints=store__read_req:send:store__read_resp:recv:1:1...</code> This     option ensures that requests to the memory are written exactly 1 cycle     before responses are read.</li> <li><code>--ram_configurations=ram:1R1W:store__read_req...</code> This option informs     codegen of the necessary information about the memory to generate the top     level ports in the correct style. \"ram\" is the name prefix the ports will     use.</li> </ol>"},{"location":"tutorials/xlscc_memory/#additional-xlscc-examples","title":"Additional XLS[cc] examples.","text":"<p>For developers, it is possible to check if a specific feature is supported by checking translator_memory_test.cc for unit tests.</p>"},{"location":"tutorials/xlscc_overview/","title":"Tutorial: XLS[cc] Overview","text":"<ul> <li>Tutorial: XLS[cc] Overview<ul> <li>Create your first C++ module.</li> <li>Translate into optimized XLS IR.</li> <li>Perform code-generation into a combinational Verilog block.</li> <li>Create your second C++ module and generate an optimized IR file.</li> <li>Perform code-generation into a pipelined Verilog block.</li> <li>Additional XLS[cc] examples.</li> </ul> </li> </ul> <p>This tutorial is aimed at walking you through getting a function written in C++ and then compiling into a working Verilog module.</p> <p>This assumes that you've already been successful in building XLS. See Installing and building if not.</p>"},{"location":"tutorials/xlscc_overview/#create-your-first-c-module","title":"Create your first C++ module.","text":"<p>XLS[cc] takes as input a single translation unit -- one <code>.cc</code> file. Other files may be included in that one file, but only the top-level file should be provided.</p> <p>Create a file called <code>test.cc</code> with the following contents.</p> <pre><code>#pragma hls_top\nint add3(int input) { return input + 3; }\n</code></pre> <p>Note that <code>#pragma hls_top</code> denotes the top-level function for the module. The xls func or proc created will follow that function's interface.</p>"},{"location":"tutorials/xlscc_overview/#translate-into-optimized-xls-ir","title":"Translate into optimized XLS IR.","text":"<p>Now that the C++ function has been created, <code>xlscc</code> can be used to translate the C++ into XLS IR. <code>opt_main</code> is used afterwards to optimize and transform the IR into a form more easily synthesized into verilog.</p> <pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test.cc &gt; test.ir\n$ ./bazel-bin/xls/tools/opt_main test.ir &gt; test.opt.ir\n</code></pre> <p>The resulting <code>test.opt.ir</code> file should look something like the following</p> <pre><code>package my_package\n\nfile_number 1 \"./test.cc\"\n\ntop fn add3(input: bits[32]) -&gt; bits[32] {\n  literal.2: bits[32] = literal(value=3, id=2, pos=[(1,2,23)])\n  ret add.3: bits[32] = add(input, literal.2, id=3, pos=[(1,2,23)])\n}\n</code></pre>"},{"location":"tutorials/xlscc_overview/#perform-code-generation-into-a-combinational-verilog-block","title":"Perform code-generation into a combinational Verilog block.","text":"<p>With the same IR, you can either generate a combinational block or a clocked pipelined block with the <code>codegen_main</code> tool. In this section, we'll demonstrate how to generate a combinational block.</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main test.opt.ir \\\n  --generator=combinational \\\n  --delay_model=\"unit\" \\\n  --output_verilog_path=test.v \\\n  --module_name=xls_test \\\n  --top=add3\n</code></pre> <p>Below is a quick summary of each option:</p> <ol> <li><code>--generator=combinational</code> states that <code>codegen_main</code> should generate a     combinational module.</li> <li><code>--delay_model=\"unit\"</code> states to use the unit delay model. Additional delay     models include asap7 and sky130.</li> <li><code>--output_verilog_path=test.v</code> is where the output verilog should be written     to.</li> <li><code>--module_name=xls_test</code> states that the generated verilog module should     have the name of <code>xls_test</code>.</li> <li><code>--top=add3</code> states that the function that should be used for codegen is the     function (<code>fn</code>) named <code>add3</code>.</li> </ol> <p>The resulting <code>test.v</code> should have contents similar to the following</p> <pre><code>module xls_test(\n  input wire [31:0] input,\n  output wire [31:0] out\n);\n  wire [31:0] add_6;\n  assign add_6 = input + 32'h0000_0003;\n  assign out = add_6;\nendmodule\n</code></pre>"},{"location":"tutorials/xlscc_overview/#create-your-second-c-module-and-generate-an-optimized-ir-file","title":"Create your second C++ module and generate an optimized IR file.","text":"<p>XLS[cc] supports two ways of handling looping C++ constructs -- it can unroll the loop, or convert the loop into sequential logic. In this section, we'll demonstrate loop unrolling.</p> <p>Unrolled loops are annotated with <code>#pragma hls_unroll yes</code>. For example, create a file called <code>test_unroll.cc</code> with the following contents.</p> <pre><code>#pragma hls_top\nint test_unroll(int x) {\n  int ret = 0;\n  #pragma hls_unroll yes\n  for(int i=0;i&lt;32;++i) {\n    ret += x * i;\n  }\n  return ret;\n}\n</code></pre> <p>Then compile, and optimize the resulting IR</p> <pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test_unroll.cc &gt; test_unroll.ir\n$ ./bazel-bin/xls/tools/opt_main test_unroll.ir &gt; test_unroll.opt.ir\n</code></pre>"},{"location":"tutorials/xlscc_overview/#perform-code-generation-into-a-pipelined-verilog-block","title":"Perform code-generation into a pipelined Verilog block.","text":"<p>The previous section should have left you with an IR file called <code>test_unroll.opt.ir</code> with a function with the signature <code>fn test_unroll(x: bits[32]) -&gt; bits[32]</code>. The function is likely too large to fit into a single clock cycle so we'll create a pipelined module.</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main test_unroll.opt.ir \\\n  --generator=pipeline \\\n  --delay_model=\"asap7\" \\\n  --output_verilog_path=test_unroll.v \\\n  --module_name=xls_test_unroll \\\n  --entry=test_unroll \\\n  --reset=rst \\\n  --reset_active_low=false \\\n  --reset_asynchronous=false \\\n  --pipeline_stages=5 \\\n  --flop_inputs=true \\\n  --flop_outputs=true\n</code></pre> <p>Below is a quick summary of each option:</p> <ol> <li><code>--generator=pipeline</code> - <code>codegen_main</code> should generate a pipelined module.</li> <li><code>--delay_model=\"asap7\"</code> - use the asap7 delay model.</li> <li><code>--output_verilog_path=test_unroll.v</code> - where the output verilog should be     written to.</li> <li><code>--module_name=xls_test_unroll</code> - the generated verilog module should have     the name of <code>xls_unroll_test</code>.</li> <li><code>--entry=test_unroll</code> - the function that should be used for codegen is the     function (<code>fn</code>) named <code>test_unroll</code>.</li> <li><code>--reset=rst</code> - there should be a reset signal named <code>rst</code>.</li> <li><code>--reset_active_low=false</code> - a high reset signal means reset the module.</li> <li><code>--reset_asynchronous=false</code> - rst is a synchronous reset signal.</li> <li><code>--pipeline_stages=5</code> - create a 5 stage pipeline.</li> <li><code>--flop_inputs=true</code> and <code>--flop_outputs=true</code> - input and outputs for the     block are registered.</li> </ol>"},{"location":"tutorials/xlscc_overview/#additional-xlscc-examples","title":"Additional XLS[cc] examples.","text":"<p>The above tutorials only touches upon the capabilities of XLS[cc]. XLS[cc] is based on libclang and supports many C++17 features. Notable unsupported features include pointers, function pointers, and virtual methods.</p> <p>For developers, it is possible to check if a specific feature is supported by checking translator_logic_test.cc and other unit tests in the same directory.</p>"},{"location":"tutorials/xlscc_pipelined_loops/","title":"Tutorial: XLS[cc] pipelined loops.","text":"<ul> <li>Tutorial: XLS[cc] pipelined loops.<ul> <li>C++ Source</li> <li>Generate optimized XLS IR.</li> <li>Examine the optimized IR</li> <li>Perform code-generation into a pipelined Verilog block.</li> <li>Additional XLS[cc] examples.</li> </ul> </li> </ul> <p>This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a C++ function containing a pipelined loop. Pipelined loops are an automatic way of generating stateful logic that can often be more intuitive and software-like than using explicit state (eg via <code>static</code>).</p>"},{"location":"tutorials/xlscc_pipelined_loops/#c-source","title":"C++ Source","text":"<p>Create a file named <code>test_loop.cc</code> with the following contents.</p> <pre><code>template&lt;typename T&gt;\nusing InputChannel = __xls_channel&lt;T, __xls_channel_dir_In&gt;;\ntemplate&lt;typename T&gt;\nusing OutputChannel = __xls_channel&lt;T, __xls_channel_dir_Out&gt;;\n\nclass TestBlock {\npublic:\n    InputChannel&lt;int&gt; in;\n    OutputChannel&lt;int&gt; out;\n\n    #pragma hls_top\n    void Run() {\n    int sum = 0;\n        #pragma hls_pipeline_unroll yes\n        for(int i=0;i&lt;4;++i) {\n            sum += in.read();\n        }\n        out.write(sum);\n    }\n};\n</code></pre>"},{"location":"tutorials/xlscc_pipelined_loops/#generate-optimized-xls-ir","title":"Generate optimized XLS IR.","text":"<p>Use a combination of <code>xlscc</code> and <code>opt_main</code> to generate optimized XLS IR.</p> <pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test_loop.cc \\\n  --block_from_class TestBlock --block_pb block.pb \\\n  &gt; test_loop.ir\n$ ./bazel-bin/xls/tools/opt_main test_loop.ir --inline_procs &gt; test_loop.opt.ir\n</code></pre> <p>The <code>--inline_procs</code> option is necessary to make pipelined loops synthesizable.</p>"},{"location":"tutorials/xlscc_pipelined_loops/#examine-the-optimized-ir","title":"Examine the optimized IR","text":"<p><code>test_loop.opt.ir</code> should look like this, containing only one proc:</p> <pre><code>package my_package\n\nfile_number 1 \"/usr/local/google/home/seanhaskell/tmp/tutorial_loop.cc\"\n\nchan in(bits[32], id=0, kind=streaming, ops=receive_only, flow_control=ready_valid, metadata=\"\"\"\"\"\")\nchan out(bits[32], id=1, kind=streaming, ops=send_only, flow_control=ready_valid, metadata=\"\"\"\"\"\")\n\ntop proc TestBlock_proc(tkn: token, __for_1_proc_state__4: bits[1], __for_1_proc_state__5: bits[32], __for_1_proc_state__6: bits[32], __for_1_proc_activation__1: bits[1], __for_1_ctx_out_receive_holds_activation__1: bits[1], TestBlock_proc_activation__1: bits[1], after_all_186_holds_activation_0__1: bits[1], after_all_186_holds_activation_1__1: bits[1], after_all_84_holds_activation_0__1: bits[1], after_all_84_holds_activation_1__1: bits[1], after_all_84_holds_activation_2__1: bits[1], __for_1_ctx_in_receive_holds_activation__1: bits[1], init={1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0}) {\n...\n  next (after_all.354, __for_1_proc___first_tick_next_state, __for_1_proc_sum_next_state, __for_1_proc_i_next_state, __for_1_ctx_out_receive_activation_out, __for_1_ctx_out_receive_holds_activation_next__1, after_all_84_is_activated, after_all_186_holds_activation_0_next__1, after_all_186_holds_activation_1_next__1, after_all_84_holds_activation_0_next__1, after_all_84_holds_activation_1_next__1, after_all_84_holds_activation_2_next__1, __for_1_ctx_in_receive_holds_activation_next__1)\n}\n</code></pre>"},{"location":"tutorials/xlscc_pipelined_loops/#perform-code-generation-into-a-pipelined-verilog-block","title":"Perform code-generation into a pipelined Verilog block.","text":"<p>In this case, we will generate a single-stage pipeline without input and output flops. This will result in a module with a 32-bit increment adder along with 32-bit of state.</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main test_loop.opt.ir \\\n  --generator=pipeline \\\n  --delay_model=\"sky130\" \\\n  --output_verilog_path=xls_counter.v \\\n  --module_name=xls_counter \\\n  --top=TestBlock_proc \\\n  --reset=rst \\\n  --reset_active_low=false \\\n  --reset_asynchronous=false \\\n  --reset_data_path=true \\\n  --pipeline_stages=1  \\\n  --flop_inputs=false \\\n  --flop_outputs=false\n</code></pre>"},{"location":"tutorials/xlscc_pipelined_loops/#additional-xlscc-examples","title":"Additional XLS[cc] examples.","text":"<p>For developers, it is possible to check if a specific feature is supported by checking translator_proc_test.cc for unit tests.</p>"},{"location":"tutorials/xlscc_state/","title":"Tutorial: XLS[cc] state.","text":"<ul> <li>Tutorial: XLS[cc] state.<ul> <li>C++ Source</li> <li>Generate optimized XLS IR.</li> <li>Perform code-generation into a pipelined Verilog block.</li> <li>Additional XLS[cc] examples.</li> </ul> </li> </ul> <p>This tutorial is aimed at walking you through the implementation and synthesis into Verilog of a C++ function containing state.</p> <p>XLS[cc] may infer that in order to achieve a particular implementation of a C++ function, operations may occur over multiple cycles and require additional proc state to be kept. Common constructs that may require this are static variables, or loops that aren't unrolled. In this tutorial we give an example using static variables.</p>"},{"location":"tutorials/xlscc_state/#c-source","title":"C++ Source","text":"<p>Create a file named <code>test_state.cc</code> with the following contents.</p> <pre><code>template&lt;typename T&gt;\nusing OutputChannel = __xls_channel&lt;T, __xls_channel_dir_Out&gt;;\n\nclass TestBlock {\npublic:\n    OutputChannel&lt;int&gt; out;\n\n    int count = 0;\n\n    #pragma hls_top\n    void Run() {\n        out.write(count);\n        count++;\n    }\n};\n</code></pre>"},{"location":"tutorials/xlscc_state/#generate-optimized-xls-ir","title":"Generate optimized XLS IR.","text":"<p>Use a combination of <code>xlscc</code> and <code>opt_main</code> to generate optimized XLS IR.</p> <pre><code>$ ./bazel-bin/xls/contrib/xlscc/xlscc test_state.cc \\\n  --block_from_class TestBlock --block_pb block.pb \\\n  &gt; test_state.ir\n$ ./bazel-bin/xls/tools/opt_main test_state.ir &gt; test_state.opt.ir\n</code></pre>"},{"location":"tutorials/xlscc_state/#perform-code-generation-into-a-pipelined-verilog-block","title":"Perform code-generation into a pipelined Verilog block.","text":"<p>In this case, we will generate a single-stage pipeline without input and output flops. This will result in a module with a 32-bit increment adder along with 32-bit of state.</p> <pre><code>$ ./bazel-bin/xls/tools/codegen_main test_state.opt.ir \\\n  --generator=pipeline \\\n  --delay_model=\"sky130\" \\\n  --output_verilog_path=xls_counter.v \\\n  --module_name=xls_counter \\\n  --top=TestBlock_proc \\\n  --reset=rst \\\n  --reset_active_low=false \\\n  --reset_asynchronous=false \\\n  --reset_data_path=true \\\n  --pipeline_stages=1  \\\n  --flop_inputs=false \\\n  --flop_outputs=false\n</code></pre> <p>After running codegen, you should see a file named <code>xls_counter.v</code> with contents similar to the following.</p> <pre><code>module xls_counter(\n  input wire clk,\n  input wire rst,\n  input wire out_rdy,\n  output wire [31:0] out,\n  output wire out_vld\n);\n  reg [31:0] __st__1;\n  wire literal_43;\n  wire literal_40;\n  wire [31:0] add_37;\n  wire pipeline_enable;\n  assign literal_43 = 1'h1;\n  assign literal_40 = 1'h1;\n  assign add_37 = __st__1 + 32'h0000_0001;\n  assign pipeline_enable = literal_43 &amp; literal_40 &amp; out_rdy &amp; (literal_43 &amp; literal_40 &amp; out_rdy);\n  always_ff @ (posedge clk) begin\n    if (rst) begin\n      __st__1 &lt;= 32'h0000_0000;\n    end else begin\n      __st__1 &lt;= pipeline_enable ? add_37 : __st__1;\n    end\n  end\n  assign out = __st__1;\n  assign out_vld = literal_40 &amp; literal_43 &amp; 1'h1;\nendmodule\n</code></pre>"},{"location":"tutorials/xlscc_state/#additional-xlscc-examples","title":"Additional XLS[cc] examples.","text":"<p>For developers, it is possible to check if a specific feature is supported by checking translator_static_test.cc and translator_proc_test.cc for unit tests.</p>"}]}