{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"XLS : Accelerated HW Synthesis What is XLS? The XLS (Accelerated HW Synthesis) project aims to enable the rapid development of hardware IP that also runs as efficient host software via \"software style\" methodology. XLS implements a High Level Synthesis (HLS) toolchain which produces synthesizable designs from flexible, high-level descriptions of functionality. It is fully Open Source: Apache 2 licensed and developed via GitHub. XLS is used inside of Google for generating feed-forward pipelines from \"building block\" routines / libraries that can be easily retargeted, reused, and composed in a latency-insensitive manner. Not yet available , but active work in progress is the implementation of XLS concurrent processes , in Communicating Sequential Processes (CSP) style, that allow pipelines to communicate with each other and induct over time. XLS is still experimental, undergoing rapid development, and not an officially supported Google product. Expect bugs and sharp edges. Please help by trying it out, reporting bugs , and letting us know what you think! Building From Source Currently, XLS must be built from source using the Bazel build system. Note: Binary distributions of the XLS library are not currently available, but we hope to enable them via continuous integration, see this issue . The following instructions are for the Ubuntu 20.04 (Focal) Linux distribution. Note that we start by assuming Bazel has been installed . # Follow the bazel install instructions: # https://docs.bazel.build/versions/master/install-ubuntu.html # # Afterwards we observe: $ bazel --version bazel 3.2.0 $ sudo apt install python3-dev python3-distutils python3-dev libtinfo5 # py_binary currently assume they can refer to /usr/bin/env python # even though Ubuntu 20.04 has no `python`, only `python3`. # See https://github.com/bazelbuild/bazel/issues/8685 $ mkdir -p $HOME/opt/bin/ $ ln -s $(which python3) $HOME/opt/bin/python $ echo 'export PATH=$HOME/opt/bin:$PATH' >> ~/.bashrc $ source ~/.bashrc $ bazel test -c opt ... Stack Diagram and Project Layout Navigating a new code base can be daunting; the following description provides a high-level view of the important directories and their intended organization / purpose, and correspond to the components in this XLS stack diagram: dependency_support : Configuration files that load, build, and expose Bazel targets for external dependencies of XLS. docs : Generated documentation served via GitHub pages: https://google.github.io/xls/ docs_src : Markdown file sources, rendered to docs via mkdocs . xls : Project-named subdirectory within the repository, in common Bazel-project style. build : Build macros that create XLS artifacts; e.g. convert DSL to IR, create test targets for DSL code, etc. codegen : Verilog AST (VAST) support to generate Verilog/SystemVerilog operations and FSMs. VAST is built up by components we call generators (e.g. PipelineGenerator, SequentialGenerator for FSMs) in the translation from XLS IR. common : \"base\" functionality that layers on top of standard library usage. Generally we use Abseil versions of base constructs wherever possible. contrib/xlscc : Experimental C++ syntax support that targets XLS IR (alternative path to DSLX) developed by a sister team at Google, sharing the same open source / testing flow as the rest of the XLS project. May be of particular interest for teams with existing C++ HLS code bases. data_structures : Generic data structures used in XLS that augment standard libraries; e.g. BDDs, union find, min cut, etc. delay_model : Functionality to characterize, describe, and interpolate data delay for XLS IR operations on a target backend process. Already-characterized descriptions are placed in xls/delay_model/models and can be referred to via command line flags. dslx : A DSL (called \"DSLX\") that mimics Rust, while being an immutable expression-language dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph. XLS team has found dataflow DSLs are a good fit to describe hardware as compared to languages designed assume von Neumann style computation. dslx/fuzzer : A whole-stack multiprocess fuzzer that generates programs at the DSL level and cross-compares different execution engines (DSL interpreter, IR interpreter, IR JIT, code-generated-Verilog simulator). Designed so that it can easily be run on different nodes in a cluster simultaneously and accumulate shared findings. examples : Example computations that are tested and executable through the XLS stack. experimental : Artifacts captured from experimental explorations. ir : XLS IR definition, text parser/formatter, and facilities for abstract evaluation and execution engines ( IR interpreter , JIT ). modules : Hardware building block DSLX \"libraries\" (outside the DSLX standard library) that may be easily reused or instantiated in a broader design. netlist : Libraries that parse/analyze/interpret netlist-level descriptions, as are generally given in simple structural Verilog with an associated cell library. passes : Passes that run on the XLS IR as part of optimization, before scheduling / code generation. scheduling : Scheduling algorithms, determine when operations execute (e.g. which pipeline stage) in a clocked design. simulation : Code that wraps Verilog simulators and generates Verilog testbenches for XLS computations. solvers : Converters from XLS IR into SMT solver input, such that formal proofs can be run on XLS computations; e.g. Logical Equalence Checks between XLS IR and a netlist description. synthesis : Interface that wraps backend synthesis flows, such that tools can be retargeted e.g. between ASIC and FPGA flows. tests : Integration tests that span various top-level components of the XLS project. tools : Many tools that work with the XLS system and its libraries in a decomposed way via command line interfaces. uncore_rtl : Helper RTL that interfaces XLS-generated blocks with device top-level for e.g. FPGA experiments. visualization : Visualization tools to inspect the XLS compiler/system interactively. See IR visualization . Community Discussions about XLS - development, debugging, usage, and anything else - should go to the xls-dev mailing list . Contributors The following are contributors to the XLS project: Brandon Jiang Chris Leary Derek Lockhart Hans Montero Jonathan Bailey Julian Viera Kevin Harlley Mark Heffernan Per Gr\u00f6n Rebecca Chen (Pytype) Robert Hundt Rob Springer Sean Purser-Haskell","title":"Overview"},{"location":"#xls-accelerated-hw-synthesis","text":"","title":"XLS: Accelerated HW Synthesis"},{"location":"#what-is-xls","text":"The XLS (Accelerated HW Synthesis) project aims to enable the rapid development of hardware IP that also runs as efficient host software via \"software style\" methodology. XLS implements a High Level Synthesis (HLS) toolchain which produces synthesizable designs from flexible, high-level descriptions of functionality. It is fully Open Source: Apache 2 licensed and developed via GitHub. XLS is used inside of Google for generating feed-forward pipelines from \"building block\" routines / libraries that can be easily retargeted, reused, and composed in a latency-insensitive manner. Not yet available , but active work in progress is the implementation of XLS concurrent processes , in Communicating Sequential Processes (CSP) style, that allow pipelines to communicate with each other and induct over time. XLS is still experimental, undergoing rapid development, and not an officially supported Google product. Expect bugs and sharp edges. Please help by trying it out, reporting bugs , and letting us know what you think!","title":"What is XLS?"},{"location":"#building-from-source","text":"Currently, XLS must be built from source using the Bazel build system. Note: Binary distributions of the XLS library are not currently available, but we hope to enable them via continuous integration, see this issue . The following instructions are for the Ubuntu 20.04 (Focal) Linux distribution. Note that we start by assuming Bazel has been installed . # Follow the bazel install instructions: # https://docs.bazel.build/versions/master/install-ubuntu.html # # Afterwards we observe: $ bazel --version bazel 3.2.0 $ sudo apt install python3-dev python3-distutils python3-dev libtinfo5 # py_binary currently assume they can refer to /usr/bin/env python # even though Ubuntu 20.04 has no `python`, only `python3`. # See https://github.com/bazelbuild/bazel/issues/8685 $ mkdir -p $HOME/opt/bin/ $ ln -s $(which python3) $HOME/opt/bin/python $ echo 'export PATH=$HOME/opt/bin:$PATH' >> ~/.bashrc $ source ~/.bashrc $ bazel test -c opt ...","title":"Building From Source"},{"location":"#stack-diagram-and-project-layout","text":"Navigating a new code base can be daunting; the following description provides a high-level view of the important directories and their intended organization / purpose, and correspond to the components in this XLS stack diagram: dependency_support : Configuration files that load, build, and expose Bazel targets for external dependencies of XLS. docs : Generated documentation served via GitHub pages: https://google.github.io/xls/ docs_src : Markdown file sources, rendered to docs via mkdocs . xls : Project-named subdirectory within the repository, in common Bazel-project style. build : Build macros that create XLS artifacts; e.g. convert DSL to IR, create test targets for DSL code, etc. codegen : Verilog AST (VAST) support to generate Verilog/SystemVerilog operations and FSMs. VAST is built up by components we call generators (e.g. PipelineGenerator, SequentialGenerator for FSMs) in the translation from XLS IR. common : \"base\" functionality that layers on top of standard library usage. Generally we use Abseil versions of base constructs wherever possible. contrib/xlscc : Experimental C++ syntax support that targets XLS IR (alternative path to DSLX) developed by a sister team at Google, sharing the same open source / testing flow as the rest of the XLS project. May be of particular interest for teams with existing C++ HLS code bases. data_structures : Generic data structures used in XLS that augment standard libraries; e.g. BDDs, union find, min cut, etc. delay_model : Functionality to characterize, describe, and interpolate data delay for XLS IR operations on a target backend process. Already-characterized descriptions are placed in xls/delay_model/models and can be referred to via command line flags. dslx : A DSL (called \"DSLX\") that mimics Rust, while being an immutable expression-language dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph. XLS team has found dataflow DSLs are a good fit to describe hardware as compared to languages designed assume von Neumann style computation. dslx/fuzzer : A whole-stack multiprocess fuzzer that generates programs at the DSL level and cross-compares different execution engines (DSL interpreter, IR interpreter, IR JIT, code-generated-Verilog simulator). Designed so that it can easily be run on different nodes in a cluster simultaneously and accumulate shared findings. examples : Example computations that are tested and executable through the XLS stack. experimental : Artifacts captured from experimental explorations. ir : XLS IR definition, text parser/formatter, and facilities for abstract evaluation and execution engines ( IR interpreter , JIT ). modules : Hardware building block DSLX \"libraries\" (outside the DSLX standard library) that may be easily reused or instantiated in a broader design. netlist : Libraries that parse/analyze/interpret netlist-level descriptions, as are generally given in simple structural Verilog with an associated cell library. passes : Passes that run on the XLS IR as part of optimization, before scheduling / code generation. scheduling : Scheduling algorithms, determine when operations execute (e.g. which pipeline stage) in a clocked design. simulation : Code that wraps Verilog simulators and generates Verilog testbenches for XLS computations. solvers : Converters from XLS IR into SMT solver input, such that formal proofs can be run on XLS computations; e.g. Logical Equalence Checks between XLS IR and a netlist description. synthesis : Interface that wraps backend synthesis flows, such that tools can be retargeted e.g. between ASIC and FPGA flows. tests : Integration tests that span various top-level components of the XLS project. tools : Many tools that work with the XLS system and its libraries in a decomposed way via command line interfaces. uncore_rtl : Helper RTL that interfaces XLS-generated blocks with device top-level for e.g. FPGA experiments. visualization : Visualization tools to inspect the XLS compiler/system interactively. See IR visualization .","title":"Stack Diagram and Project Layout"},{"location":"#community","text":"Discussions about XLS - development, debugging, usage, and anything else - should go to the xls-dev mailing list .","title":"Community"},{"location":"#contributors","text":"The following are contributors to the XLS project: Brandon Jiang Chris Leary Derek Lockhart Hans Montero Jonathan Bailey Julian Viera Kevin Harlley Mark Heffernan Per Gr\u00f6n Rebecca Chen (Pytype) Robert Hundt Rob Springer Sean Purser-Haskell","title":"Contributors"},{"location":"build_system/","text":"Build system XLS uses the Bazel build system for itself and all its dependencies. Bazel is an easy to configure and use, and has powerful extension facilities. (It's also well-documented !) XLS provdes a number of Starlark macros , described below, to simplify build target definition. Build system Macros dslx_codegen dslx_jit_wrapper dslx_test dslx_generated_rtl Bazel queries Finding transitive dependencies Finding dependees (\"reverse dependencies\") Macros Below are summaries of public macro functionality. Full documentation is available in xls/build/build_defs.bzl . dslx_codegen This macro generates Verilog from an DSLX target (currently given as a dslx_test target). This target also accepts several key/value parameters (as configs for generation: clock_period_ps : The target clock period in picoseconds. pipeline_stages : The desired number of pipeline stages. entry : The DSLX function to synthesize. If not given, a \"best guess\" will be made. clock_margin_percent : The percent of the target clock period to reserve as \"margin\". dslx_jit_wrapper Generates a source/header file pair to wrap invocation of a DSLX function by the JIT - this wrapper is a much more straightforward way to interact with the JIT versus direct use. It accepts a DSLX target and entry function in the same manner as above. dslx_test Main driver for: Compiling DSLX files to IR Running DSLX test cases (defined alongside the designs) Proving DSLX/IR equivalence Generating a benchmark for the associated IR Proving logical equivalence for the optimized vs. unoptimized IR In general, if one has a DSLX .x file, there should be an associated dslx_test target - this is the entry point to \"downstream\" capabilities (IR transformations, codegen, interpretation/JIT, etc.). Note: This macro is targeted for refactoring, as it currently contains a broad swath of not-always-directly-related functionality. dslx_generated_rtl This macro is for internal developer use only, for generating RTL with internally-released toolchains. Bazel queries Understanding the build tree for a new project can be difficult, but fortunately Bazel provides a powerful query mechanism . bazel query enables a user to examine build targets, dependencies between them, and much more. A few usage examples are provided here, but the full documentation (linked above) is comprehensive. Finding transitive dependencies To understand why, for example, the combinational verilog generator depends on the ABSL container algorithm library, one could run: $ bazel query 'somepath(//xls/codegen:combinational_generator, @com_google_absl//absl/algorithm:container)' //xls/codegen:combinational_generator //xls/codegen:vast @com_google_absl//absl/algorithm:container This result shows that one such path goes through the :vast target. Another such path goes through the xls/ir:ir target, then the xls/ir:value target. somepath provides some path, not all paths (that's what allpaths is for). Finding dependees (\"reverse dependencies\") Sometimes it's useful to identify the set of targets depending on some other target - the rdeps query performs this: $ bazel query 'rdeps(//xls/codegen:all, //xls/codegen:combinational_generator)' //xls/codegen:flattening_test //xls/ir:ir_test_base //xls/codegen:combinational_generator_test //xls/codegen:combinational_generator This shows the transitive closure of all dependencies of the combinational generator, with the starting set being all targets in //xls/codegen:all . This set of dependees can quickly grow to be unmanageable, so keep the initial set (the first argument) as small as possible, and consider specifying a third argument for maximum search depth.","title":"Build System"},{"location":"build_system/#build-system","text":"XLS uses the Bazel build system for itself and all its dependencies. Bazel is an easy to configure and use, and has powerful extension facilities. (It's also well-documented !) XLS provdes a number of Starlark macros , described below, to simplify build target definition. Build system Macros dslx_codegen dslx_jit_wrapper dslx_test dslx_generated_rtl Bazel queries Finding transitive dependencies Finding dependees (\"reverse dependencies\")","title":"Build system"},{"location":"build_system/#macros","text":"Below are summaries of public macro functionality. Full documentation is available in xls/build/build_defs.bzl .","title":"Macros"},{"location":"build_system/#dslx_codegen","text":"This macro generates Verilog from an DSLX target (currently given as a dslx_test target). This target also accepts several key/value parameters (as configs for generation: clock_period_ps : The target clock period in picoseconds. pipeline_stages : The desired number of pipeline stages. entry : The DSLX function to synthesize. If not given, a \"best guess\" will be made. clock_margin_percent : The percent of the target clock period to reserve as \"margin\".","title":"dslx_codegen"},{"location":"build_system/#dslx_jit_wrapper","text":"Generates a source/header file pair to wrap invocation of a DSLX function by the JIT - this wrapper is a much more straightforward way to interact with the JIT versus direct use. It accepts a DSLX target and entry function in the same manner as above.","title":"dslx_jit_wrapper"},{"location":"build_system/#dslx_test","text":"Main driver for: Compiling DSLX files to IR Running DSLX test cases (defined alongside the designs) Proving DSLX/IR equivalence Generating a benchmark for the associated IR Proving logical equivalence for the optimized vs. unoptimized IR In general, if one has a DSLX .x file, there should be an associated dslx_test target - this is the entry point to \"downstream\" capabilities (IR transformations, codegen, interpretation/JIT, etc.). Note: This macro is targeted for refactoring, as it currently contains a broad swath of not-always-directly-related functionality.","title":"dslx_test"},{"location":"build_system/#dslx_generated_rtl","text":"This macro is for internal developer use only, for generating RTL with internally-released toolchains.","title":"dslx_generated_rtl"},{"location":"build_system/#bazel-queries","text":"Understanding the build tree for a new project can be difficult, but fortunately Bazel provides a powerful query mechanism . bazel query enables a user to examine build targets, dependencies between them, and much more. A few usage examples are provided here, but the full documentation (linked above) is comprehensive.","title":"Bazel queries"},{"location":"build_system/#finding-transitive-dependencies","text":"To understand why, for example, the combinational verilog generator depends on the ABSL container algorithm library, one could run: $ bazel query 'somepath(//xls/codegen:combinational_generator, @com_google_absl//absl/algorithm:container)' //xls/codegen:combinational_generator //xls/codegen:vast @com_google_absl//absl/algorithm:container This result shows that one such path goes through the :vast target. Another such path goes through the xls/ir:ir target, then the xls/ir:value target. somepath provides some path, not all paths (that's what allpaths is for).","title":"Finding transitive dependencies"},{"location":"build_system/#finding-dependees-reverse-dependencies","text":"Sometimes it's useful to identify the set of targets depending on some other target - the rdeps query performs this: $ bazel query 'rdeps(//xls/codegen:all, //xls/codegen:combinational_generator)' //xls/codegen:flattening_test //xls/ir:ir_test_base //xls/codegen:combinational_generator_test //xls/codegen:combinational_generator This shows the transitive closure of all dependencies of the combinational generator, with the starting set being all targets in //xls/codegen:all . This set of dependees can quickly grow to be unmanageable, so keep the initial set (the first argument) as small as possible, and consider specifying a third argument for maximum search depth.","title":"Finding dependees (\"reverse dependencies\")"},{"location":"contributing/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Community Guidelines This project follows Google's Open Source Community Guidelines . Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Code style When writing code contributions to the project, please make sure to follow the style guides: The Google C++ Style Guide and the Google Python Style Guide . There are a few small XLS clarifications for local style on this project where the style guide is ambiguous. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Pull Request Style We ask contributors to squash all the commits in the PR into a single one, in order to have a cleaner revision history. Generally, this can be accomplished by: proj/xls$ # Here we assume origin points to google/xls. proj/xls$ git fetch origin main proj/xls$ git merge-base origin/main my-branch-name # Tells you common ancesor COMMIT_HASH. proj/xls$ git reset --soft $COMMIT_HASH proj/xls$ git commit -a -m \"My awesome squashed commit message!!!1\" proj/xls$ # Now we can more easily rebase our squashed commit on main. proj/xls$ git rebase origin/main Rebased branches can be pushed to their corresponding PRs with --force . See also this Stack Overflow question . Rendering Documentation XLS uses mkdocs to render its documentation, and serves it via GitHub pages on https://google.github.io/xls -- to render documentation locally as a preview, set up mkdocs as follows: proj/xls$ mkvirtualenv xls-mkdocs-env proj/xls$ pip install mkdocs-material mdx_truly_sane_lists proj/xls$ mkdocs serve","title":"Contributing"},{"location":"contributing/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"contributing/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"contributing/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://cla.developers.google.com/ to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"contributing/#code-style","text":"When writing code contributions to the project, please make sure to follow the style guides: The Google C++ Style Guide and the Google Python Style Guide . There are a few small XLS clarifications for local style on this project where the style guide is ambiguous.","title":"Code style"},{"location":"contributing/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"Code reviews"},{"location":"contributing/#pull-request-style","text":"We ask contributors to squash all the commits in the PR into a single one, in order to have a cleaner revision history. Generally, this can be accomplished by: proj/xls$ # Here we assume origin points to google/xls. proj/xls$ git fetch origin main proj/xls$ git merge-base origin/main my-branch-name # Tells you common ancesor COMMIT_HASH. proj/xls$ git reset --soft $COMMIT_HASH proj/xls$ git commit -a -m \"My awesome squashed commit message!!!1\" proj/xls$ # Now we can more easily rebase our squashed commit on main. proj/xls$ git rebase origin/main Rebased branches can be pushed to their corresponding PRs with --force . See also this Stack Overflow question .","title":"Pull Request Style"},{"location":"contributing/#rendering-documentation","text":"XLS uses mkdocs to render its documentation, and serves it via GitHub pages on https://google.github.io/xls -- to render documentation locally as a preview, set up mkdocs as follows: proj/xls$ mkvirtualenv xls-mkdocs-env proj/xls$ pip install mkdocs-material mdx_truly_sane_lists proj/xls$ mkdocs serve","title":"Rendering Documentation"},{"location":"data_layout/","text":"Data layout For many uses, XLS types exist within their own conceptual space or domain, so \"portability\" concerns don't exist. When interacting with the JIT, however, XLS and host-native types must interact, so the data layouts of both must be understood and possibly reconciled. Data layout XLS data layout Host data layout JIT data layout Packed views XLS data layout The concrete XLS Bits type is the ultimate container of actual data for any XLS IR type: tuples and arrays may be contain any number of tuple, array, or bits types, but whatever the layout of the type tree, all leaf nodes are Bits. When accessing the underlying storage of a Bits via the ToBytes() member function, the results are returned in a big-endian layout, i.e, with the most-significant data elements stored in the lowest addressible location. For example, the 32-bit value 12,345,678 (0xBC614E), would be returned as: High <-- Low 0x 4E 61 BC 00 Host data layout Different architectures can use different native layouts. For example, x86 (and descendants) use little-endian (i.e., 0x00 BC 61 4E ), and modern ARM can be configurable as either. (There are actually other layouts, but they're best left to the dustbin of history). JIT data layout From the above, we can see that XLS' native layout differs from that of most modern hosts. When compiling XLS code, the [LLVM] JIT understandably uses the host's native layout. What this means is that any data fed into the JIT from XLS will need to be byte-swapped before ingestion. For Value or unpacked view input, this swapping is handled automatically, in LlvmIrRuntime::PackArgs() (via LlvmIrRuntime::BlitValueToBuffer() ) - and the __un__swapping is also automatically performed in LlvmIrRuntime::UnpackBuffer() . Thus, for these uses, no special action is required of the user. Packed views However , this is not the case for use of packed views. The motivating use case for packed views is to allow users to map native types directly into JIT-usable values - for example, to use an IEEE float32 (e.g., a C float ) directly , without needing to be exploded into a bits[1] for the sign, a bits[8] for the exponent, and a bits[23] for the significand. When creating a packed view from a C float , no special action is needed - that float is in native host layout, which is the layout used by the JIT. If, however, data is coming from XLS (perhaps a float converted into a Value, manipulated in some way, then passed into the JIT), then the user must un-swap the bits back to native layout. This is because the JIT has no way of knowing the provenance of that data (if it's from a native type or XLS), so it's up to the provider of that data to ensure proper layout. Distilled into a simple rule of thumb: if packed view data is coming from XLS, it needs to be byte swapped before being passed into the JIT.","title":"Data Layout"},{"location":"data_layout/#data-layout","text":"For many uses, XLS types exist within their own conceptual space or domain, so \"portability\" concerns don't exist. When interacting with the JIT, however, XLS and host-native types must interact, so the data layouts of both must be understood and possibly reconciled. Data layout XLS data layout Host data layout JIT data layout Packed views","title":"Data layout"},{"location":"data_layout/#xls-data-layout","text":"The concrete XLS Bits type is the ultimate container of actual data for any XLS IR type: tuples and arrays may be contain any number of tuple, array, or bits types, but whatever the layout of the type tree, all leaf nodes are Bits. When accessing the underlying storage of a Bits via the ToBytes() member function, the results are returned in a big-endian layout, i.e, with the most-significant data elements stored in the lowest addressible location. For example, the 32-bit value 12,345,678 (0xBC614E), would be returned as: High <-- Low 0x 4E 61 BC 00","title":"XLS data layout"},{"location":"data_layout/#host-data-layout","text":"Different architectures can use different native layouts. For example, x86 (and descendants) use little-endian (i.e., 0x00 BC 61 4E ), and modern ARM can be configurable as either. (There are actually other layouts, but they're best left to the dustbin of history).","title":"Host data layout"},{"location":"data_layout/#jit-data-layout","text":"From the above, we can see that XLS' native layout differs from that of most modern hosts. When compiling XLS code, the [LLVM] JIT understandably uses the host's native layout. What this means is that any data fed into the JIT from XLS will need to be byte-swapped before ingestion. For Value or unpacked view input, this swapping is handled automatically, in LlvmIrRuntime::PackArgs() (via LlvmIrRuntime::BlitValueToBuffer() ) - and the __un__swapping is also automatically performed in LlvmIrRuntime::UnpackBuffer() . Thus, for these uses, no special action is required of the user.","title":"JIT data layout"},{"location":"data_layout/#packed-views","text":"However , this is not the case for use of packed views. The motivating use case for packed views is to allow users to map native types directly into JIT-usable values - for example, to use an IEEE float32 (e.g., a C float ) directly , without needing to be exploded into a bits[1] for the sign, a bits[8] for the exponent, and a bits[23] for the significand. When creating a packed view from a C float , no special action is needed - that float is in native host layout, which is the layout used by the JIT. If, however, data is coming from XLS (perhaps a float converted into a Value, manipulated in some way, then passed into the JIT), then the user must un-swap the bits back to native layout. This is because the JIT has no way of knowing the provenance of that data (if it's from a native type or XLS), so it's up to the provider of that data to ensure proper layout. Distilled into a simple rule of thumb: if packed view data is coming from XLS, it needs to be byte swapped before being passed into the JIT.","title":"Packed views"},{"location":"delay_estimation/","text":"Delay Estimation Methodology Context This doc describes the delay estimation methodology used in XLS and related background. Estimating delays through networks of CMOS gates is a rich topic, and Static Timing Analysis (STA) is used in chip backend flows to ensure that, even for parts operating at the tail end of the distribution, chips continue to function as specified logically in the netlist. In stark contrast to something with very concrete constraints, like a \"post-global routing, high-accuracy parasitics static timing analysis\", the HLS tool needs to estimate at a high level, reasonably near to the user's behavioral specification, what delay HLS operations will have when they are \"stacked on top\" of each other in a data dependent fashion in the design. This information lets the HLS tool schedule operations into cycles without violating timing constraints; e.g. if the user specifies 1GHz (= 1000ps) as their target clock frequency, the HLS tool may choose to pack as much data dependent work into the 1000ps budget (minus clock uncertainty) as it can on any given cycle. Although what we're estimating is a close relative of Static Timing Analysis, the fact it's being analyzed at a very high level presents a different set of tradeoffs, where coarse granularity estimation and conservative bounds are more relevant. Fine-grained / precise analyses are used later in the chip design process, in the backend flows, but the HLS tool acts more like an RTL designer, creating an RTL output that early timing analysis deems acceptable. Notably, we don't want to be too conservative, as being conservative on timing can lead to designs that take more area and consume more power, as the flops introduced by additional pipeline stages are significant. We want to be as accurate as possible while providing a good experience of being able to close timing quickly (say, in a single pass, or with a small number of iterations in case of a pathological design). Background What XLS is compiling XLS currently supports feed-forward pipelines -- once the user program is unrolled into a big \"sea of nodes\" we must schedule each of those operations (represented by nodes) to occur in a cycle. It seems clear that an operation like add(bits[32], bits[32]) -> bits[32] takes some amount of time to produce a result -- we need to be able to determine what that amount of time is for packing that operation into a given cycle. 1 Note that XLS operations are parametric in their bitwidth, so add(bits[17], bits[17]) -> bits[17] is just as possible as a value like 32 . This ain't C code. 1 : Note that we currently pack operations into cycles atomically -- that is, we don't break an add that would straddle a cycle boundary into add.first_half and add.second_half automatically to pull things as early as possible in the pipeline, but this is future work of interest. Ideally operations would be described structurally in a way that could automatically be cut up according to available delay budget. This would also permit operations in the IR that take more than a single cycle to produce a value (currently they would have to be \"legalized\" into operations that fit within a cycle, but that is not yet done, the user will simply receive a scheduling error). Some operations, such as bit_slice or concat are just wiring \"feng shui\"; however, they still have some relevance for delay calculations! Say we concatenate a value with zeros for zero extension. Even if we could schedule that in \"cycle 0\", if the consumer can only be placed in \"cycle 1\", we would want to \"sink\" the concat down into \"cycle 1\" as well to avoid unnecessary registers being materialized sending the zero values from \"cycle 0\". The delay problem Separately from XLS considerations, there are fundamental considerations in calculating the delay through clouds of functional logic in (generated) RTL. Between each launching and capturing flop is a functional network of logic gates, implemented with standard cells in our ASIC process flows. Chip designs target a particular clock frequency as their operating point, and the functional network has to produce its output value with a delay that meets the timing constraint of the clock frequency. The RTL designer typically has to iterate their design until: timing path delay <= target clock period - clock uncertainty For all timing paths in their design, where clock uncertainty includes setup/hold time constraints, and slop that's built in as margin for later sources of timing variability (like instantiating a clock tree, which can skew the clock signal observed by different flops). In a reasonable model, gate delay is affected by a small handful of properties, as reflected in the \"(Method of) Logical Effort\" book: The transistor network used to implement a logic function (AKA logical effort): on an input pin change, the gate of each transistor must be driven to a point it recognizes whether a 0 or 1 voltage is being presented. More gates to drive, or larger gates, means more work for the driver. The load being driven by the logic function (AKA electrical effort): fanning out to more gates generally means more work to drive them all to their threshold voltages. Being loaded down by bigger gates means more work to drive it to its threshold voltage. Parasitic delays: RC elements in the system that leech useful work, typically in a smaller way compared to the efforts listed above. The logical effort book describes a way to analyze the delays through a network of gates to find the minimal delay, and size transistors in a way that can achieve that minimal delay (namely by geometrically smoothing the ability for gate to drive capacitance). Confounding factors include: Medium/large wires: sizing transistors to smooth capacitance becomes difficult as fixed-capacitance elements (wires) are introduced. It seems that small wires have low enough capacitance they can generally be treated as parasitic. Divergence/reconvergence in the functional logic network (as a DAG). Different numbers of logic levels and different drive currents may be presented from different branches of a fork/join the logic graph, which forces delay analysis into a system of equations to attempt to minimize the overall delay, as observed by the critical path, with transistor sizing and gate choices. (Some simplifications are possible, like buffering non-critical paths until they have the same number of logic levels so they also have plenty of current to supply at join points.) Somewhat orthogonal to the analytical modeling problem, there are also several industry standards for supplying process information to Static Timing Analysis engines for determining delay through a netlist. This information is often given in interpolated tables for each standard cell, for example in the NLDM model describing how delay changes as a function of input transition time and load (load capacitance). These models and supplied pieces of data are important to keep in mind for contrast, as we now ignore it all and do something very simple. Simple Delay Estimation Currently, XLS delay estimation follows a conceptually simple procedure: For every operation in XLS (e.g. binary addition): * For some relevant-seeming set of bitwidths; e.g. {2, 4, 8, 16, ..., 2048} * Find the maximum frequency at which that operation closes timing at that bitwidth, in 100MHz units as determined by the synthesis tool. 2 Call the clock period for this frequency t_best . (Note that we currently just use a single process corner / voltage for this sweep.) * Subtract the clock uncertainty from t_best . * Record that value in a table (with the keys of the table being operation / bitwidth). 2 : The timing report can provide the delay through a path at any clock frequency, but a wrinkle is that synthesis tools potentially only start using their more aggressive techniques as you bump up against the failure-to-close-timing point -- there it'll be more likely to change the structure of the design to make it more delay friendly. The sweep helps to try to cajole it in that way. Inspecting the data acquired in this way we observe all of the plots consist of one or more the following delay components: Constant as a function of bitwidth for a given op (e.g. binary-or just requires a single gate for each bit regardless of the width of the inputs). Logarithmic as a function of bitwidth (e.g. adders may end up using tree-like structures to minimize delay, single-selector muxes end up using a tree to fan out the selector to the muxes, etc.). Linear as a function of bitwidth (e.g., ripple-carry adders and some components of multipliers). So given this observation we fit a curve of the form: a * bitwidth + b * log_2(bitwidth) + c to the sweep data for each operation, giving us (a, b, c) values to use in our XLS delay estimator. The utility delay_model_visualizer under the tools directory renders a graph of the delay model estimate against the measured data points. This graph for add shows a good correspondence to the measured delay. Sweeping multiple dimensions Operations with attributes in addition to bitwidth that affect delay are swept across multiple dimensions. An example is Op::kOneHotSelect which has dimensions of bitwidth and number of cases. For the Op::kOneHotSelect example the formula is: a * bitwidth + b * log_2(bitwidth) + c * (# of cases) + d * log_2(# of cases) + c Below is plot of delay for Op::kOneHotSelect showing the two dimensions of bitwidth and operand count affecting delay: Sources of pessimism/optimism This simple method has both sources of optimism and pessimism, though we hope to employ a method that will be generally conservative, so that users can easily close timing and get a close-to-best-achievable result (say, within tens of percent) with a single HLS iteration. Sources of pessimism (estimate is conservative): The operation sweeps mentioned are not bisecting to the picosecond, so there is inherent slop in the measurement on account of sweep granularity. We expect, in cycles where multiple dependent operations are present, there would be \"K-map style\" logic reductions with adjacent operations. For example, because we don't do cell library mapping in XLS delay estimation, something a user wrote that mapped to an AOI21 cell would be the sum of (and+or+invert) delays. [Unsure] May there be additional logic branch splitting options and earlier-produced results available to the synthesis tool when there are more operations in the graph (vs a lone critical path measured for a single operation)? Sources of optimism (estimate is overeager): For purposes of the sweep the outputs of an operation are only loaded by a single capture flop flop -- when operations have fanout the delay will increase. Note that we do account for fanout within individual operations as part of this sweep; e.g. a 128-bit selector fanout (e.g. 128 ways to 128 muxes) for a select is accounted for the in delay timing of the select operation. It is the output logic layer that is only loaded by a single flop in our characterization. Notably because most of these operations turn into trees of logic, there are $$log_2(bitcount)$$ layers of logic in which we can potentially smoothly increase drive strength out to the output logic layer, and paths can presumably be replicated by synthesis tools to reduce pointwise fanout when multiple high-level operations are data-dependent within a cycle. (Is it possible for a user to break up their 32-bit select into bitwise pieces in their XLS code to mess with our modeling? Sure, but probably not too expected, so we're currently sort of relying on the notion people are using high level operations instead of compodecomposing them into bitwise pieces in our graph.) A potential way to reconcile this output fanout in the future is to do a delay sweep with a high-capacitive fanout (e.g. four flops of load) and then ensure the IR has a maximum fanout of four for our delay estimation. Wiring delay / load / congestion / length are not estimated. This will need additional analysis / refinement as we run XLS through synthesis tools with advanced timing analysis, as it is certainly not viable for arbitrary designs (tight pipelines may be ok for now, though). Iterative refinement The caveats mentioned above seem somewhat daunting, but this first cut approach appears to work comfortably at target frequenties, in practice, for the real-world blocks being designed as XLS \"first samples\". Notably, human RTL designers fail to close timing on a first cut as well -- HLS estimations like the above assist in getting a numeric understanding (in lieu of an intuitive guess) of something that may close timing on a first cut. As this early model fails, we will continue to refine it; however, there is also a secondary procedure that can assist as the model improves. Let's call the delay estimation described above applied to a program a prediction of its delays. Let's call the first prediction we make p0 : p0 will either meet timing or fail to meet timing. When we meet timing with p0 , there may be additional wins left on the table. If we're willing to put synthesis tool runs \"in the loop\" (say running a \"tuner\" overnight), we can refine XLS's estimates according to the realities of the current program, and, for example, try to squeeze as much as possible into as few cycles as possible if near-optimality of latency/area/power were a large consideration. This loop would generate p1 , p2 , ... as it refined its model according to empirical data observed from the synthesis tool's more refined analysis. When we fail to close timing with p0 , we can feed back the negative slack delays for comparison with our estimates and relax estimates accordingly. Additionally, an \"aggression\" knob could be implemented that backs off delay estimations geometrically (say via a \"fudge factor\" coefficient) in order to ensure HLS developer time is not wasted unnecessarily to ensure. Once a combination of these mechanisms has obtained a design that closes timing, the \"meeting timing, now refine\" procedure can be employed as described above. On Hints To whatever extent possible, XLS should be the tool that reasons about how to target the backend (vs having a tool that sits on top of it and messes with XLS' input in an attempt to achieve a result). User-facing hint systems are typically very fragile, owing to the fact they don't have easily obeyed semantics. XLS, by contrast, knows about its own internals, so can do things with awareness of what's happened upstream and what remains to happen downstream. By contrast, we should continue to add ways for users to provide more semantic information / intent as part of their program (e.g. via more high-level patterns that make high level structure more clear), and make XLS smarter about how to lower those constructs into hardware (and why it should be lowering them that way) in the face of some prioritized objectives (power/area/latency). That being said, because we're probably trying to produce hardware at a given point in time against a given technology, it likely makes sense to permit human users to specify things directly (at a point in time), even if those specifications might be ignored / handled very differently in the future or against different technology nodes. This would be the moral equivalent of what existing EDA tools do as a \"one-off TCL file\" used in a particular design, vs something carried from design to design. Recall, though, that the intent of XLS is to make things easier to carry from design to design and require fewer one-off modifications! Tools XLS provides tools for analyzing its delay estimation model. (Note that the given IR should be in a form suitable for code generation; e.g. it has run through the opt_main binary). $ tools/benchmark_main crc32.opt.ir --clock_period_ps=500 <snip> Return value delay: 1362ps Critical path entry count: 42 Critical path: 1362ps (+ 5ps): not.29: bits[32] = not(xor.205: bits[32], pos=0,29,50) 1357ps (+ 20ps): xor.205: bits[32] = xor(concat.195: bits[32], and.196: bits[32], pos=0,24,19) 1337ps (+ 15ps): and.196: bits[32] = and(neg.194: bits[32], literal.304: bits[32], pos=0,24,33) 1322ps (+134ps): neg.194: bits[32] = neg(concat.191: bits[32], pos=0,23,15) <snip> 154ps (+ 15ps): and.133: bits[32] = and(neg.131: bits[32], literal.19: bits[32], pos=0,24,33) 139ps (+134ps): neg.131: bits[32] = neg(concat.219: bits[32], pos=0,23,15) 5ps (+ 0ps): concat.219: bits[32] = concat(literal.297: bits[31], bit_slice.214: bits[1], pos=0,23,21) 5ps (+ 0ps): bit_slice.214: bits[1] = bit_slice(not.208: bits[8], start=0, width=1, pos=0,23,21) 5ps (+ 5ps): not.208: bits[8] = not(message: bits[8], pos=0,20,16) <snip> In addition to the critical path, the cycle-by-cycle breakdown of which operations have been scheduled is provided in stdout.","title":"Delay Estimation"},{"location":"delay_estimation/#delay-estimation-methodology","text":"","title":"Delay Estimation Methodology"},{"location":"delay_estimation/#context","text":"This doc describes the delay estimation methodology used in XLS and related background. Estimating delays through networks of CMOS gates is a rich topic, and Static Timing Analysis (STA) is used in chip backend flows to ensure that, even for parts operating at the tail end of the distribution, chips continue to function as specified logically in the netlist. In stark contrast to something with very concrete constraints, like a \"post-global routing, high-accuracy parasitics static timing analysis\", the HLS tool needs to estimate at a high level, reasonably near to the user's behavioral specification, what delay HLS operations will have when they are \"stacked on top\" of each other in a data dependent fashion in the design. This information lets the HLS tool schedule operations into cycles without violating timing constraints; e.g. if the user specifies 1GHz (= 1000ps) as their target clock frequency, the HLS tool may choose to pack as much data dependent work into the 1000ps budget (minus clock uncertainty) as it can on any given cycle. Although what we're estimating is a close relative of Static Timing Analysis, the fact it's being analyzed at a very high level presents a different set of tradeoffs, where coarse granularity estimation and conservative bounds are more relevant. Fine-grained / precise analyses are used later in the chip design process, in the backend flows, but the HLS tool acts more like an RTL designer, creating an RTL output that early timing analysis deems acceptable. Notably, we don't want to be too conservative, as being conservative on timing can lead to designs that take more area and consume more power, as the flops introduced by additional pipeline stages are significant. We want to be as accurate as possible while providing a good experience of being able to close timing quickly (say, in a single pass, or with a small number of iterations in case of a pathological design).","title":"Context"},{"location":"delay_estimation/#background","text":"","title":"Background"},{"location":"delay_estimation/#what-xls-is-compiling","text":"XLS currently supports feed-forward pipelines -- once the user program is unrolled into a big \"sea of nodes\" we must schedule each of those operations (represented by nodes) to occur in a cycle. It seems clear that an operation like add(bits[32], bits[32]) -> bits[32] takes some amount of time to produce a result -- we need to be able to determine what that amount of time is for packing that operation into a given cycle. 1 Note that XLS operations are parametric in their bitwidth, so add(bits[17], bits[17]) -> bits[17] is just as possible as a value like 32 . This ain't C code. 1 : Note that we currently pack operations into cycles atomically -- that is, we don't break an add that would straddle a cycle boundary into add.first_half and add.second_half automatically to pull things as early as possible in the pipeline, but this is future work of interest. Ideally operations would be described structurally in a way that could automatically be cut up according to available delay budget. This would also permit operations in the IR that take more than a single cycle to produce a value (currently they would have to be \"legalized\" into operations that fit within a cycle, but that is not yet done, the user will simply receive a scheduling error). Some operations, such as bit_slice or concat are just wiring \"feng shui\"; however, they still have some relevance for delay calculations! Say we concatenate a value with zeros for zero extension. Even if we could schedule that in \"cycle 0\", if the consumer can only be placed in \"cycle 1\", we would want to \"sink\" the concat down into \"cycle 1\" as well to avoid unnecessary registers being materialized sending the zero values from \"cycle 0\".","title":"What XLS is compiling"},{"location":"delay_estimation/#the-delay-problem","text":"Separately from XLS considerations, there are fundamental considerations in calculating the delay through clouds of functional logic in (generated) RTL. Between each launching and capturing flop is a functional network of logic gates, implemented with standard cells in our ASIC process flows. Chip designs target a particular clock frequency as their operating point, and the functional network has to produce its output value with a delay that meets the timing constraint of the clock frequency. The RTL designer typically has to iterate their design until: timing path delay <= target clock period - clock uncertainty For all timing paths in their design, where clock uncertainty includes setup/hold time constraints, and slop that's built in as margin for later sources of timing variability (like instantiating a clock tree, which can skew the clock signal observed by different flops). In a reasonable model, gate delay is affected by a small handful of properties, as reflected in the \"(Method of) Logical Effort\" book: The transistor network used to implement a logic function (AKA logical effort): on an input pin change, the gate of each transistor must be driven to a point it recognizes whether a 0 or 1 voltage is being presented. More gates to drive, or larger gates, means more work for the driver. The load being driven by the logic function (AKA electrical effort): fanning out to more gates generally means more work to drive them all to their threshold voltages. Being loaded down by bigger gates means more work to drive it to its threshold voltage. Parasitic delays: RC elements in the system that leech useful work, typically in a smaller way compared to the efforts listed above. The logical effort book describes a way to analyze the delays through a network of gates to find the minimal delay, and size transistors in a way that can achieve that minimal delay (namely by geometrically smoothing the ability for gate to drive capacitance). Confounding factors include: Medium/large wires: sizing transistors to smooth capacitance becomes difficult as fixed-capacitance elements (wires) are introduced. It seems that small wires have low enough capacitance they can generally be treated as parasitic. Divergence/reconvergence in the functional logic network (as a DAG). Different numbers of logic levels and different drive currents may be presented from different branches of a fork/join the logic graph, which forces delay analysis into a system of equations to attempt to minimize the overall delay, as observed by the critical path, with transistor sizing and gate choices. (Some simplifications are possible, like buffering non-critical paths until they have the same number of logic levels so they also have plenty of current to supply at join points.) Somewhat orthogonal to the analytical modeling problem, there are also several industry standards for supplying process information to Static Timing Analysis engines for determining delay through a netlist. This information is often given in interpolated tables for each standard cell, for example in the NLDM model describing how delay changes as a function of input transition time and load (load capacitance). These models and supplied pieces of data are important to keep in mind for contrast, as we now ignore it all and do something very simple.","title":"The delay problem"},{"location":"delay_estimation/#simple-delay-estimation","text":"Currently, XLS delay estimation follows a conceptually simple procedure: For every operation in XLS (e.g. binary addition): * For some relevant-seeming set of bitwidths; e.g. {2, 4, 8, 16, ..., 2048} * Find the maximum frequency at which that operation closes timing at that bitwidth, in 100MHz units as determined by the synthesis tool. 2 Call the clock period for this frequency t_best . (Note that we currently just use a single process corner / voltage for this sweep.) * Subtract the clock uncertainty from t_best . * Record that value in a table (with the keys of the table being operation / bitwidth). 2 : The timing report can provide the delay through a path at any clock frequency, but a wrinkle is that synthesis tools potentially only start using their more aggressive techniques as you bump up against the failure-to-close-timing point -- there it'll be more likely to change the structure of the design to make it more delay friendly. The sweep helps to try to cajole it in that way. Inspecting the data acquired in this way we observe all of the plots consist of one or more the following delay components: Constant as a function of bitwidth for a given op (e.g. binary-or just requires a single gate for each bit regardless of the width of the inputs). Logarithmic as a function of bitwidth (e.g. adders may end up using tree-like structures to minimize delay, single-selector muxes end up using a tree to fan out the selector to the muxes, etc.). Linear as a function of bitwidth (e.g., ripple-carry adders and some components of multipliers). So given this observation we fit a curve of the form: a * bitwidth + b * log_2(bitwidth) + c to the sweep data for each operation, giving us (a, b, c) values to use in our XLS delay estimator. The utility delay_model_visualizer under the tools directory renders a graph of the delay model estimate against the measured data points. This graph for add shows a good correspondence to the measured delay.","title":"Simple Delay Estimation"},{"location":"delay_estimation/#sweeping-multiple-dimensions","text":"Operations with attributes in addition to bitwidth that affect delay are swept across multiple dimensions. An example is Op::kOneHotSelect which has dimensions of bitwidth and number of cases. For the Op::kOneHotSelect example the formula is: a * bitwidth + b * log_2(bitwidth) + c * (# of cases) + d * log_2(# of cases) + c Below is plot of delay for Op::kOneHotSelect showing the two dimensions of bitwidth and operand count affecting delay:","title":"Sweeping multiple dimensions"},{"location":"delay_estimation/#sources-of-pessimismoptimism","text":"This simple method has both sources of optimism and pessimism, though we hope to employ a method that will be generally conservative, so that users can easily close timing and get a close-to-best-achievable result (say, within tens of percent) with a single HLS iteration. Sources of pessimism (estimate is conservative): The operation sweeps mentioned are not bisecting to the picosecond, so there is inherent slop in the measurement on account of sweep granularity. We expect, in cycles where multiple dependent operations are present, there would be \"K-map style\" logic reductions with adjacent operations. For example, because we don't do cell library mapping in XLS delay estimation, something a user wrote that mapped to an AOI21 cell would be the sum of (and+or+invert) delays. [Unsure] May there be additional logic branch splitting options and earlier-produced results available to the synthesis tool when there are more operations in the graph (vs a lone critical path measured for a single operation)? Sources of optimism (estimate is overeager): For purposes of the sweep the outputs of an operation are only loaded by a single capture flop flop -- when operations have fanout the delay will increase. Note that we do account for fanout within individual operations as part of this sweep; e.g. a 128-bit selector fanout (e.g. 128 ways to 128 muxes) for a select is accounted for the in delay timing of the select operation. It is the output logic layer that is only loaded by a single flop in our characterization. Notably because most of these operations turn into trees of logic, there are $$log_2(bitcount)$$ layers of logic in which we can potentially smoothly increase drive strength out to the output logic layer, and paths can presumably be replicated by synthesis tools to reduce pointwise fanout when multiple high-level operations are data-dependent within a cycle. (Is it possible for a user to break up their 32-bit select into bitwise pieces in their XLS code to mess with our modeling? Sure, but probably not too expected, so we're currently sort of relying on the notion people are using high level operations instead of compodecomposing them into bitwise pieces in our graph.) A potential way to reconcile this output fanout in the future is to do a delay sweep with a high-capacitive fanout (e.g. four flops of load) and then ensure the IR has a maximum fanout of four for our delay estimation. Wiring delay / load / congestion / length are not estimated. This will need additional analysis / refinement as we run XLS through synthesis tools with advanced timing analysis, as it is certainly not viable for arbitrary designs (tight pipelines may be ok for now, though).","title":"Sources of pessimism/optimism"},{"location":"delay_estimation/#iterative-refinement","text":"The caveats mentioned above seem somewhat daunting, but this first cut approach appears to work comfortably at target frequenties, in practice, for the real-world blocks being designed as XLS \"first samples\". Notably, human RTL designers fail to close timing on a first cut as well -- HLS estimations like the above assist in getting a numeric understanding (in lieu of an intuitive guess) of something that may close timing on a first cut. As this early model fails, we will continue to refine it; however, there is also a secondary procedure that can assist as the model improves. Let's call the delay estimation described above applied to a program a prediction of its delays. Let's call the first prediction we make p0 : p0 will either meet timing or fail to meet timing. When we meet timing with p0 , there may be additional wins left on the table. If we're willing to put synthesis tool runs \"in the loop\" (say running a \"tuner\" overnight), we can refine XLS's estimates according to the realities of the current program, and, for example, try to squeeze as much as possible into as few cycles as possible if near-optimality of latency/area/power were a large consideration. This loop would generate p1 , p2 , ... as it refined its model according to empirical data observed from the synthesis tool's more refined analysis. When we fail to close timing with p0 , we can feed back the negative slack delays for comparison with our estimates and relax estimates accordingly. Additionally, an \"aggression\" knob could be implemented that backs off delay estimations geometrically (say via a \"fudge factor\" coefficient) in order to ensure HLS developer time is not wasted unnecessarily to ensure. Once a combination of these mechanisms has obtained a design that closes timing, the \"meeting timing, now refine\" procedure can be employed as described above.","title":"Iterative refinement"},{"location":"delay_estimation/#on-hints","text":"To whatever extent possible, XLS should be the tool that reasons about how to target the backend (vs having a tool that sits on top of it and messes with XLS' input in an attempt to achieve a result). User-facing hint systems are typically very fragile, owing to the fact they don't have easily obeyed semantics. XLS, by contrast, knows about its own internals, so can do things with awareness of what's happened upstream and what remains to happen downstream. By contrast, we should continue to add ways for users to provide more semantic information / intent as part of their program (e.g. via more high-level patterns that make high level structure more clear), and make XLS smarter about how to lower those constructs into hardware (and why it should be lowering them that way) in the face of some prioritized objectives (power/area/latency). That being said, because we're probably trying to produce hardware at a given point in time against a given technology, it likely makes sense to permit human users to specify things directly (at a point in time), even if those specifications might be ignored / handled very differently in the future or against different technology nodes. This would be the moral equivalent of what existing EDA tools do as a \"one-off TCL file\" used in a particular design, vs something carried from design to design. Recall, though, that the intent of XLS is to make things easier to carry from design to design and require fewer one-off modifications!","title":"On Hints"},{"location":"delay_estimation/#tools","text":"XLS provides tools for analyzing its delay estimation model. (Note that the given IR should be in a form suitable for code generation; e.g. it has run through the opt_main binary). $ tools/benchmark_main crc32.opt.ir --clock_period_ps=500 <snip> Return value delay: 1362ps Critical path entry count: 42 Critical path: 1362ps (+ 5ps): not.29: bits[32] = not(xor.205: bits[32], pos=0,29,50) 1357ps (+ 20ps): xor.205: bits[32] = xor(concat.195: bits[32], and.196: bits[32], pos=0,24,19) 1337ps (+ 15ps): and.196: bits[32] = and(neg.194: bits[32], literal.304: bits[32], pos=0,24,33) 1322ps (+134ps): neg.194: bits[32] = neg(concat.191: bits[32], pos=0,23,15) <snip> 154ps (+ 15ps): and.133: bits[32] = and(neg.131: bits[32], literal.19: bits[32], pos=0,24,33) 139ps (+134ps): neg.131: bits[32] = neg(concat.219: bits[32], pos=0,23,15) 5ps (+ 0ps): concat.219: bits[32] = concat(literal.297: bits[31], bit_slice.214: bits[1], pos=0,23,21) 5ps (+ 0ps): bit_slice.214: bits[1] = bit_slice(not.208: bits[8], start=0, width=1, pos=0,23,21) 5ps (+ 5ps): not.208: bits[8] = not(message: bits[8], pos=0,20,16) <snip> In addition to the critical path, the cycle-by-cycle breakdown of which operations have been scheduled is provided in stdout.","title":"Tools"},{"location":"dslx_intro_example1/","text":"DSLX Example: Compute CRC32 Checksum In this document we explain in detail the implementation of routine to compute a CRC32 checksum on a single 8-bit input. We don't discuss the algorithm here, only the language features necessary to implement the algorithm. Refer to the full implementation, in examples/dslx_intro/crc32_one_byte.x , while following this document. Function Prototype Let's explain how the function is being defined: fn crc32_one_byte(byte: u8, polynomial: u32, crc: u32) -> u32 { Functions are defined starting with the keyword fn , followed by the function's name, crc32_one_byte in this case. Then comes the list of parameters, followed by the declaration of the return type. This function accepts 3 parameters: byte , which is of type u8 . u8 is a shortcut for bits[8] polynomial , which is of type u32 , a 32-bit type. crc , which is also of type u32 The return type, which is declared after the -> , is also a u32 . The first line of the function's body is quite curious: let crc: u32 = crc ^ u32:byte; The expression to the right side of the = is easy to understand, it computes the xor operation between the incoming parameters crc and byte , which has been cast to a u32 . The let expression re-binds crc to the expression on the right. This looks like a classic variable assignment. However, since these expressions are all scoped by the let expression, the newly assigned crc values are different and distinguishable from their previous values. In other words, the original value of crc is not visible to anybody else after the re-binding. The next line specifies a for loop. Index variable and accumulator are i and crc , both of type u32 . The iterable range expression specifies that the loop should execute 8 times. // 8 rounds of updates. for (i, crc): (u32, u32) in range(u32:8) { At the end of the loop, the calculated value is being assigned to the accumulator crc - the last expression in the loop body is assigned to the accumulator: let mask: u32 = -(crc & u32:1); (crc >> u32:1) ^ (polynomial & mask) Finally, the accumulator's initial value is being passed to the for expression as a parameter. This can be confusing, especially when compared to other languages, where the init value typically is provided at or near the top of a loop. }(crc)","title":"CRC32"},{"location":"dslx_intro_example1/#dslx-example-compute-crc32-checksum","text":"In this document we explain in detail the implementation of routine to compute a CRC32 checksum on a single 8-bit input. We don't discuss the algorithm here, only the language features necessary to implement the algorithm. Refer to the full implementation, in examples/dslx_intro/crc32_one_byte.x , while following this document.","title":"DSLX Example: Compute CRC32 Checksum"},{"location":"dslx_intro_example1/#function-prototype","text":"Let's explain how the function is being defined: fn crc32_one_byte(byte: u8, polynomial: u32, crc: u32) -> u32 { Functions are defined starting with the keyword fn , followed by the function's name, crc32_one_byte in this case. Then comes the list of parameters, followed by the declaration of the return type. This function accepts 3 parameters: byte , which is of type u8 . u8 is a shortcut for bits[8] polynomial , which is of type u32 , a 32-bit type. crc , which is also of type u32 The return type, which is declared after the -> , is also a u32 . The first line of the function's body is quite curious: let crc: u32 = crc ^ u32:byte; The expression to the right side of the = is easy to understand, it computes the xor operation between the incoming parameters crc and byte , which has been cast to a u32 . The let expression re-binds crc to the expression on the right. This looks like a classic variable assignment. However, since these expressions are all scoped by the let expression, the newly assigned crc values are different and distinguishable from their previous values. In other words, the original value of crc is not visible to anybody else after the re-binding. The next line specifies a for loop. Index variable and accumulator are i and crc , both of type u32 . The iterable range expression specifies that the loop should execute 8 times. // 8 rounds of updates. for (i, crc): (u32, u32) in range(u32:8) { At the end of the loop, the calculated value is being assigned to the accumulator crc - the last expression in the loop body is assigned to the accumulator: let mask: u32 = -(crc & u32:1); (crc >> u32:1) ^ (polynomial & mask) Finally, the accumulator's initial value is being passed to the for expression as a parameter. This can be confusing, especially when compared to other languages, where the init value typically is provided at or near the top of a loop. }(crc)","title":"Function Prototype"},{"location":"dslx_intro_example3/","text":"DSLX Example: Prefix Scan Computation In this document we explain in detail the implementation of a 8 byte prefix scan computation. In order to understand the implementation, it is useful to understand the intended functionality first. For a given input of 8 bytes, the scan iterates from left to right over the input and produces an output of the same size. Each element in the output contains the count of duplicate values seen so far in the input. The counter resets to 0 if a new value is found. For example, for this input: let input = bits[8,32]:[0, 0, 0, 0, 0, 0, 0, 0] the code should produce this output: bits[8,3]:[0, 1, 2, 3, 4, 5, 6, 7]) At index 0 it has not yet found any value, so it assigns a counter value of 0 . At index 1 it finds the second occurrence of the value '0' (which is the 1st duplicate) and therefore adds a 1 to the counter from index 0. At index 2 it finds the third occurrence of the value '0' (which is the 2nd duplicate) and therefore adds a 1 to the counter from index 1. And so on. Correspondingly, for this input: let input = bits[8,32]:[0, 0, 1, 1, 2, 2, 3, 3] it should produce: assert_eq(result, bits[8,3]:[0, 1, 0, 1, 0, 1, 0, 1]) The full listing is in examples/dslx_intro/prefix_scan_equality.x . Function prefix_scan_eq The implementation displays a few interesting language features. The function prototype is straigt-forward. Input is an array of 8 values of type u32 . Output is an array of size 8 holding 3-bit values (the maximum resulting count can only be 7, which fits in 3 bits). fn prefix_scan_eq(x: u32[8]) -> bits[8,3] { The first let expression produces a tuple of 3 values. It only cares about the last value result , so it stubs out the other two elements via the 'ignore' placeholder _ . let (_, _, result) = Why a 3-Tuple? Because he following loop has tuple of three values as the accumulator. The return type of the loop is the type of the accumulator, so above let needs to be of the same type. Enumerated Loop Using tuples as the accumulator is a convenient way to model multiple loop-carried values: for ((i, elem), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) in enumerate(x) { The iterable of this loop is enumerate(x) . On each iteration, this construct delivers a tuple consisting of current index and current element. This is represented as the tuple (i, elem) in the for construct. The loop next specifies the accumulator, which is a 3-tuple consisting of the values named prior , count , and result . The types of the iterable and accumulator are specified next. The iterable is a tuple consisting of two u32 values. The accumulator is more interesting, it is a tuple consiting of a u32 value ( prior ), a u3 value ( count ), and a 2-dimension array type bits[8, 3] , which is an array holding 8 elements of bit-width 3. This is the type of result in the accumulator. Looping back to the prior let statement, it ignores the prior and count members of the tuple and will only return the result part. A Match Expression The next expression is an interesting match expression. The let expression binds the tuple (to_place, new_count): (u3, u3) to the result of the following match expression: let (to_place, new_count): (u3, u3) = match (i == u32:0, prior == elem) { to_place will hold the value that is to be written at a given index. new_count will contain the updated counter value. The match expression evaluates two conditions in parallel: is i == 0? is the prior element the same as the current elem Two tests mean there are four possible cases, which are all handled in the following four cases: // i == 0 (no matter whether prior == elem or not): // we set position 0 to 0 and update the new_counter to 1 (true, true) => (u3:0, u3:1); (true, false) => (u3:0, u3:1); // if i != 0 - if the current element is the same as pior, // set to_place to the value of the current count // update new_counter with the increased counter value (false, true) => (count, count + u3:1); // if i != 0 - if current element is different from prior, // set to_place back to 0 // set new_counter back to 1 (false, false) => (u3:0, u3:1); }; To update the result, we set index i in the result array to the value to_place via the built-in update function, which produces a new value new_result ): let new_result: bits[8,3] = update(result, i, to_place); Finally the updated accumulator value is constructed, it is the last expression in the loop: (elem, new_count, new_result) Following the loop body, as an argument to the loop, we initialize the accumulator in the following way. set element prior to -1, in order to not match any other value. set element count to 0. set element result to 8 0's of size u3 }((u32:-1, u3:0, bits[8,3]:[u3:0, u3:0, u3:0, u3:0, u3:0, u3:0, u3:0, u3:0])); And, finally, the function simply returns result : result } Testing To test the two cases we've described above, we add the following two test cases right to this implementation file: test prefix_scan_eq_all_zero { let input = bits[8,32]:[0, 0, 0, 0, 0, 0, 0, 0]; let result = prefix_scan_eq(input); assert_eq(result, bits[8,3]:[0, 1, 2, 3, 4, 5, 6, 7]) } test prefix_scan_eq_doubles { let input = bits[8,32]:[0, 0, 1, 1, 2, 2, 3, 3]; let result = prefix_scan_eq(input); assert_eq(result, bits[8,3]:[0, 1, 0, 1, 0, 1, 0, 1]) }","title":"Prefix Scan"},{"location":"dslx_intro_example3/#dslx-example-prefix-scan-computation","text":"In this document we explain in detail the implementation of a 8 byte prefix scan computation. In order to understand the implementation, it is useful to understand the intended functionality first. For a given input of 8 bytes, the scan iterates from left to right over the input and produces an output of the same size. Each element in the output contains the count of duplicate values seen so far in the input. The counter resets to 0 if a new value is found. For example, for this input: let input = bits[8,32]:[0, 0, 0, 0, 0, 0, 0, 0] the code should produce this output: bits[8,3]:[0, 1, 2, 3, 4, 5, 6, 7]) At index 0 it has not yet found any value, so it assigns a counter value of 0 . At index 1 it finds the second occurrence of the value '0' (which is the 1st duplicate) and therefore adds a 1 to the counter from index 0. At index 2 it finds the third occurrence of the value '0' (which is the 2nd duplicate) and therefore adds a 1 to the counter from index 1. And so on. Correspondingly, for this input: let input = bits[8,32]:[0, 0, 1, 1, 2, 2, 3, 3] it should produce: assert_eq(result, bits[8,3]:[0, 1, 0, 1, 0, 1, 0, 1]) The full listing is in examples/dslx_intro/prefix_scan_equality.x .","title":"DSLX Example: Prefix Scan Computation"},{"location":"dslx_intro_example3/#function-prefix_scan_eq","text":"The implementation displays a few interesting language features. The function prototype is straigt-forward. Input is an array of 8 values of type u32 . Output is an array of size 8 holding 3-bit values (the maximum resulting count can only be 7, which fits in 3 bits). fn prefix_scan_eq(x: u32[8]) -> bits[8,3] { The first let expression produces a tuple of 3 values. It only cares about the last value result , so it stubs out the other two elements via the 'ignore' placeholder _ . let (_, _, result) = Why a 3-Tuple? Because he following loop has tuple of three values as the accumulator. The return type of the loop is the type of the accumulator, so above let needs to be of the same type.","title":"Function prefix_scan_eq"},{"location":"dslx_intro_example3/#enumerated-loop","text":"Using tuples as the accumulator is a convenient way to model multiple loop-carried values: for ((i, elem), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) in enumerate(x) { The iterable of this loop is enumerate(x) . On each iteration, this construct delivers a tuple consisting of current index and current element. This is represented as the tuple (i, elem) in the for construct. The loop next specifies the accumulator, which is a 3-tuple consisting of the values named prior , count , and result . The types of the iterable and accumulator are specified next. The iterable is a tuple consisting of two u32 values. The accumulator is more interesting, it is a tuple consiting of a u32 value ( prior ), a u3 value ( count ), and a 2-dimension array type bits[8, 3] , which is an array holding 8 elements of bit-width 3. This is the type of result in the accumulator. Looping back to the prior let statement, it ignores the prior and count members of the tuple and will only return the result part.","title":"Enumerated Loop"},{"location":"dslx_intro_example3/#a-match-expression","text":"The next expression is an interesting match expression. The let expression binds the tuple (to_place, new_count): (u3, u3) to the result of the following match expression: let (to_place, new_count): (u3, u3) = match (i == u32:0, prior == elem) { to_place will hold the value that is to be written at a given index. new_count will contain the updated counter value. The match expression evaluates two conditions in parallel: is i == 0? is the prior element the same as the current elem Two tests mean there are four possible cases, which are all handled in the following four cases: // i == 0 (no matter whether prior == elem or not): // we set position 0 to 0 and update the new_counter to 1 (true, true) => (u3:0, u3:1); (true, false) => (u3:0, u3:1); // if i != 0 - if the current element is the same as pior, // set to_place to the value of the current count // update new_counter with the increased counter value (false, true) => (count, count + u3:1); // if i != 0 - if current element is different from prior, // set to_place back to 0 // set new_counter back to 1 (false, false) => (u3:0, u3:1); }; To update the result, we set index i in the result array to the value to_place via the built-in update function, which produces a new value new_result ): let new_result: bits[8,3] = update(result, i, to_place); Finally the updated accumulator value is constructed, it is the last expression in the loop: (elem, new_count, new_result) Following the loop body, as an argument to the loop, we initialize the accumulator in the following way. set element prior to -1, in order to not match any other value. set element count to 0. set element result to 8 0's of size u3 }((u32:-1, u3:0, bits[8,3]:[u3:0, u3:0, u3:0, u3:0, u3:0, u3:0, u3:0, u3:0])); And, finally, the function simply returns result : result }","title":"A Match Expression"},{"location":"dslx_intro_example3/#testing","text":"To test the two cases we've described above, we add the following two test cases right to this implementation file: test prefix_scan_eq_all_zero { let input = bits[8,32]:[0, 0, 0, 0, 0, 0, 0, 0]; let result = prefix_scan_eq(input); assert_eq(result, bits[8,3]:[0, 1, 2, 3, 4, 5, 6, 7]) } test prefix_scan_eq_doubles { let input = bits[8,32]:[0, 0, 1, 1, 2, 2, 3, 3]; let result = prefix_scan_eq(input); assert_eq(result, bits[8,3]:[0, 1, 0, 1, 0, 1, 0, 1]) }","title":"Testing"},{"location":"dslx_reference/","text":"DSLX Reference DSLX Reference Comments Identifiers Functions Parameters Parametric Functions Function Calls Types Bit Type Enum Types Tuple Type Struct Type Array Type Type Aliases Type Casting Type Checking and Inference Type Inference Details Operator Example Type errors Let Bindings, Names, and the Environment Expressions Unary Expressions Binary Expressions Comparison Expressions Concat Expression Match Expression let Expression Ternary If Expression Iterable Expression for Expression Numerical Conversions Array Conversions Advanced Understanding: Parametricity, Constraints, and Unification Statements Imports Public module members Typedefs Const Parallel Primitives map reduce group-by Builtins Bit Slicing clz, ctz signex rev Bitwise reductions update Testing and Debugging Unit Tests QuickCheck assert_eq trace fail!() Appendix Operator Precedence DSLX is a domain specific, functional language to build hardware that can also run effectively as host software. The DSL targets the XLS compiler (by conversion to XLS IR) to enable flows for FPGAs and ASICs (note that other frontends will become available in the future). DSLX mimics Rust, while being an immutable expression-based dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph, etc. To avoid arbitrary new syntax/semantics choices the DSL mimics Rust where it is reasonably possible; for example, integer conversions all follow the same semantics as Rust. Note: There are some unnecessary differences today from Rust syntax due to early experimentation, but they are quickly being removed to converge on Rust syntax. Dataflow DSLs are a good fit for describing hardware, compared to languages designed assume von Neumann style computation (global mutable state, sequential mutation by a sequential thread of control). Using a DSL provides a more hardware-oriented representation of a given computation that matches XLS compiler (IR) constructs closely. The DSL also allows an exploration of HLS without being encumbered by C++ language or compiler limitations such as non-portable pragmas, magic macros, or semantically important syntactic conventions. The language is still experimental and likely to change, but it is already useful for experimentation and exploration. This document provides a reference for DSLX, mostly by example. After perusing it and learning about the language features, we recommend exploring the following, detailed examples to learn how the language features are put to action: CRC32 Floating-point addition Prefix Sum Computation In this document we use the function to compute a CRC32 checksum to describe language features. The full code is in examples/dslx_intro/crc32_one_byte.x . Comments Just as in languages like C/C++, comments start with // and last through the end of the line. Identifiers All identifiers, eg., for function names, parameters, and values, follow the typical naming rules of other languages. The identifiers can start with a character or an underscore, and can then contain more characters, underscores, or numbers. Valid examples are: a // valid CamelCase // valid like_under_scores // valid __also_ok // valid _Ok123_321 // valid _ // valid 2ab // not valid &ade // not valid However, we suggest the following DSLX style rules : Functions are written_like_this User-defined data types are NamesLikeThis Constant bindings are NAMES_LIKE_THIS _ is the black-hole identifier, as in Python. It should never be used in a binding-reference. Functions Function definitions begin with the keyword fn , followed by the function name, a parameter list to the function in parenthesis, followed by an -> and the return type of the function. After this, curly braces denote the begin and end of the function body. The list of parameters can be empty. A single input file can contain many functions. Simple examples: fn ret3() -> u32 { 3 // this function always returns 3. } fn add1(x: u32) -> u32 { x + u32:1 // returns x + 1, but you knew that. } Functions return the result of their last computed expression as their return value. There are no explicit return statements. Functions also don't have multiple return statements. Tuples should be returned if a function should return multiple values. Parameters Parameters are written as pairs name followed by a colon : followed by the type of that parameter. Each parameter needs to declare its own type. Examples: // a simple parameter x of type u32 x: u32 // t is a tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 t: (u32, (u8, (u16,), u8)) Parametric Functions DSLX functions can be parameterized in terms of the types of its arguments and in terms of types derived from other parametric values. For instance: fn double(n: bits[32]) -> bits[32] { n * bits[32]:2 } fn [A: u32, B: u32 = double(A)] self_append(x: bits[A]) -> bits[B] { x++x } fn main() -> bits[10] { self_append(bits[5]:1) } In self_append(bits[5]:1) , we see that A = 5 based off of formal argument instantiation. Using that value, we can evaluate B = double(A=5) . This derived expression is analogous to C++'s constexpr \u2013 a simple expression that can be evaluated at that point in compilation. See advanced understanding for more information on parametricity. Function Calls Function calls are expressions and look and feel just like one would expect from other languages. For example: fn callee(x: bits[32], y: bits[32]) -> bits[32] { x + y } fn caller() -> u32 { callee(u32:2, u32:3) } If more than one value should be returned by a function, a tuple type should be returned. Types Bit Type The most fundamental type in DSLX is a variable length bit type denoted as bits[n] , where n is a constant. For example: bits[0] // possible, but, don't do that bits[1] // a single bit uN[1] // explicitly noting single bit is unsigned u1 // convenient shorthand for bits[1] bits[8] // an 8-bit datatype, yes, a byte u8 // convenient shorthand for bits[8] bits[32] // a 32-bit datatype u32 // convenient shorthand for bits[32] bits[256] // a 256-bit datatype DSLX introduces shortcuts for commonly used types, such as u8 for an 8-wide bit type, or u32 for a 32-bit wide bit type. These are defined up to u64 . All u* and bits[*] types are interpreted as unsigned numbers. Signed numbers are specified via sN[*] , which is analogous to bits[*] and uN[*] which are the corresponding unsigned version of the bit width * . For example: sN[0] s0 sN[1] s1 sN[64] s64 sN[256] Similarly to unsigned numbers, the s* shorthands are defined up to s64 . Signed numbers differ in their behavior from unsigned numbers primarily via operations like comparisons, (variable width) multiplications, and divisions. Enum Types DSLX supports enumerations as a way of defining a group of related, scoped, named constants that do not pollute the module namespace. For example: enum Opcode : u3 { FIRE_THE_MISSILES = 0, BE_TIRED = 1, TAKE_A_NAP = 2, } fn get_my_favorite_opcode() -> Opcode { Opcode::FIRE_THE_MISSILES } Note the use of the double-colon to reference the enum value. This code specifies that the enum behaves like a u3 : its storage and extension (via casting) behavior are defined to be those of a u3 . Attempts to define an enum value outside of the representable u3 range will produce a compile time error. enum Opcode : u3 { FOO = 8 // Causes compile time error! } Enums can be compared for equality/inequality, but they do not permit arithmetic operations, they must be cast to numerical types in order to perform arithmetic: fn same_opcode(x: Opcode, y: Opcode) -> bool { x == y // ok } fn next_in_sequence(x: Opcode, y: Opcode) -> bool { // x+1 == y // does not work, arithmetic! u3:x + u3:1 == u3:y // ok, casted first } As mentioned above casting of enum-values works with the same casting/extension rules that apply to the underlying enum type definition. For example, this cast will sign extend because the source type for the enum is signed. (See numerical conversions for the full description of extension/truncation behavior.) enum MySignedEnum : s3 { LOW = -1 ZERO = 0 HIGH = 1 } fn extend_to_32b(x: MySignedEnum) -> u32 { u32:x // Sign-extends because the source type is signed. } test extend_to_32b { assert_eq(extend_to_32b(MySignedEnum::LOW), u32:-1) } Casting to an enum is also permitted. However, in most cases errors from invalid casting can only be found at runtime, e.g., in the DSL interpreter or flagging a fatal error from hardware. Because of that, it is recommended to avoid such casts as much as possible. Tuple Type A tuple is an ordered set of fixed size containing elements of potentially different types. Tuples can contain bits, arrays, or other tuples. Examples: (0b100, 0b101) a tuple containing two bits elements // A tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 (u32, (u8, (u16,), u8) To access individual tuple elements use simple indices, starting at 0 (Member access by field names is work in progress). For example, to access the 2nd element of a tuple (index 1): let t = (u32:2, u8:3); assert_eq(u8:3, t[u32:1]) Another way to \"destructure\" a tuple into names is to use tuple assignment: let t = (u32:2, u8:3); let (a, b) = t; let _ = assert_eq(u32:2, a); assert_eq(u8:3, b) Just as values can be discarded in a let by using the \"black hole identifier\" _ , don't care values can also be discarded when destructuring a tuple: let t = (u32:2, u8:3, true); let (_, _, v) = t; assert_eq(v, true) Struct Type Structs are \"sugar\" on top of tuples that give names to the various slots and have convenient ways of constructing / accessing the members. The following syntax is used to define a struct: struct Point { x: u32, y: u32 } Once a struct is defined it can be constructed by naming the fields in any order: struct Point { x: u32, y: u32, } test struct_equality { let p0 = Point { x: u32:42, y: u32:64 }; let p1 = Point { y: u32:64, x: u32:42 }; assert_eq(p0, p1) } There is a simple syntax when defining fields with names that are the same as the specified values: ``` struct Point { x: u32, y: u32, } test struct_equality { let x = u32:42; let y = u32:64; let p0 = Point { x, y }; let p1 = Point { y, x }; assert_eq(p0, p1) } ``` Struct fields can also be accessed with \"dot\" syntax: struct Point { x: u32, y: u32, } fn f(p: Point) -> u32 { p.x + p.y } fn main() -> u32 { f(Point { x: u32:42, y: u32:64 }) } test main { assert_eq(u32:106, main()) } Note that structs cannot be mutated \"in place\", the user must construct new values by extracting the fields of the original struct mixed together with new field values, as in the following: struct Point3 { x: u32, y: u32, z: u32, } fn update_y(p: Point3, new_y: u32) -> Point3 { Point3 { x: p.x, y: new_y, z: p.z } } fn main() -> Point3 { let p = Point3 { x: u32:42, y: u32:64, z: u32:256 }; update_y(p, u32:128) } test main { let want = Point3 { x: u32:42, y: u32:128, z: u32:256 }; assert_eq(want, main()) } The DSL has syntax for conveniently producing a new value with a subset of fields updated to \"feel\" like the convenience of mutation. The \"struct update\" syntax is: fn update_y(p: Point3) -> Point3 { Point3 { y: u32:42, ..p } } fn update_x_and_y(p: Point3) -> Point3 { Point3 { x: u32:42, y: u32:42, ..p } } Note that structs are not compatible with other structs that happen to have the same definition (this is called \"nominal typing\"). For example: def test_nominal_typing(self): # Nominal typing not structural, e.g. OtherPoint cannot be passed where we # want a Point, even though their members are the same. self._typecheck( \"\"\" struct Point { x: s8, y: u32, } struct OtherPoint { x: s8, y: u32 } fn f(x: Point) -> Point { x } fn g() -> Point { let shp = OtherPoint { x: s8:255, y: u32:1024 }; f(shp) } \"\"\", error='parameter type name: \\'Point\\'; argument type name: \\'OtherPoint\\'' ) DSLX also supports parametric structs. For more information on how type-parametricity works, see the parametric functions section. ``` fn double(n: u32) -> u32 { n * u32:2 } struct [N: u32, M: u32 = double(N)] Point { x: bits[N], y: bits[M], } fn [A: u32, B: u32] make_point(x: bits[A], y: bits[B]) -> Point[A, B] { Point { x, y } } test struct_construction { let p = make_point(u16:42, u32:42); assert_eq(u16:42, p.x) } ``` Array Type Arrays can be constructed via bracket notation. All values that make up the array must have the same type. Arrays can be indexed with indexing notation ( a[i] ) to retrieve a single element. fn main(a: u32[2], i: u1) -> u32 { a[i] } test main { let x = u32:42; let y = u32:64; // Make an array with \"bracket notation\". let my_array: u32[2] = [x, y]; let _ = assert_eq(main(my_array, u1:0), x); let _ = assert_eq(main(my_array, u1:1), y); () } Because arrays with repeated trailing elements are common, the DSL supports ellipsis ( ... ) at the end of an array to fill the remainder of the array with the last noted element. Because the compiler must know how many elements to fill, in order to use the ellipsis the type must be annotated explicitly as shown. fn make_array(x: u32) -> u32[3] { u32[3]:[u32:42, x, ...] } test make_array { let _ = assert_eq(u32[3]:[u32:42, u32:42, u32:42], make_array(u32:42)); let _ = assert_eq(u32[3]:[u32:42, u32:64, u32:64], make_array(u32:64)); () } TODO(meheff): Explain arrays and the intricacies of our bits type interpretation and how it affects arrays of bits etc. Type Aliases DLSX supports the definition of type aliases. For example, to define a tuple type to represent a float number with a sign bit, an 8-bit mantissa, and a 23-bit mantissa, one would write: type F32 = { u1, u8, u23, } After this definition, the F32 may be used as a type annotation interchangeably with (u1, u8, u23) . Note, however, that structs are generally preferred as described above, as they are more readable and users do not need to rely on tuple indices remaining the same in the future. For direct type aliasing, users can use the following coding style (similar to the C++ typedef keyword): type TypeWeight = u6; Type Casting Bit types can be cast from one bit-width to another. To cast a given value to a different bit-width, the required code looks similar to how other types are being specified in DSLX: Simply prefix the value with the desired type, followed by a : . In this example: fn add_with_carry(x: bits[24], y: bits[24]) -> (u1, bits[24]) { let result = (u1:0 ++ x) + (u1:0 ++ y); (u1:(result >> bits[25]:24), bits[24]:result) } The type of result is inferred to be of type bits[25] (concatenation of a 24-bit value with an additional 1-bit value in the front). The expression bits[25]:24 specifies the type of the value 24 to be of bits[25] . The larger expression (u1:(result >> bits[25]:24) casts the result of the shift expression to a u1 . Only a single bit is returned, the least significant bit (the right-most bit). The expression bits[24]:result casts the 25-bit value 'result' down to a 24-bit value, basically chopping off the leading bit. Type Checking and Inference DSLX performs type checking and produces an error if types in an expression don't match up. let expressions also perform type inference, which is quite convenient. For example, instead of writing: let ch: u32 = (e & f) ^ ((!e) & g); let (h, g, f): (u32, u32, u32) = (g, f, e); one can write the following, as long as the types can be properly inferred: let ch = (e & f) ^ ((!e) & g); let (h, g, f) = (g, f, e); Note that type annotations can still be added and be used for program understanding, as they they will be checked by DSLX. Type Inference Details DSLX uses deductive type inference to check the types present in the program. Deductive type inference is a set of (typically straight-forward) deduction rules: Hindley-Milner style deductive type inference determines the result type of a function with a rule that only observes the input types to that function. (Note that operators like '+' are just slightly special functions in that they have pre-defined special-syntax-rule names.) Operator Example For example, consider the binary (meaning takes two operands) / infix (meaning it syntactically is placed in the center of its operands) '+' operator. The simple deductive type inference rule for '+' is: (T, T) -> T Meaning that the left hand side operand to the '+' operator is of some type (call it T), the right hand side operand to the '+' operator must be of that same type, T, and the result of that operator is then (deduced) to be of the same type as its operands, T. Let's instantiate this rule in a function: fn add_wrapper(x: bits[2], y: bits[2]) -> bits[2] { x + y } This function wraps the '+' operator. It presents two arguments to the '+' operator and then checks that the annotated return type on add_wrapper matches the deduced type for the body of that function; that is, we ask the following question of the '+' operator (since the type of the operands must be known at the point the add is performed): (bits[2], bits[2]) -> ? To resolve the '?' the following procedure is being used: Pattern match the rule given above (T, T) -> T to determine the type T: the left hand side operand is bits[2] , called T. Check that the right hand side is also that same T, which it is: another bits[2] . Deduce that the result type is that same type T: bits[2] . That becomes the return type of the body of the function. Check that it is the same type as the annotated return type for the function, and it is! The function is annotated to return bits[2] , and the deduced type of the body is also bits[2] . Qed. Type errors A type error would occur in the following: fn add_wrapper(x: bits[2], y: bits[3]) -> bits[2] { x + y } Applying the type deduction rule for '+' finds an inconsistency. The left hand side operand has type bits[2] , called T, but the right hand side is bits[3] , which is not the same as T. Because the deductive type inference rule does not say what to do when the operand types are different, it results in a type error which is flagged at this point in the program. Let Bindings, Names, and the Environment All expressions in the language's expression grammar have a deductive type inference rule. The types must be known for inputs to an operator/function (otherwise there'd be a use-before-definition error) and every expression has a way to determine its type from its operand expressions. A more interesting deduction rule comes into view with \"let\" expressions, which are of the form: let $name: $annotated_type = $expr in $subexpr An example of this is: let x: u32 = u32:2 in x That's an expression which evaluates to the value '2' of type u32 . In a let expression like this, we say $name gets \"bound\" to a value of type $annotated_type . The let typecheck must both check that $expr is of type $annotated_type , as well as determine the type of $subexpr , which is the type of the overall \"let expression\". This leads to the deduction rule that \"let just returns the type of $subexpr \". But, in this example, the subexpr needs some information from the outer let expression, because if asked \"what's the type of some symbol y \" one immediately asks \"well what comes before that in the program text?\" Let bindings lead to the introduction of the notion of an environment that is passed to type inference rules. The deduction rule says, \"put the bindings that $name is of type $annotated_type in the environment, then deduce the type of $subexpr . Then we can simply say that the type of some identifier $identifier is the type that we find looking up $identifier up in that environment. In the DSLX prototype code this environment is called the Bindings , and it maps identifiers to the AST node that defines the name ( {Text: AstNode} ), which can be combined with a mapping from AST node to its deduced type ( {AstNode: ConcreteType} ) to resolve the type of an identifier. Expressions Unary Expressions DSLX supports three types of unary expressions: bit-wise not (the ! operator) negate (the - operator, computes the two's complement negation) Binary Expressions DSLX support a familiar set of binary expressions. For those, both operands to the expression must have the same type. This is true even for the shift operators. shift-right ( >> ) shift-right arithmetic ( >>> ) shift-left ( << ) bit-wise or ( | ) bit-wise and ( & ) add ( + ) subtract ( - ) xor ( ^ ) multiply ( * ) logical or ( || ) logical and ( && ) Comparison Expressions For comparison expressions the types of both operands must match. However these operations return a result of type bits[1] , aka bool . equal ( == ) not-equal ( != ) greater-equal ( >= ) greater ( > ) less-equal ( <= ) less ( < ) Concat Expression TODO(meheff): Explain the intricacies of our bits type interpretation and how it affects concat. Match Expression Match expressions permit \"pattern matching\" on data, like a souped-up switch statement. It can both test for values (like a conditional guard) and bind values to identifiers for subsequent use. For example: fn f(t: (u8, u32)) -> u32 { match t { (u8:42, y) => y; (_, y) => y+u8:77 } } If the first member of the tuple is the value is 42 , we pass the second tuple member back as-is from the function. Otherwise, we add 77 to the value and return that. The _ symbolizes \"I don't care about this value\". Just like literal constants, pattern matching can also match via named constants; For example, consider this variation on the above: const MY_FAVORITE_NUMBER = u8:42; fn f(t: (u8, u32)) -> u32 { match t { (MY_FAVORITE_NUMBER, y) => y; (_, y) => y+u8:77 } } This also works with nested tuples; for example: const MY_FAVORITE_NUMBER = u8:42; fn f(t: (u8, (u16, u32))) -> u32 { match t { (MY_FAVORITE_NUMBER, (y, z)) => u32:y+z; (_, (y, u32:42)) => u32:y; _ => u32:7 } } Here we use a \"catch all\" wildcard pattern in the last match arm to ensure the match expression always matches the input somehow. let Expression let expressions work the same way as let expressions in other functional languages, such as the ML languages or Haskell. let expressions provide a nested, lexically-scoped, list of declarations. The scope of the declaration is the expression and the right hand side of the declaration. For example, let a: u32 = u32:1 + u32:2; let b: u32 = a + u32:3; b would bind (and return) the value 6 to b . In effect there is little difference to other languages like C/C++ or python, where the same result would be achieved with code similar to this: a = 1+2 b = a+3 return b However, let expressions are lexically scoped. In above example, the value 3 is bound to a only during the combined let expression sequence. There is no other type of scoping in DSLX. Ternary If Expression DSLX offers a ternary if expression, which is very similar to the Python ternary if . Blueprint: consequent if condition else alternate This corresponds to the C/C++ ternary ?: operator, but with the order of the operands changed: condition ? consequent : alternate For example, in the FP adder module (modules/fpadd_2x32.x), there is code like the following: [...] let result_sfd = result_sfd if wide_exponent < u9:255 else u23:0; let result_exponent = wide_exponent as u8 if wide_exponent < u9:255 else u8:255; Iterable Expression Iterable expressions are used in counted for loops. DSLX currently support two types of iterable expressions, range and enumerate . The range expression range(m, n) produces values from m to n-1 (similar to how typical loops are constructed in C/C++). This example will run from 0 to 4 (exclusive): for (i, accum): (u32, u32) in range(u32:0, u32:4) { enumerate iterates over the elements of an array type and produces pairs of (index, value) , similar to enumeration constructs in languages like Python or Go. In the example below, the loop will iterate 8 times, following the array dimension of x . Each iteration produces a tuple with the current index ( i ranging from 0 to 7) and the value at the index ( e = x[i] ). fn prefix_scan_eq(x: u32[8]) -> bits[8,3] { let (_, _, result) = for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) in enumerate(x) {... for Expression DSLX currently supports counted loops. Blueprint: for (index, accumulator): (type-of-index, type-of-accumulator) in iterable { body-expression } (initial-accumulator-value) Examples: Add up all values from 0 to 4 (exclusive). Note that we pass the accumulator's initial value in as a parameter to this expression. for (i, accum): (u32, u32) in range(u32:0, u32:4) { accum + i }(u32:0) To add up values from 7 to 11 (exclusive), one would write: let base: u32 = u32:7; for (i, accum): (u32, u32) in range(u32:0, u32:4) { accum + base + i }(u32:0) Invariants can be used in the loop body, for example: let outer_thing: u32 = u32:42; for (i, accum): (u32, u32) in range(u32:0, u32:4) { accum + i + outer_thing }(u32:0) Both the index and accumulator can be of any valid type, in particular, they can be tuple types. For example: for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) Numerical Conversions DSLX adopts the Rust rules for semantics of numeric casts: Casting from larger bit-widths to smaller bit-widths will truncate (to the LSbs). Casting from a smaller bit-width to a larger bit-width will zero-extend if the source is unsigned, sign-extend if the source is signed. Casting from a bit-width to its own bit-width, between signed/unsigned, is a no-op. test numerical_conversions { let s8_m2 = s8:-2; let u8_m2 = u8:-2; // Sign extension (source type is signed). let _ = assert_eq(s32:-2, s8_m2 as s32); let _ = assert_eq(u32:-2, s8_m2 as u32); let _ = assert_eq(s16:-2, s8_m2 as s16); let _ = assert_eq(u16:-2, s8_m2 as u16); // Zero extension (source type is unsigned). let _ = assert_eq(u32:0xfe, u8_m2 as u32); let _ = assert_eq(s32:0xfe, u8_m2 as s32); // Nop (bitwidth is unchanged). let _ = assert_eq(s8:-2, s8_m2 as s8); let _ = assert_eq(s8:-2, u8_m2 as s8); let _ = assert_eq(u8:-2, u8_m2 as u8); let _ = assert_eq(s8:-2, u8_m2 as s8); () } Array Conversions Casting to an array takes bits from the MSb to the LSb; that is, the group of bits including the MSb ends up as element 0, the next group ends up as element 1, and so on. Casting from an array to bits performs the inverse operation: element 0 becomes the MSbs of the resulting value. All casts between arrays and bits must have the same total bit count. fn cast_to_array(x: u6) -> u2[3] { x as u2[3] } fn cast_from_array(a: u2[3]) -> u6 { a as u6 } fn concat_arrays(a: u2[3], b: u2[3]) -> u2[6] { a ++ b } test cast_to_array { let a_value: u6 = u6:0b011011; let a: u2[3] = cast_to_array(a_value); let a_array = u2[3]:[1, 2, 3]; let _ = assert_eq(a, a_array); // Note: converting back from array to bits gives the original value. let _ = assert_eq(a_value, cast_from_array(a)); let b_value: u6 = u6:0b111001; let b_array: u2[3] = u2[3]:[3, 2, 1]; let b: u2[3] = cast_to_array(b_value); let _ = assert_eq(b, b_array); let _ = assert_eq(b_value, cast_from_array(b)); // Concatenation of bits is analogous to concatenation of their converted // arrays. That is: // // convert(concat(a, b)) == concat(convert(a), convert(b)) let concat_value: u12 = a_value ++ b_value; let concat_array: u2[6] = concat_value as u2[6]; let _ = assert_eq(concat_array, concat_arrays(a_array, b_array)); // Show a few classic \"endianness\" example using 8-bit array values. let x = u32:0xdeadbeef; let _ = assert_eq(x as u8[4], u8[4]:[0xde, 0xad, 0xbe, 0xef]); let y = u16:0xbeef; let _ = assert_eq(y as u8[2], u8[2]:[0xbe, 0xef]); () } Advanced Understanding: Parametricity, Constraints, and Unification An infamous wrinkle is introduced for parametric functions: consider the following function: fn [T: type, U: type] add_wrapper(x: T, y: U) -> T { x + y } Based on the inference rule, we know that '+' can only type check when the operand types are the same. This means we can conclude that type T is the same as type U . Once we determine this, we need to make sure anywhere U is used it is consistent with the fact it is the same as T . In a sense the + operator is \"adding a constraint\" that T is equivalent to U , and trying to check that fact is valid is under the purview of type inference. The fact that the constraint is added that T and U are the same type is referred to as \"unification\", as what was previously two entities with potentially different constraints now has a single set of constraints that comes from the union of its operand types. DSLX's typechecker will go through the body of parametric functions per invocation. As such, the typechecker will always have the invocation's parametric values for use in asserting type consistency against \"constraints\" such as derived parametric expressions, body vs. annotated return type equality, and expression inference rules. Statements Imports DSLX modules can import other modules via the import keyword. Circular imports are not permitted (the dependencies among DSLX modules must form a DAG, as in languages like Go). The import statement takes the following form (note the lack of semicolon): import path.to.my.imported_module With that statement, the module will be accessible as (the trailing identifier after the last dot) imported_module ; e.g. the program can refer to imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT . NOTE Imports are relative to the Bazel \"depot root\" -- for external use of the tools a DSLX_PATH will be exposed, akin to a PYTHONPATH , for users to indicate paths where were should attempt module discovery. NOTE Importing does not introduce any names into the current file other than the one referred to by the import statement. That is, if imported_module had a constant defined in it FOO , this is referred to via imported_module::FOO , FOO does not \"magically\" get put in the current scope. This is analogous to how wildcard imports are discouraged in other languages (e.g. from import * in Python) on account of leading to \"namespace pollution\" and needing to specify what happens when names conflict. If you want to change the name of the imported module (for reference inside of the importing file) you can use the as keyword: import path.to.my.imported_module as im Just using the above construct, imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT is not valid, only im::IMPORTED_MODULE_PUBLIC_CONSTANT . However, both statements can be used on different lines: import path.to.my.imported_module import path.to.my.imported_module as im In this case, either im::IMPORTED_MODULE_PUBLIC_CONSTANT or imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT can be used to refer to the same thing. Here is an example using the same function via two different aliases for the same module: import xls.dslx.interpreter.tests.mod_imported import xls.dslx.interpreter.tests.mod_imported as mi fn main(x: u3) -> u1 { mod_imported::my_lsb(x) || mi::my_lsb(x) } test main { assert_eq(u1:0b1, main(u3:0b001)) } Public module members Module members are private by default and not accessible from any importing module. To make a member public/visible to importing modules, the pub keyword must be added as a prefix; e.g. const FOO = u32:42; // Not accessible to importing modules. pub const BAR = u32:64; // Accessible to importing modules. This applies to other things defined at module scope as well: functions, enums, typedefs, etc. import xls.dslx.interpreter.tests.mod_imported import xls.dslx.interpreter.tests.mod_imported as mi fn main(x: u3) -> u1 { mod_imported::my_lsb(x) || mi::my_lsb(x) } test main { assert_eq(u1:0b1, main(u3:0b001)) } Typedefs To import a type defined in an imported module or make a convenient shorthand for an existing type, the typedef construct can be used at module scope; e.g. for the case of an enum: import xls.dslx.interpreter.tests.mod_imported type MyEnum = mod_imported::MyEnum; fn main(x: u8) -> MyEnum { x as MyEnum } test main { let _ = assert_eq(main(u8:42), MyEnum::FOO); let _ = assert_eq(main(u8:64), MyEnum::BAR); () } Const The const keyword is used to define module-level constant values. Named constants should be usable anywhere a literal value can be used: const FOO = u8:42; fn match_const(x: u8) -> u8 { match x { FOO => u8:0; _ => u8:42; } } test match_const_not_binding { let _ = assert_eq(u8:42, match_const(u8:0)); let _ = assert_eq(u8:42, match_const(u8:1)); let _ = assert_eq(u8:0, match_const(u8:42)); () } fn h(t: (u8, (u16, u32))) -> u32 { match t { (FOO, (x, y)) => (x as u32) + y; (_, (y, u32:42)) => y as u32; _ => u32:7; } } test match_nested { let _ = assert_eq(u32:3, h((u8:42, (u16:1, u32:2)))); let _ = assert_eq(u32:1, h((u8:0, (u16:1, u32:42)))); let _ = assert_eq(u32:7, h((u8:0, (u16:1, u32:0)))); () } Parallel Primitives map reduce group-by TODO Builtins Bit Slicing DSLX supports Python-style bit slicing over bits types. Note that bits are numbered 0..N starting \"from the right\" (least significant bit, AKA LSb), for example: Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 A slice expression [n:m] means to get from bit n (inclusive) to bit 'm' exclusive. This can be confusing, because the n stands to the left of m in the expression, but bit n would be to the 'right' of m in the classical bit numbering (note: Not in the classical array visualization, where element 0 is usually drawn to the left). For example, the expression [0:2] would yield: Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 ^ ^ included ^ excluded Result: 0b11 Note that, as of now, the indices for this [n:m] form must be literal numbers (so the compiler can determine the width of the result). To perform a slice with a non-literal-number start position, see the +: form described below. The slicing operation also support the python style slices with offsets from start or end. To visualize, one can think of x[ : -1] as the equivalent of x[from the start : bitwidth - 1] . Correspondingly, x[-1 : ] can be visualized as [ bitwidth - 1 : to the end] . For example, to get all bits, except the MSb (from the beginning, until the top element minus 1): x[:-1] Or to get the left-most 2 bits (from bitwidth - 2, all the way to the end): x[-2:] There is also a \"counted\" form x[start +: bits[N]] - starting from a specified bit, slice out the next N bits. This is equivalent to: bits[N]:(x >> start) . The type can be specified as either signed or unsigned; e.g. [start +: s8] will produce an 8-bit signed value starting at start , whereas [start +: u4] will produce a 4-bit unsigned number starting at start . Here are many more examples: // Identity function helper. fn [N: u32] id(x: bits[N]) -> bits[N] { x } test bit_slice_syntax { let x = u6:0b100111; // Slice out two bits. let _ = assert_eq(u2:0b11, x[0:2]); let _ = assert_eq(u2:0b11, x[1:3]); let _ = assert_eq(u2:0b01, x[2:4]); let _ = assert_eq(u2:0b00, x[3:5]); // Slice out three bits. let _ = assert_eq(u3:0b111, x[0:3]); let _ = assert_eq(u3:0b011, x[1:4]); let _ = assert_eq(u3:0b001, x[2:5]); let _ = assert_eq(u3:0b100, x[3:6]); // Slice out from the end. let _ = assert_eq(u1:0b1, x[-1:]); let _ = assert_eq(u1:0b1, x[-1:6]); let _ = assert_eq(u2:0b10, x[-2:]); let _ = assert_eq(u2:0b10, x[-2:6]); let _ = assert_eq(u3:0b100, x[-3:]); let _ = assert_eq(u3:0b100, x[-3:6]); let _ = assert_eq(u4:0b1001, x[-4:]); let _ = assert_eq(u4:0b1001, x[-4:6]); // Slice both relative to the end (MSb). let _ = assert_eq(u2:0b01, x[-4:-2]); let _ = assert_eq(u2:0b11, x[-6:-4]); // Slice out from the beginning (LSb). let _ = assert_eq(u5:0b00111, x[:-1]); let _ = assert_eq(u4:0b0111, x[:-2]); let _ = assert_eq(u3:0b111, x[:-3]); let _ = assert_eq(u2:0b11, x[:-4]); let _ = assert_eq(u1:0b1, x[:-5]); // Slicing past the end just means we hit the end (as in Python). let _ = assert_eq(u1:0b1, x[5:7]); let _ = assert_eq(u1:0b1, x[-7:1]); let _ = assert_eq(bits[0]:0, x[-7:-6]); let _ = assert_eq(bits[0]:0, x[-6:-6]); let _ = assert_eq(bits[0]:0, x[6:6]); let _ = assert_eq(bits[0]:0, x[6:7]); let _ = assert_eq(u1:1, x[-6:-5]); // Slice of a slice. let _ = assert_eq(u2:0b11, x[:4][1:3]); // Slice of an invocation. let _ = assert_eq(u2:0b01, id(x)[2:4]); // Explicit-width slices. let _ = assert_eq(u2:0b01, x[2+:u2]); let _ = assert_eq(s3:0b100, x[3+:s3]); let _ = assert_eq(u3:0b001, x[5+:u3]); () } clz, ctz DSLX provides the common \"count leading zeroes\" and \"count trailing zeroes\" functions: let x0 = u32:0x0FFFFFF8; let x1 = clz(x0); let x2 = ctz(x0); let _ = assert_eq(u32:4, x1); assert_eq(u32:3, x2) signex Casting has well-defined extension rules, but in some cases it is necessary to be explicit about sign-extensions, if just for code readability. For this, there is the signex builtin. To invoke the signex builtin, provide it with the operand to sign extend (lhs), as well as the target type to extend to: these operands may be either signed or unsigned. Note that the value of the right hand side is ignored, only its type is used to determine the result type of the sign extension. let x = u8:-1; let s: s32 = signex(x, s32:0); let u: u32 = signex(x, u32:0); assert_eq(u32:s, u) Note that both s and u contain the same bits in the above example. rev rev is used to reverse the bits in an unsigned bits value. The LSb in the input becomes the MSb in the result, the 2nd LSb becomes the 2nd MSb in the result, and so on. // (Dummy) wrapper around reverse. fn [N: u32] wrapper(x: bits[N]) -> bits[N] { rev(x) } // Target for IR conversion that works on u3s. fn main(x: u3) -> u3 { wrapper(x) } // Reverse examples. test reverse { let _ = assert_eq(u3:0b100, main(u3:0b001)); let _ = assert_eq(u3:0b001, main(u3:0b100)); let _ = assert_eq(bits[0]:0, rev(bits[0]:0)); let _ = assert_eq(u1:1, rev(u1:1)); let _ = assert_eq(u2:0b10, rev(u2:0b01)); let _ = assert_eq(u2:0b00, rev(u2:0b00)); () } Bitwise reductions These are unary reduction operations applied to a bits-typed value: and_reduce : evaluates to bits[N]:1 if all bits are set or_reduce : evaluates to bits[N]:1 if any bit is set in the input, and 0 otherwise. xor_reduce : evaluates to bits[N]:1 if there is an odd number of bits set in the input, and 0 otherwise. update update(array, index, new_value) updates array by replacing the value previously at index with new_value and returns the updated array. Note that this is not an in-place update of the array, it is an \"evolution\" of the array value and it is up to the compiler to find places in which an in-place replacement is viable. Testing and Debugging DSLX allows specifying tests right in the implementation file via the test and quickcheck directives. Having key test code in the implementation file serves two purposes. It helps to ensure the code behaves as expected. Additionally it serves as 'executable' documentation, similar in spirit to Python doc strings. Unit Tests Unit tests are specified by the test directive, as seen below: #![test] fn test_reverse() { let _ = assert_eq(u1:1, rev(u1:1)); let _ = assert_eq(u2:0b10, rev(u2:0b01)); let _ = assert_eq(u2:0b00, rev(u2:0b00)); () } The DSLX interpreter will execute all functions that are proceeded by a test directive. These functions should be non-parametric, take no arguments, and should return a unit-type. Unless otherwise specified in the implementation's build configs, functions called by unit tests are also converted to XLS IR and run through the toolchain's LLVM JIT. The resulting values from the DSLX interpreter and the LLVM JIT are compared against each other to assert equality. This is to ensure DSLX implementations are IR-convertable and that IR translation is correct. QuickCheck QuickCheck is a testing framework concept founded on property-based testing. Instead of specifying expected and test values, QuickCheck asks for properties of the implementation that should hold true against any input of the specified type(s). In DSLX, we use the quickcheck directive to designate functions to be run via the toolchain's QuickCheck framework. Here is an example that complements the unit testing of DSLX's rev implementation from above: // Reversing a value twice gets you the original value. #![quickcheck] fn prop_double_reverse(x: u32) -> bool { x == rev(rev(x)) } The DSLX interpreter will also execute all functions that are proceeded by a quickcheck directive. These functions should be non-parametric and return a bool . The framework will provide randomized input based on the types of the arguments to the function (e.g. above, the framework will provided randomized u32 's as x ). By default, the framework will run the function against 1000 sets of randomized inputs. This default may be changed by specifying the test_count key in the quickcheck directive before a particular test: #![quickcheck(test_count=50000)] The framework also allows programmers to specify a seed to use in generating the random inputs, as opposed to letting the framework pick one. The seed chosen for production can be found in the execution log. For determinism, the DSLX interpreter should be run with the seed flag: ./interpreter_main --seed=1234 <DSLX source file> assert_eq In a unit test pseudo function all valid DSLX code is allowed. To evaluate test results DSLX provides the assert_eq primitive (we'll add more of those in the future). Here is an example of a divceil implementation with its corresponding tests: fn divceil(x: u32, y: u32) -> u32 { (x-u32:1) / y + u32:1 } #![test] fn test_divceil() { let _ = assert_eq(u32:3, divceil(u32:5, u32:2)); let _ = assert_eq(u32:2, divceil(u32:4, u32:2)); let _ = assert_eq(u32:2, divceil(u32:3, u32:2)); let _ = assert_eq(u32:1, divceil(u32:2, u32:2)); _ } Note that in this example, the final let _ = ... in _ construct could be omitted. assert_eq cannot be synthesized into equivalent Verilog. Because of that it is recommended to use it within test constructs (interpretation) only. trace DSLX supports printf-style debugging via the trace expression, which allows dumping of current values to stdout. For example: fn decode_s_instruction(ins: u32) -> (u12, u5, u5, u3, u7) { let imm_11_5 = (ins >> u32:25); let rs2 = (ins >> u32:20) & u32:0x1F; let rs1 = (ins >> u32:15) & u32:0x1F; let funct3 = (ins >> u32:12) & u32:0x07; let imm_4_0 = (ins >> u32:7) & u32:0x1F; let opcode = ins & u32:0x7F; let _ = trace(imm_11_5); let _ = trace(imm_4_0); (u12:(u7:imm_11_5 ++ u5:imm_4_0), u5:rs2, u5:rs1, u3:funct3, u7:opcode) } would produce the following output, with each trace being annotated with its corresponding source position: [...] [ RUN ] decode_s_test_lsb trace of imm_11_5 @ 69:17-69:27: bits[32]:0x1 trace of imm_4_0 @ 70:17-70:26: bits[32]:0x1 [...] trace also returns the value passed to it, so it can be used inline, as in: match trace(my_thing) { [...] } To see the values of all expressions during interpretation, invoke the interpreter or test with the --trace_all flag: $ ./interpreter_main clz.x -logtostderr -trace_all [ RUN ] clz trace of (u3:0) @ clz.x:2:24: bits[3]:0x0 trace of (u3:0b111) @ clz.x:2:34-2:39: bits[3]:0x7 trace of clz((u3:0b111)) @ clz.x:2:30-2:40: bits[3]:0x0 trace of assert_eq((u3:0), clz((u3:0b111))) @ clz.x:2:20-2:41: () trace of (u3:1) @ clz.x:3:24: bits[3]:0x1 trace of (u3:0b011) @ clz.x:3:34-3:39: bits[3]:0x3 trace of clz((u3:0b011)) @ clz.x:3:30-3:40: bits[3]:0x1 trace of assert_eq((u3:1), clz((u3:0b011))) @ clz.x:3:20-3:41: () trace of (u3:2) @ clz.x:4:24: bits[3]:0x2 trace of (u3:0b001) @ clz.x:4:34-4:39: bits[3]:0x1 trace of clz((u3:0b001)) @ clz.x:4:30-4:40: bits[3]:0x2 trace of assert_eq((u3:2), clz((u3:0b001))) @ clz.x:4:20-4:41: () trace of (u3:3) @ clz.x:5:24: bits[3]:0x3 trace of (u3:0b000) @ clz.x:5:34-5:39: bits[3]:0x0 trace of clz((u3:0b000)) @ clz.x:5:30-5:40: bits[3]:0x3 trace of assert_eq((u3:3), clz((u3:0b000))) @ clz.x:5:20-5:41: () trace of () @ clz.x:6:3-6:5: () [ OK ] clz Tracing has no equivalent node in the IR (nor would such a node make sense), so any trace nodes are silently dropped during conversion. fail!() TODO(leary): Document the fail() expression. Appendix Operator Precedence DSLX's operator precedence matches Rust's. Listed below are DSLX's operators in descending precedence order. Binary operators at the same level share the same associativity and will be grouped accordingly. Operator Associativity Unary - ! n/a as Left to right * / % Left to right + - Left to right << >> >>> Left to right & Left to right ^ Left to right \\| Left to right == != < > <= >= Left to right && Left to right \\|\\| Left to right","title":"Reference"},{"location":"dslx_reference/#dslx-reference","text":"DSLX Reference Comments Identifiers Functions Parameters Parametric Functions Function Calls Types Bit Type Enum Types Tuple Type Struct Type Array Type Type Aliases Type Casting Type Checking and Inference Type Inference Details Operator Example Type errors Let Bindings, Names, and the Environment Expressions Unary Expressions Binary Expressions Comparison Expressions Concat Expression Match Expression let Expression Ternary If Expression Iterable Expression for Expression Numerical Conversions Array Conversions Advanced Understanding: Parametricity, Constraints, and Unification Statements Imports Public module members Typedefs Const Parallel Primitives map reduce group-by Builtins Bit Slicing clz, ctz signex rev Bitwise reductions update Testing and Debugging Unit Tests QuickCheck assert_eq trace fail!() Appendix Operator Precedence DSLX is a domain specific, functional language to build hardware that can also run effectively as host software. The DSL targets the XLS compiler (by conversion to XLS IR) to enable flows for FPGAs and ASICs (note that other frontends will become available in the future). DSLX mimics Rust, while being an immutable expression-based dataflow DSL with hardware-oriented features; e.g. arbitrary bitwidths, entirely fixed size objects, fully analyzeable call graph, etc. To avoid arbitrary new syntax/semantics choices the DSL mimics Rust where it is reasonably possible; for example, integer conversions all follow the same semantics as Rust. Note: There are some unnecessary differences today from Rust syntax due to early experimentation, but they are quickly being removed to converge on Rust syntax. Dataflow DSLs are a good fit for describing hardware, compared to languages designed assume von Neumann style computation (global mutable state, sequential mutation by a sequential thread of control). Using a DSL provides a more hardware-oriented representation of a given computation that matches XLS compiler (IR) constructs closely. The DSL also allows an exploration of HLS without being encumbered by C++ language or compiler limitations such as non-portable pragmas, magic macros, or semantically important syntactic conventions. The language is still experimental and likely to change, but it is already useful for experimentation and exploration. This document provides a reference for DSLX, mostly by example. After perusing it and learning about the language features, we recommend exploring the following, detailed examples to learn how the language features are put to action: CRC32 Floating-point addition Prefix Sum Computation In this document we use the function to compute a CRC32 checksum to describe language features. The full code is in examples/dslx_intro/crc32_one_byte.x .","title":"DSLX Reference"},{"location":"dslx_reference/#comments","text":"Just as in languages like C/C++, comments start with // and last through the end of the line.","title":"Comments"},{"location":"dslx_reference/#identifiers","text":"All identifiers, eg., for function names, parameters, and values, follow the typical naming rules of other languages. The identifiers can start with a character or an underscore, and can then contain more characters, underscores, or numbers. Valid examples are: a // valid CamelCase // valid like_under_scores // valid __also_ok // valid _Ok123_321 // valid _ // valid 2ab // not valid &ade // not valid However, we suggest the following DSLX style rules : Functions are written_like_this User-defined data types are NamesLikeThis Constant bindings are NAMES_LIKE_THIS _ is the black-hole identifier, as in Python. It should never be used in a binding-reference.","title":"Identifiers"},{"location":"dslx_reference/#functions","text":"Function definitions begin with the keyword fn , followed by the function name, a parameter list to the function in parenthesis, followed by an -> and the return type of the function. After this, curly braces denote the begin and end of the function body. The list of parameters can be empty. A single input file can contain many functions. Simple examples: fn ret3() -> u32 { 3 // this function always returns 3. } fn add1(x: u32) -> u32 { x + u32:1 // returns x + 1, but you knew that. } Functions return the result of their last computed expression as their return value. There are no explicit return statements. Functions also don't have multiple return statements. Tuples should be returned if a function should return multiple values.","title":"Functions"},{"location":"dslx_reference/#parameters","text":"Parameters are written as pairs name followed by a colon : followed by the type of that parameter. Each parameter needs to declare its own type. Examples: // a simple parameter x of type u32 x: u32 // t is a tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 t: (u32, (u8, (u16,), u8))","title":"Parameters"},{"location":"dslx_reference/#parametric-functions","text":"DSLX functions can be parameterized in terms of the types of its arguments and in terms of types derived from other parametric values. For instance: fn double(n: bits[32]) -> bits[32] { n * bits[32]:2 } fn [A: u32, B: u32 = double(A)] self_append(x: bits[A]) -> bits[B] { x++x } fn main() -> bits[10] { self_append(bits[5]:1) } In self_append(bits[5]:1) , we see that A = 5 based off of formal argument instantiation. Using that value, we can evaluate B = double(A=5) . This derived expression is analogous to C++'s constexpr \u2013 a simple expression that can be evaluated at that point in compilation. See advanced understanding for more information on parametricity.","title":"Parametric Functions"},{"location":"dslx_reference/#function-calls","text":"Function calls are expressions and look and feel just like one would expect from other languages. For example: fn callee(x: bits[32], y: bits[32]) -> bits[32] { x + y } fn caller() -> u32 { callee(u32:2, u32:3) } If more than one value should be returned by a function, a tuple type should be returned.","title":"Function Calls"},{"location":"dslx_reference/#types","text":"","title":"Types"},{"location":"dslx_reference/#bit-type","text":"The most fundamental type in DSLX is a variable length bit type denoted as bits[n] , where n is a constant. For example: bits[0] // possible, but, don't do that bits[1] // a single bit uN[1] // explicitly noting single bit is unsigned u1 // convenient shorthand for bits[1] bits[8] // an 8-bit datatype, yes, a byte u8 // convenient shorthand for bits[8] bits[32] // a 32-bit datatype u32 // convenient shorthand for bits[32] bits[256] // a 256-bit datatype DSLX introduces shortcuts for commonly used types, such as u8 for an 8-wide bit type, or u32 for a 32-bit wide bit type. These are defined up to u64 . All u* and bits[*] types are interpreted as unsigned numbers. Signed numbers are specified via sN[*] , which is analogous to bits[*] and uN[*] which are the corresponding unsigned version of the bit width * . For example: sN[0] s0 sN[1] s1 sN[64] s64 sN[256] Similarly to unsigned numbers, the s* shorthands are defined up to s64 . Signed numbers differ in their behavior from unsigned numbers primarily via operations like comparisons, (variable width) multiplications, and divisions.","title":"Bit Type"},{"location":"dslx_reference/#enum-types","text":"DSLX supports enumerations as a way of defining a group of related, scoped, named constants that do not pollute the module namespace. For example: enum Opcode : u3 { FIRE_THE_MISSILES = 0, BE_TIRED = 1, TAKE_A_NAP = 2, } fn get_my_favorite_opcode() -> Opcode { Opcode::FIRE_THE_MISSILES } Note the use of the double-colon to reference the enum value. This code specifies that the enum behaves like a u3 : its storage and extension (via casting) behavior are defined to be those of a u3 . Attempts to define an enum value outside of the representable u3 range will produce a compile time error. enum Opcode : u3 { FOO = 8 // Causes compile time error! } Enums can be compared for equality/inequality, but they do not permit arithmetic operations, they must be cast to numerical types in order to perform arithmetic: fn same_opcode(x: Opcode, y: Opcode) -> bool { x == y // ok } fn next_in_sequence(x: Opcode, y: Opcode) -> bool { // x+1 == y // does not work, arithmetic! u3:x + u3:1 == u3:y // ok, casted first } As mentioned above casting of enum-values works with the same casting/extension rules that apply to the underlying enum type definition. For example, this cast will sign extend because the source type for the enum is signed. (See numerical conversions for the full description of extension/truncation behavior.) enum MySignedEnum : s3 { LOW = -1 ZERO = 0 HIGH = 1 } fn extend_to_32b(x: MySignedEnum) -> u32 { u32:x // Sign-extends because the source type is signed. } test extend_to_32b { assert_eq(extend_to_32b(MySignedEnum::LOW), u32:-1) } Casting to an enum is also permitted. However, in most cases errors from invalid casting can only be found at runtime, e.g., in the DSL interpreter or flagging a fatal error from hardware. Because of that, it is recommended to avoid such casts as much as possible.","title":"Enum Types"},{"location":"dslx_reference/#tuple-type","text":"A tuple is an ordered set of fixed size containing elements of potentially different types. Tuples can contain bits, arrays, or other tuples. Examples: (0b100, 0b101) a tuple containing two bits elements // A tuple with 2 elements. // the 1st element is of type u32 // the 2nd element is a tuple with 3 elements // the 1st element is of type u8 // the 2nd element is another tuple with 1 element of type u16 // the 3rd element is of type u8 (u32, (u8, (u16,), u8) To access individual tuple elements use simple indices, starting at 0 (Member access by field names is work in progress). For example, to access the 2nd element of a tuple (index 1): let t = (u32:2, u8:3); assert_eq(u8:3, t[u32:1]) Another way to \"destructure\" a tuple into names is to use tuple assignment: let t = (u32:2, u8:3); let (a, b) = t; let _ = assert_eq(u32:2, a); assert_eq(u8:3, b) Just as values can be discarded in a let by using the \"black hole identifier\" _ , don't care values can also be discarded when destructuring a tuple: let t = (u32:2, u8:3, true); let (_, _, v) = t; assert_eq(v, true)","title":"Tuple Type"},{"location":"dslx_reference/#struct-type","text":"Structs are \"sugar\" on top of tuples that give names to the various slots and have convenient ways of constructing / accessing the members. The following syntax is used to define a struct: struct Point { x: u32, y: u32 } Once a struct is defined it can be constructed by naming the fields in any order: struct Point { x: u32, y: u32, } test struct_equality { let p0 = Point { x: u32:42, y: u32:64 }; let p1 = Point { y: u32:64, x: u32:42 }; assert_eq(p0, p1) } There is a simple syntax when defining fields with names that are the same as the specified values: ``` struct Point { x: u32, y: u32, } test struct_equality { let x = u32:42; let y = u32:64; let p0 = Point { x, y }; let p1 = Point { y, x }; assert_eq(p0, p1) } ``` Struct fields can also be accessed with \"dot\" syntax: struct Point { x: u32, y: u32, } fn f(p: Point) -> u32 { p.x + p.y } fn main() -> u32 { f(Point { x: u32:42, y: u32:64 }) } test main { assert_eq(u32:106, main()) } Note that structs cannot be mutated \"in place\", the user must construct new values by extracting the fields of the original struct mixed together with new field values, as in the following: struct Point3 { x: u32, y: u32, z: u32, } fn update_y(p: Point3, new_y: u32) -> Point3 { Point3 { x: p.x, y: new_y, z: p.z } } fn main() -> Point3 { let p = Point3 { x: u32:42, y: u32:64, z: u32:256 }; update_y(p, u32:128) } test main { let want = Point3 { x: u32:42, y: u32:128, z: u32:256 }; assert_eq(want, main()) } The DSL has syntax for conveniently producing a new value with a subset of fields updated to \"feel\" like the convenience of mutation. The \"struct update\" syntax is: fn update_y(p: Point3) -> Point3 { Point3 { y: u32:42, ..p } } fn update_x_and_y(p: Point3) -> Point3 { Point3 { x: u32:42, y: u32:42, ..p } } Note that structs are not compatible with other structs that happen to have the same definition (this is called \"nominal typing\"). For example: def test_nominal_typing(self): # Nominal typing not structural, e.g. OtherPoint cannot be passed where we # want a Point, even though their members are the same. self._typecheck( \"\"\" struct Point { x: s8, y: u32, } struct OtherPoint { x: s8, y: u32 } fn f(x: Point) -> Point { x } fn g() -> Point { let shp = OtherPoint { x: s8:255, y: u32:1024 }; f(shp) } \"\"\", error='parameter type name: \\'Point\\'; argument type name: \\'OtherPoint\\'' ) DSLX also supports parametric structs. For more information on how type-parametricity works, see the parametric functions section. ``` fn double(n: u32) -> u32 { n * u32:2 } struct [N: u32, M: u32 = double(N)] Point { x: bits[N], y: bits[M], } fn [A: u32, B: u32] make_point(x: bits[A], y: bits[B]) -> Point[A, B] { Point { x, y } } test struct_construction { let p = make_point(u16:42, u32:42); assert_eq(u16:42, p.x) } ```","title":"Struct Type"},{"location":"dslx_reference/#array-type","text":"Arrays can be constructed via bracket notation. All values that make up the array must have the same type. Arrays can be indexed with indexing notation ( a[i] ) to retrieve a single element. fn main(a: u32[2], i: u1) -> u32 { a[i] } test main { let x = u32:42; let y = u32:64; // Make an array with \"bracket notation\". let my_array: u32[2] = [x, y]; let _ = assert_eq(main(my_array, u1:0), x); let _ = assert_eq(main(my_array, u1:1), y); () } Because arrays with repeated trailing elements are common, the DSL supports ellipsis ( ... ) at the end of an array to fill the remainder of the array with the last noted element. Because the compiler must know how many elements to fill, in order to use the ellipsis the type must be annotated explicitly as shown. fn make_array(x: u32) -> u32[3] { u32[3]:[u32:42, x, ...] } test make_array { let _ = assert_eq(u32[3]:[u32:42, u32:42, u32:42], make_array(u32:42)); let _ = assert_eq(u32[3]:[u32:42, u32:64, u32:64], make_array(u32:64)); () } TODO(meheff): Explain arrays and the intricacies of our bits type interpretation and how it affects arrays of bits etc.","title":"Array Type"},{"location":"dslx_reference/#type-aliases","text":"DLSX supports the definition of type aliases. For example, to define a tuple type to represent a float number with a sign bit, an 8-bit mantissa, and a 23-bit mantissa, one would write: type F32 = { u1, u8, u23, } After this definition, the F32 may be used as a type annotation interchangeably with (u1, u8, u23) . Note, however, that structs are generally preferred as described above, as they are more readable and users do not need to rely on tuple indices remaining the same in the future. For direct type aliasing, users can use the following coding style (similar to the C++ typedef keyword): type TypeWeight = u6;","title":"Type Aliases"},{"location":"dslx_reference/#type-casting","text":"Bit types can be cast from one bit-width to another. To cast a given value to a different bit-width, the required code looks similar to how other types are being specified in DSLX: Simply prefix the value with the desired type, followed by a : . In this example: fn add_with_carry(x: bits[24], y: bits[24]) -> (u1, bits[24]) { let result = (u1:0 ++ x) + (u1:0 ++ y); (u1:(result >> bits[25]:24), bits[24]:result) } The type of result is inferred to be of type bits[25] (concatenation of a 24-bit value with an additional 1-bit value in the front). The expression bits[25]:24 specifies the type of the value 24 to be of bits[25] . The larger expression (u1:(result >> bits[25]:24) casts the result of the shift expression to a u1 . Only a single bit is returned, the least significant bit (the right-most bit). The expression bits[24]:result casts the 25-bit value 'result' down to a 24-bit value, basically chopping off the leading bit.","title":"Type Casting"},{"location":"dslx_reference/#type-checking-and-inference","text":"DSLX performs type checking and produces an error if types in an expression don't match up. let expressions also perform type inference, which is quite convenient. For example, instead of writing: let ch: u32 = (e & f) ^ ((!e) & g); let (h, g, f): (u32, u32, u32) = (g, f, e); one can write the following, as long as the types can be properly inferred: let ch = (e & f) ^ ((!e) & g); let (h, g, f) = (g, f, e); Note that type annotations can still be added and be used for program understanding, as they they will be checked by DSLX.","title":"Type Checking and Inference"},{"location":"dslx_reference/#type-inference-details","text":"DSLX uses deductive type inference to check the types present in the program. Deductive type inference is a set of (typically straight-forward) deduction rules: Hindley-Milner style deductive type inference determines the result type of a function with a rule that only observes the input types to that function. (Note that operators like '+' are just slightly special functions in that they have pre-defined special-syntax-rule names.)","title":"Type Inference Details"},{"location":"dslx_reference/#operator-example","text":"For example, consider the binary (meaning takes two operands) / infix (meaning it syntactically is placed in the center of its operands) '+' operator. The simple deductive type inference rule for '+' is: (T, T) -> T Meaning that the left hand side operand to the '+' operator is of some type (call it T), the right hand side operand to the '+' operator must be of that same type, T, and the result of that operator is then (deduced) to be of the same type as its operands, T. Let's instantiate this rule in a function: fn add_wrapper(x: bits[2], y: bits[2]) -> bits[2] { x + y } This function wraps the '+' operator. It presents two arguments to the '+' operator and then checks that the annotated return type on add_wrapper matches the deduced type for the body of that function; that is, we ask the following question of the '+' operator (since the type of the operands must be known at the point the add is performed): (bits[2], bits[2]) -> ? To resolve the '?' the following procedure is being used: Pattern match the rule given above (T, T) -> T to determine the type T: the left hand side operand is bits[2] , called T. Check that the right hand side is also that same T, which it is: another bits[2] . Deduce that the result type is that same type T: bits[2] . That becomes the return type of the body of the function. Check that it is the same type as the annotated return type for the function, and it is! The function is annotated to return bits[2] , and the deduced type of the body is also bits[2] . Qed.","title":"Operator Example"},{"location":"dslx_reference/#type-errors","text":"A type error would occur in the following: fn add_wrapper(x: bits[2], y: bits[3]) -> bits[2] { x + y } Applying the type deduction rule for '+' finds an inconsistency. The left hand side operand has type bits[2] , called T, but the right hand side is bits[3] , which is not the same as T. Because the deductive type inference rule does not say what to do when the operand types are different, it results in a type error which is flagged at this point in the program.","title":"Type errors"},{"location":"dslx_reference/#let-bindings-names-and-the-environment","text":"All expressions in the language's expression grammar have a deductive type inference rule. The types must be known for inputs to an operator/function (otherwise there'd be a use-before-definition error) and every expression has a way to determine its type from its operand expressions. A more interesting deduction rule comes into view with \"let\" expressions, which are of the form: let $name: $annotated_type = $expr in $subexpr An example of this is: let x: u32 = u32:2 in x That's an expression which evaluates to the value '2' of type u32 . In a let expression like this, we say $name gets \"bound\" to a value of type $annotated_type . The let typecheck must both check that $expr is of type $annotated_type , as well as determine the type of $subexpr , which is the type of the overall \"let expression\". This leads to the deduction rule that \"let just returns the type of $subexpr \". But, in this example, the subexpr needs some information from the outer let expression, because if asked \"what's the type of some symbol y \" one immediately asks \"well what comes before that in the program text?\" Let bindings lead to the introduction of the notion of an environment that is passed to type inference rules. The deduction rule says, \"put the bindings that $name is of type $annotated_type in the environment, then deduce the type of $subexpr . Then we can simply say that the type of some identifier $identifier is the type that we find looking up $identifier up in that environment. In the DSLX prototype code this environment is called the Bindings , and it maps identifiers to the AST node that defines the name ( {Text: AstNode} ), which can be combined with a mapping from AST node to its deduced type ( {AstNode: ConcreteType} ) to resolve the type of an identifier.","title":"Let Bindings, Names, and the Environment"},{"location":"dslx_reference/#expressions","text":"","title":"Expressions"},{"location":"dslx_reference/#unary-expressions","text":"DSLX supports three types of unary expressions: bit-wise not (the ! operator) negate (the - operator, computes the two's complement negation)","title":"Unary Expressions"},{"location":"dslx_reference/#binary-expressions","text":"DSLX support a familiar set of binary expressions. For those, both operands to the expression must have the same type. This is true even for the shift operators. shift-right ( >> ) shift-right arithmetic ( >>> ) shift-left ( << ) bit-wise or ( | ) bit-wise and ( & ) add ( + ) subtract ( - ) xor ( ^ ) multiply ( * ) logical or ( || ) logical and ( && )","title":"Binary Expressions"},{"location":"dslx_reference/#comparison-expressions","text":"For comparison expressions the types of both operands must match. However these operations return a result of type bits[1] , aka bool . equal ( == ) not-equal ( != ) greater-equal ( >= ) greater ( > ) less-equal ( <= ) less ( < )","title":"Comparison Expressions"},{"location":"dslx_reference/#concat-expression","text":"TODO(meheff): Explain the intricacies of our bits type interpretation and how it affects concat.","title":"Concat Expression"},{"location":"dslx_reference/#match-expression","text":"Match expressions permit \"pattern matching\" on data, like a souped-up switch statement. It can both test for values (like a conditional guard) and bind values to identifiers for subsequent use. For example: fn f(t: (u8, u32)) -> u32 { match t { (u8:42, y) => y; (_, y) => y+u8:77 } } If the first member of the tuple is the value is 42 , we pass the second tuple member back as-is from the function. Otherwise, we add 77 to the value and return that. The _ symbolizes \"I don't care about this value\". Just like literal constants, pattern matching can also match via named constants; For example, consider this variation on the above: const MY_FAVORITE_NUMBER = u8:42; fn f(t: (u8, u32)) -> u32 { match t { (MY_FAVORITE_NUMBER, y) => y; (_, y) => y+u8:77 } } This also works with nested tuples; for example: const MY_FAVORITE_NUMBER = u8:42; fn f(t: (u8, (u16, u32))) -> u32 { match t { (MY_FAVORITE_NUMBER, (y, z)) => u32:y+z; (_, (y, u32:42)) => u32:y; _ => u32:7 } } Here we use a \"catch all\" wildcard pattern in the last match arm to ensure the match expression always matches the input somehow.","title":"Match Expression"},{"location":"dslx_reference/#let-expression","text":"let expressions work the same way as let expressions in other functional languages, such as the ML languages or Haskell. let expressions provide a nested, lexically-scoped, list of declarations. The scope of the declaration is the expression and the right hand side of the declaration. For example, let a: u32 = u32:1 + u32:2; let b: u32 = a + u32:3; b would bind (and return) the value 6 to b . In effect there is little difference to other languages like C/C++ or python, where the same result would be achieved with code similar to this: a = 1+2 b = a+3 return b However, let expressions are lexically scoped. In above example, the value 3 is bound to a only during the combined let expression sequence. There is no other type of scoping in DSLX.","title":"let Expression"},{"location":"dslx_reference/#ternary-if-expression","text":"DSLX offers a ternary if expression, which is very similar to the Python ternary if . Blueprint: consequent if condition else alternate This corresponds to the C/C++ ternary ?: operator, but with the order of the operands changed: condition ? consequent : alternate For example, in the FP adder module (modules/fpadd_2x32.x), there is code like the following: [...] let result_sfd = result_sfd if wide_exponent < u9:255 else u23:0; let result_exponent = wide_exponent as u8 if wide_exponent < u9:255 else u8:255;","title":"Ternary If Expression"},{"location":"dslx_reference/#iterable-expression","text":"Iterable expressions are used in counted for loops. DSLX currently support two types of iterable expressions, range and enumerate . The range expression range(m, n) produces values from m to n-1 (similar to how typical loops are constructed in C/C++). This example will run from 0 to 4 (exclusive): for (i, accum): (u32, u32) in range(u32:0, u32:4) { enumerate iterates over the elements of an array type and produces pairs of (index, value) , similar to enumeration constructs in languages like Python or Go. In the example below, the loop will iterate 8 times, following the array dimension of x . Each iteration produces a tuple with the current index ( i ranging from 0 to 7) and the value at the index ( e = x[i] ). fn prefix_scan_eq(x: u32[8]) -> bits[8,3] { let (_, _, result) = for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3])) in enumerate(x) {...","title":"Iterable Expression"},{"location":"dslx_reference/#for-expression","text":"DSLX currently supports counted loops. Blueprint: for (index, accumulator): (type-of-index, type-of-accumulator) in iterable { body-expression } (initial-accumulator-value) Examples: Add up all values from 0 to 4 (exclusive). Note that we pass the accumulator's initial value in as a parameter to this expression. for (i, accum): (u32, u32) in range(u32:0, u32:4) { accum + i }(u32:0) To add up values from 7 to 11 (exclusive), one would write: let base: u32 = u32:7; for (i, accum): (u32, u32) in range(u32:0, u32:4) { accum + base + i }(u32:0) Invariants can be used in the loop body, for example: let outer_thing: u32 = u32:42; for (i, accum): (u32, u32) in range(u32:0, u32:4) { accum + i + outer_thing }(u32:0) Both the index and accumulator can be of any valid type, in particular, they can be tuple types. For example: for ((i, e), (prior, count, result)): ((u32, u32), (u32, u3, bits[8,3]))","title":"for Expression"},{"location":"dslx_reference/#numerical-conversions","text":"DSLX adopts the Rust rules for semantics of numeric casts: Casting from larger bit-widths to smaller bit-widths will truncate (to the LSbs). Casting from a smaller bit-width to a larger bit-width will zero-extend if the source is unsigned, sign-extend if the source is signed. Casting from a bit-width to its own bit-width, between signed/unsigned, is a no-op. test numerical_conversions { let s8_m2 = s8:-2; let u8_m2 = u8:-2; // Sign extension (source type is signed). let _ = assert_eq(s32:-2, s8_m2 as s32); let _ = assert_eq(u32:-2, s8_m2 as u32); let _ = assert_eq(s16:-2, s8_m2 as s16); let _ = assert_eq(u16:-2, s8_m2 as u16); // Zero extension (source type is unsigned). let _ = assert_eq(u32:0xfe, u8_m2 as u32); let _ = assert_eq(s32:0xfe, u8_m2 as s32); // Nop (bitwidth is unchanged). let _ = assert_eq(s8:-2, s8_m2 as s8); let _ = assert_eq(s8:-2, u8_m2 as s8); let _ = assert_eq(u8:-2, u8_m2 as u8); let _ = assert_eq(s8:-2, u8_m2 as s8); () }","title":"Numerical Conversions"},{"location":"dslx_reference/#array-conversions","text":"Casting to an array takes bits from the MSb to the LSb; that is, the group of bits including the MSb ends up as element 0, the next group ends up as element 1, and so on. Casting from an array to bits performs the inverse operation: element 0 becomes the MSbs of the resulting value. All casts between arrays and bits must have the same total bit count. fn cast_to_array(x: u6) -> u2[3] { x as u2[3] } fn cast_from_array(a: u2[3]) -> u6 { a as u6 } fn concat_arrays(a: u2[3], b: u2[3]) -> u2[6] { a ++ b } test cast_to_array { let a_value: u6 = u6:0b011011; let a: u2[3] = cast_to_array(a_value); let a_array = u2[3]:[1, 2, 3]; let _ = assert_eq(a, a_array); // Note: converting back from array to bits gives the original value. let _ = assert_eq(a_value, cast_from_array(a)); let b_value: u6 = u6:0b111001; let b_array: u2[3] = u2[3]:[3, 2, 1]; let b: u2[3] = cast_to_array(b_value); let _ = assert_eq(b, b_array); let _ = assert_eq(b_value, cast_from_array(b)); // Concatenation of bits is analogous to concatenation of their converted // arrays. That is: // // convert(concat(a, b)) == concat(convert(a), convert(b)) let concat_value: u12 = a_value ++ b_value; let concat_array: u2[6] = concat_value as u2[6]; let _ = assert_eq(concat_array, concat_arrays(a_array, b_array)); // Show a few classic \"endianness\" example using 8-bit array values. let x = u32:0xdeadbeef; let _ = assert_eq(x as u8[4], u8[4]:[0xde, 0xad, 0xbe, 0xef]); let y = u16:0xbeef; let _ = assert_eq(y as u8[2], u8[2]:[0xbe, 0xef]); () }","title":"Array Conversions"},{"location":"dslx_reference/#advanced-understanding-parametricity-constraints-and-unification","text":"An infamous wrinkle is introduced for parametric functions: consider the following function: fn [T: type, U: type] add_wrapper(x: T, y: U) -> T { x + y } Based on the inference rule, we know that '+' can only type check when the operand types are the same. This means we can conclude that type T is the same as type U . Once we determine this, we need to make sure anywhere U is used it is consistent with the fact it is the same as T . In a sense the + operator is \"adding a constraint\" that T is equivalent to U , and trying to check that fact is valid is under the purview of type inference. The fact that the constraint is added that T and U are the same type is referred to as \"unification\", as what was previously two entities with potentially different constraints now has a single set of constraints that comes from the union of its operand types. DSLX's typechecker will go through the body of parametric functions per invocation. As such, the typechecker will always have the invocation's parametric values for use in asserting type consistency against \"constraints\" such as derived parametric expressions, body vs. annotated return type equality, and expression inference rules.","title":"Advanced Understanding: Parametricity, Constraints, and Unification"},{"location":"dslx_reference/#statements","text":"","title":"Statements"},{"location":"dslx_reference/#imports","text":"DSLX modules can import other modules via the import keyword. Circular imports are not permitted (the dependencies among DSLX modules must form a DAG, as in languages like Go). The import statement takes the following form (note the lack of semicolon): import path.to.my.imported_module With that statement, the module will be accessible as (the trailing identifier after the last dot) imported_module ; e.g. the program can refer to imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT . NOTE Imports are relative to the Bazel \"depot root\" -- for external use of the tools a DSLX_PATH will be exposed, akin to a PYTHONPATH , for users to indicate paths where were should attempt module discovery. NOTE Importing does not introduce any names into the current file other than the one referred to by the import statement. That is, if imported_module had a constant defined in it FOO , this is referred to via imported_module::FOO , FOO does not \"magically\" get put in the current scope. This is analogous to how wildcard imports are discouraged in other languages (e.g. from import * in Python) on account of leading to \"namespace pollution\" and needing to specify what happens when names conflict. If you want to change the name of the imported module (for reference inside of the importing file) you can use the as keyword: import path.to.my.imported_module as im Just using the above construct, imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT is not valid, only im::IMPORTED_MODULE_PUBLIC_CONSTANT . However, both statements can be used on different lines: import path.to.my.imported_module import path.to.my.imported_module as im In this case, either im::IMPORTED_MODULE_PUBLIC_CONSTANT or imported_module::IMPORTED_MODULE_PUBLIC_CONSTANT can be used to refer to the same thing. Here is an example using the same function via two different aliases for the same module: import xls.dslx.interpreter.tests.mod_imported import xls.dslx.interpreter.tests.mod_imported as mi fn main(x: u3) -> u1 { mod_imported::my_lsb(x) || mi::my_lsb(x) } test main { assert_eq(u1:0b1, main(u3:0b001)) }","title":"Imports"},{"location":"dslx_reference/#public-module-members","text":"Module members are private by default and not accessible from any importing module. To make a member public/visible to importing modules, the pub keyword must be added as a prefix; e.g. const FOO = u32:42; // Not accessible to importing modules. pub const BAR = u32:64; // Accessible to importing modules. This applies to other things defined at module scope as well: functions, enums, typedefs, etc. import xls.dslx.interpreter.tests.mod_imported import xls.dslx.interpreter.tests.mod_imported as mi fn main(x: u3) -> u1 { mod_imported::my_lsb(x) || mi::my_lsb(x) } test main { assert_eq(u1:0b1, main(u3:0b001)) }","title":"Public module members"},{"location":"dslx_reference/#typedefs","text":"To import a type defined in an imported module or make a convenient shorthand for an existing type, the typedef construct can be used at module scope; e.g. for the case of an enum: import xls.dslx.interpreter.tests.mod_imported type MyEnum = mod_imported::MyEnum; fn main(x: u8) -> MyEnum { x as MyEnum } test main { let _ = assert_eq(main(u8:42), MyEnum::FOO); let _ = assert_eq(main(u8:64), MyEnum::BAR); () }","title":"Typedefs"},{"location":"dslx_reference/#const","text":"The const keyword is used to define module-level constant values. Named constants should be usable anywhere a literal value can be used: const FOO = u8:42; fn match_const(x: u8) -> u8 { match x { FOO => u8:0; _ => u8:42; } } test match_const_not_binding { let _ = assert_eq(u8:42, match_const(u8:0)); let _ = assert_eq(u8:42, match_const(u8:1)); let _ = assert_eq(u8:0, match_const(u8:42)); () } fn h(t: (u8, (u16, u32))) -> u32 { match t { (FOO, (x, y)) => (x as u32) + y; (_, (y, u32:42)) => y as u32; _ => u32:7; } } test match_nested { let _ = assert_eq(u32:3, h((u8:42, (u16:1, u32:2)))); let _ = assert_eq(u32:1, h((u8:0, (u16:1, u32:42)))); let _ = assert_eq(u32:7, h((u8:0, (u16:1, u32:0)))); () }","title":"Const"},{"location":"dslx_reference/#parallel-primitives","text":"","title":"Parallel Primitives"},{"location":"dslx_reference/#map","text":"","title":"map"},{"location":"dslx_reference/#reduce","text":"","title":"reduce"},{"location":"dslx_reference/#group-by","text":"TODO","title":"group-by"},{"location":"dslx_reference/#builtins","text":"","title":"Builtins"},{"location":"dslx_reference/#bit-slicing","text":"DSLX supports Python-style bit slicing over bits types. Note that bits are numbered 0..N starting \"from the right\" (least significant bit, AKA LSb), for example: Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 A slice expression [n:m] means to get from bit n (inclusive) to bit 'm' exclusive. This can be confusing, because the n stands to the left of m in the expression, but bit n would be to the 'right' of m in the classical bit numbering (note: Not in the classical array visualization, where element 0 is usually drawn to the left). For example, the expression [0:2] would yield: Bit 6 5 4 3 2 1 0 Value 1 0 0 0 1 1 1 ^ ^ included ^ excluded Result: 0b11 Note that, as of now, the indices for this [n:m] form must be literal numbers (so the compiler can determine the width of the result). To perform a slice with a non-literal-number start position, see the +: form described below. The slicing operation also support the python style slices with offsets from start or end. To visualize, one can think of x[ : -1] as the equivalent of x[from the start : bitwidth - 1] . Correspondingly, x[-1 : ] can be visualized as [ bitwidth - 1 : to the end] . For example, to get all bits, except the MSb (from the beginning, until the top element minus 1): x[:-1] Or to get the left-most 2 bits (from bitwidth - 2, all the way to the end): x[-2:] There is also a \"counted\" form x[start +: bits[N]] - starting from a specified bit, slice out the next N bits. This is equivalent to: bits[N]:(x >> start) . The type can be specified as either signed or unsigned; e.g. [start +: s8] will produce an 8-bit signed value starting at start , whereas [start +: u4] will produce a 4-bit unsigned number starting at start . Here are many more examples: // Identity function helper. fn [N: u32] id(x: bits[N]) -> bits[N] { x } test bit_slice_syntax { let x = u6:0b100111; // Slice out two bits. let _ = assert_eq(u2:0b11, x[0:2]); let _ = assert_eq(u2:0b11, x[1:3]); let _ = assert_eq(u2:0b01, x[2:4]); let _ = assert_eq(u2:0b00, x[3:5]); // Slice out three bits. let _ = assert_eq(u3:0b111, x[0:3]); let _ = assert_eq(u3:0b011, x[1:4]); let _ = assert_eq(u3:0b001, x[2:5]); let _ = assert_eq(u3:0b100, x[3:6]); // Slice out from the end. let _ = assert_eq(u1:0b1, x[-1:]); let _ = assert_eq(u1:0b1, x[-1:6]); let _ = assert_eq(u2:0b10, x[-2:]); let _ = assert_eq(u2:0b10, x[-2:6]); let _ = assert_eq(u3:0b100, x[-3:]); let _ = assert_eq(u3:0b100, x[-3:6]); let _ = assert_eq(u4:0b1001, x[-4:]); let _ = assert_eq(u4:0b1001, x[-4:6]); // Slice both relative to the end (MSb). let _ = assert_eq(u2:0b01, x[-4:-2]); let _ = assert_eq(u2:0b11, x[-6:-4]); // Slice out from the beginning (LSb). let _ = assert_eq(u5:0b00111, x[:-1]); let _ = assert_eq(u4:0b0111, x[:-2]); let _ = assert_eq(u3:0b111, x[:-3]); let _ = assert_eq(u2:0b11, x[:-4]); let _ = assert_eq(u1:0b1, x[:-5]); // Slicing past the end just means we hit the end (as in Python). let _ = assert_eq(u1:0b1, x[5:7]); let _ = assert_eq(u1:0b1, x[-7:1]); let _ = assert_eq(bits[0]:0, x[-7:-6]); let _ = assert_eq(bits[0]:0, x[-6:-6]); let _ = assert_eq(bits[0]:0, x[6:6]); let _ = assert_eq(bits[0]:0, x[6:7]); let _ = assert_eq(u1:1, x[-6:-5]); // Slice of a slice. let _ = assert_eq(u2:0b11, x[:4][1:3]); // Slice of an invocation. let _ = assert_eq(u2:0b01, id(x)[2:4]); // Explicit-width slices. let _ = assert_eq(u2:0b01, x[2+:u2]); let _ = assert_eq(s3:0b100, x[3+:s3]); let _ = assert_eq(u3:0b001, x[5+:u3]); () }","title":"Bit Slicing"},{"location":"dslx_reference/#clz-ctz","text":"DSLX provides the common \"count leading zeroes\" and \"count trailing zeroes\" functions: let x0 = u32:0x0FFFFFF8; let x1 = clz(x0); let x2 = ctz(x0); let _ = assert_eq(u32:4, x1); assert_eq(u32:3, x2)","title":"clz, ctz"},{"location":"dslx_reference/#signex","text":"Casting has well-defined extension rules, but in some cases it is necessary to be explicit about sign-extensions, if just for code readability. For this, there is the signex builtin. To invoke the signex builtin, provide it with the operand to sign extend (lhs), as well as the target type to extend to: these operands may be either signed or unsigned. Note that the value of the right hand side is ignored, only its type is used to determine the result type of the sign extension. let x = u8:-1; let s: s32 = signex(x, s32:0); let u: u32 = signex(x, u32:0); assert_eq(u32:s, u) Note that both s and u contain the same bits in the above example.","title":"signex"},{"location":"dslx_reference/#rev","text":"rev is used to reverse the bits in an unsigned bits value. The LSb in the input becomes the MSb in the result, the 2nd LSb becomes the 2nd MSb in the result, and so on. // (Dummy) wrapper around reverse. fn [N: u32] wrapper(x: bits[N]) -> bits[N] { rev(x) } // Target for IR conversion that works on u3s. fn main(x: u3) -> u3 { wrapper(x) } // Reverse examples. test reverse { let _ = assert_eq(u3:0b100, main(u3:0b001)); let _ = assert_eq(u3:0b001, main(u3:0b100)); let _ = assert_eq(bits[0]:0, rev(bits[0]:0)); let _ = assert_eq(u1:1, rev(u1:1)); let _ = assert_eq(u2:0b10, rev(u2:0b01)); let _ = assert_eq(u2:0b00, rev(u2:0b00)); () }","title":"rev"},{"location":"dslx_reference/#bitwise-reductions","text":"These are unary reduction operations applied to a bits-typed value: and_reduce : evaluates to bits[N]:1 if all bits are set or_reduce : evaluates to bits[N]:1 if any bit is set in the input, and 0 otherwise. xor_reduce : evaluates to bits[N]:1 if there is an odd number of bits set in the input, and 0 otherwise.","title":"Bitwise reductions"},{"location":"dslx_reference/#update","text":"update(array, index, new_value) updates array by replacing the value previously at index with new_value and returns the updated array. Note that this is not an in-place update of the array, it is an \"evolution\" of the array value and it is up to the compiler to find places in which an in-place replacement is viable.","title":"update"},{"location":"dslx_reference/#testing-and-debugging","text":"DSLX allows specifying tests right in the implementation file via the test and quickcheck directives. Having key test code in the implementation file serves two purposes. It helps to ensure the code behaves as expected. Additionally it serves as 'executable' documentation, similar in spirit to Python doc strings.","title":"Testing and Debugging"},{"location":"dslx_reference/#unit-tests","text":"Unit tests are specified by the test directive, as seen below: #![test] fn test_reverse() { let _ = assert_eq(u1:1, rev(u1:1)); let _ = assert_eq(u2:0b10, rev(u2:0b01)); let _ = assert_eq(u2:0b00, rev(u2:0b00)); () } The DSLX interpreter will execute all functions that are proceeded by a test directive. These functions should be non-parametric, take no arguments, and should return a unit-type. Unless otherwise specified in the implementation's build configs, functions called by unit tests are also converted to XLS IR and run through the toolchain's LLVM JIT. The resulting values from the DSLX interpreter and the LLVM JIT are compared against each other to assert equality. This is to ensure DSLX implementations are IR-convertable and that IR translation is correct.","title":"Unit Tests"},{"location":"dslx_reference/#quickcheck","text":"QuickCheck is a testing framework concept founded on property-based testing. Instead of specifying expected and test values, QuickCheck asks for properties of the implementation that should hold true against any input of the specified type(s). In DSLX, we use the quickcheck directive to designate functions to be run via the toolchain's QuickCheck framework. Here is an example that complements the unit testing of DSLX's rev implementation from above: // Reversing a value twice gets you the original value. #![quickcheck] fn prop_double_reverse(x: u32) -> bool { x == rev(rev(x)) } The DSLX interpreter will also execute all functions that are proceeded by a quickcheck directive. These functions should be non-parametric and return a bool . The framework will provide randomized input based on the types of the arguments to the function (e.g. above, the framework will provided randomized u32 's as x ). By default, the framework will run the function against 1000 sets of randomized inputs. This default may be changed by specifying the test_count key in the quickcheck directive before a particular test: #![quickcheck(test_count=50000)] The framework also allows programmers to specify a seed to use in generating the random inputs, as opposed to letting the framework pick one. The seed chosen for production can be found in the execution log. For determinism, the DSLX interpreter should be run with the seed flag: ./interpreter_main --seed=1234 <DSLX source file>","title":"QuickCheck"},{"location":"dslx_reference/#assert_eq","text":"In a unit test pseudo function all valid DSLX code is allowed. To evaluate test results DSLX provides the assert_eq primitive (we'll add more of those in the future). Here is an example of a divceil implementation with its corresponding tests: fn divceil(x: u32, y: u32) -> u32 { (x-u32:1) / y + u32:1 } #![test] fn test_divceil() { let _ = assert_eq(u32:3, divceil(u32:5, u32:2)); let _ = assert_eq(u32:2, divceil(u32:4, u32:2)); let _ = assert_eq(u32:2, divceil(u32:3, u32:2)); let _ = assert_eq(u32:1, divceil(u32:2, u32:2)); _ } Note that in this example, the final let _ = ... in _ construct could be omitted. assert_eq cannot be synthesized into equivalent Verilog. Because of that it is recommended to use it within test constructs (interpretation) only.","title":"assert_eq"},{"location":"dslx_reference/#trace","text":"DSLX supports printf-style debugging via the trace expression, which allows dumping of current values to stdout. For example: fn decode_s_instruction(ins: u32) -> (u12, u5, u5, u3, u7) { let imm_11_5 = (ins >> u32:25); let rs2 = (ins >> u32:20) & u32:0x1F; let rs1 = (ins >> u32:15) & u32:0x1F; let funct3 = (ins >> u32:12) & u32:0x07; let imm_4_0 = (ins >> u32:7) & u32:0x1F; let opcode = ins & u32:0x7F; let _ = trace(imm_11_5); let _ = trace(imm_4_0); (u12:(u7:imm_11_5 ++ u5:imm_4_0), u5:rs2, u5:rs1, u3:funct3, u7:opcode) } would produce the following output, with each trace being annotated with its corresponding source position: [...] [ RUN ] decode_s_test_lsb trace of imm_11_5 @ 69:17-69:27: bits[32]:0x1 trace of imm_4_0 @ 70:17-70:26: bits[32]:0x1 [...] trace also returns the value passed to it, so it can be used inline, as in: match trace(my_thing) { [...] } To see the values of all expressions during interpretation, invoke the interpreter or test with the --trace_all flag: $ ./interpreter_main clz.x -logtostderr -trace_all [ RUN ] clz trace of (u3:0) @ clz.x:2:24: bits[3]:0x0 trace of (u3:0b111) @ clz.x:2:34-2:39: bits[3]:0x7 trace of clz((u3:0b111)) @ clz.x:2:30-2:40: bits[3]:0x0 trace of assert_eq((u3:0), clz((u3:0b111))) @ clz.x:2:20-2:41: () trace of (u3:1) @ clz.x:3:24: bits[3]:0x1 trace of (u3:0b011) @ clz.x:3:34-3:39: bits[3]:0x3 trace of clz((u3:0b011)) @ clz.x:3:30-3:40: bits[3]:0x1 trace of assert_eq((u3:1), clz((u3:0b011))) @ clz.x:3:20-3:41: () trace of (u3:2) @ clz.x:4:24: bits[3]:0x2 trace of (u3:0b001) @ clz.x:4:34-4:39: bits[3]:0x1 trace of clz((u3:0b001)) @ clz.x:4:30-4:40: bits[3]:0x2 trace of assert_eq((u3:2), clz((u3:0b001))) @ clz.x:4:20-4:41: () trace of (u3:3) @ clz.x:5:24: bits[3]:0x3 trace of (u3:0b000) @ clz.x:5:34-5:39: bits[3]:0x0 trace of clz((u3:0b000)) @ clz.x:5:30-5:40: bits[3]:0x3 trace of assert_eq((u3:3), clz((u3:0b000))) @ clz.x:5:20-5:41: () trace of () @ clz.x:6:3-6:5: () [ OK ] clz Tracing has no equivalent node in the IR (nor would such a node make sense), so any trace nodes are silently dropped during conversion.","title":"trace"},{"location":"dslx_reference/#fail","text":"TODO(leary): Document the fail() expression.","title":"fail!()"},{"location":"dslx_reference/#appendix","text":"","title":"Appendix"},{"location":"dslx_reference/#operator-precedence","text":"DSLX's operator precedence matches Rust's. Listed below are DSLX's operators in descending precedence order. Binary operators at the same level share the same associativity and will be grouped accordingly. Operator Associativity Unary - ! n/a as Left to right * / % Left to right + - Left to right << >> >>> Left to right & Left to right ^ Left to right \\| Left to right == != < > <= >= Left to right && Left to right \\|\\| Left to right","title":"Operator Precedence"},{"location":"fpadd_example/","text":"DSLX Example: Floating-point addition This document explains how floating-point addition is implemented in DSLX, along with a discussion on the algorithm and how it maps to DSLX. This document assumes familiarity with floating-point numbers in general (layout, precision, error, etc.). DSLX Example: Floating-point addition Background DSLX implementation Result sign determination Rounding Background Floating-point addition, like any FP operation, is much more complicated than integer addition, and has many more steps. Expand significands: Floating-point operations are computed with bits beyond that in their normal representations for increased precision. For IEEE 754 numbers, there are three extra, called the guard, rounding and sticky bits. The first two behave normally, but the last, the \"sticky\" bit, is special. During shift operations (below), if a \"1\" value is ever shifted into the sticky bit, it \"sticks\" - the bit will remain \"1\" through any further shift operations. In this step, the significands are expanded by these three bits. Align significands: To ensure that significands are added with appropriate magnitudes, they must be aligned according to their exponents. To do so, the smaller significant needs to be shifted to the right (each right shift is equivalent to increasing the exponent by one). - The extra precision bits are populated in this shift. - As part of this step, the leading 1 bit... and a sign bit Note: The sticky bit is calculated and applied in this step. Sign-adjustment: if the significands differ in sign, then the significand with the smaller initial exponent needs to be (two's complement) negated. Add the significands and capture the carry bit. Note that, if the signs of the significands differs, then this could result in higher bits being cleared. Normalize the significands: Shift the result so that the leading '1' is present in the proper space. This means shifting right one place if the result set the carry bit, and to the left some number of places if high bits were cleared. - The sticky bit must be preserved in any of these shifts! Rounding: Here, the extra precision bits are examined to determine if the result significand's last bit should be rounded up. IEEE 754 supports five rounding modes: - Round towards 0: just chop off the extra precision bits. - Round towards +infinity: round up if any extra precision bits are set. - Round towards -infinity: round down if any extra precision bits are set. - Round to nearest, ties away from zero: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, i.e., 0b100, then the result is rounded up for positive numbers and down for negative ones. - Round to nearest, ties to even: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, then the result is rounded in whichever direction causes the LSB of the result significant to be 0. - This is the most commonly-used rounding mode. - This is [currently] the only supported mode by the DSLX implementation. Special case handling: The results are examined for special cases such as NaNs, infinities, or (optionally) subnormals. DSLX implementation With an understanding of the algorithm above, the DSLX implementation is relatively straightforward. \"Interesting\" chunks are described below. Result sign determination The sign of the result will normally be the same as the sign of the operand with the greater exponent, but there are two extra cases to consider. If the operands have the same exponent, then the sign will be that of the greater significand, and if the result is 0, then we favor positive 0 vs. negative 0. let sfd = (addend_x as s29) + (addend_y as s29); let sfd_is_zero = sfd == s29:0; let result_sign = match (sfd_is_zero, sfd < s29:0) { (true, _) => u1:0; (false, true) => !greater_exp.sign; _ => greater_exp.sign; }; Rounding As complicated as rounding is to describe, its implementation is relatively straightforward. let normal_chunk = shifted_sfd[0:3]; let half_way_chunk = shifted_sfd[2:4]; let do_round_up = u1:1 if (normal_chunk > u3:0x4) | (half_way_chunk == u2:0x3) else u1:0; // We again need an extra bit for carry. let rounded_sfd = (shifted_sfd as u28) + u28:0x8 if do_round_up else (shifted_sfd as u28); let rounding_carry = rounded_sfd[-1:]; The behavior of logic descriptions - even in a higher level language such as DSLX - can be non-obvious to a new reader, so extensive comments, such as those here, are invaluable.","title":"FP adder"},{"location":"fpadd_example/#dslx-example-floating-point-addition","text":"This document explains how floating-point addition is implemented in DSLX, along with a discussion on the algorithm and how it maps to DSLX. This document assumes familiarity with floating-point numbers in general (layout, precision, error, etc.). DSLX Example: Floating-point addition Background DSLX implementation Result sign determination Rounding","title":"DSLX Example: Floating-point addition"},{"location":"fpadd_example/#background","text":"Floating-point addition, like any FP operation, is much more complicated than integer addition, and has many more steps. Expand significands: Floating-point operations are computed with bits beyond that in their normal representations for increased precision. For IEEE 754 numbers, there are three extra, called the guard, rounding and sticky bits. The first two behave normally, but the last, the \"sticky\" bit, is special. During shift operations (below), if a \"1\" value is ever shifted into the sticky bit, it \"sticks\" - the bit will remain \"1\" through any further shift operations. In this step, the significands are expanded by these three bits. Align significands: To ensure that significands are added with appropriate magnitudes, they must be aligned according to their exponents. To do so, the smaller significant needs to be shifted to the right (each right shift is equivalent to increasing the exponent by one). - The extra precision bits are populated in this shift. - As part of this step, the leading 1 bit... and a sign bit Note: The sticky bit is calculated and applied in this step. Sign-adjustment: if the significands differ in sign, then the significand with the smaller initial exponent needs to be (two's complement) negated. Add the significands and capture the carry bit. Note that, if the signs of the significands differs, then this could result in higher bits being cleared. Normalize the significands: Shift the result so that the leading '1' is present in the proper space. This means shifting right one place if the result set the carry bit, and to the left some number of places if high bits were cleared. - The sticky bit must be preserved in any of these shifts! Rounding: Here, the extra precision bits are examined to determine if the result significand's last bit should be rounded up. IEEE 754 supports five rounding modes: - Round towards 0: just chop off the extra precision bits. - Round towards +infinity: round up if any extra precision bits are set. - Round towards -infinity: round down if any extra precision bits are set. - Round to nearest, ties away from zero: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, i.e., 0b100, then the result is rounded up for positive numbers and down for negative ones. - Round to nearest, ties to even: Rounds to the nearest value. In cases where the extra precision bits are halfway between values, then the result is rounded in whichever direction causes the LSB of the result significant to be 0. - This is the most commonly-used rounding mode. - This is [currently] the only supported mode by the DSLX implementation. Special case handling: The results are examined for special cases such as NaNs, infinities, or (optionally) subnormals.","title":"Background"},{"location":"fpadd_example/#dslx-implementation","text":"With an understanding of the algorithm above, the DSLX implementation is relatively straightforward. \"Interesting\" chunks are described below.","title":"DSLX implementation"},{"location":"fpadd_example/#result-sign-determination","text":"The sign of the result will normally be the same as the sign of the operand with the greater exponent, but there are two extra cases to consider. If the operands have the same exponent, then the sign will be that of the greater significand, and if the result is 0, then we favor positive 0 vs. negative 0. let sfd = (addend_x as s29) + (addend_y as s29); let sfd_is_zero = sfd == s29:0; let result_sign = match (sfd_is_zero, sfd < s29:0) { (true, _) => u1:0; (false, true) => !greater_exp.sign; _ => greater_exp.sign; };","title":"Result sign determination"},{"location":"fpadd_example/#rounding","text":"As complicated as rounding is to describe, its implementation is relatively straightforward. let normal_chunk = shifted_sfd[0:3]; let half_way_chunk = shifted_sfd[2:4]; let do_round_up = u1:1 if (normal_chunk > u3:0x4) | (half_way_chunk == u2:0x3) else u1:0; // We again need an extra bit for carry. let rounded_sfd = (shifted_sfd as u28) + u28:0x8 if do_round_up else (shifted_sfd as u28); let rounding_carry = rounded_sfd[-1:]; The behavior of logic descriptions - even in a higher level language such as DSLX - can be non-obvious to a new reader, so extensive comments, such as those here, are invaluable.","title":"Rounding"},{"location":"fuzzer/","text":"XLS Fuzzer XLS Fuzzer Crashers Directory Single-file reproducers {#reproducers} IR minimization {#minimization} Summaries Debugging a failing sample {#debugging} Debugging a tool crash Result miscomparison: unoptimized IR Result miscomparison: optimized IR Debugging the LLVM JIT Result miscomparison: simulated Verilog To execute the XLS fuzz driver simply run a command line like the following: bazel run -c opt \\ //xls/dslx/fuzzer/run_fuzz_multiprocess \\ -- --crash_path=/tmp/crashers-$(date +'%Y-%m-%d') --seed=0 --duration=8h The XLS fuzzer generates a sequence of randomly generated DSLX functions and a set of random inputs to each function often with interesting bit patterns. Given that stimulus, the fuzz driver performs the following actions some of which may be disabled/enabled via flags (run with --help for more details): Runs the DSLX program through the DSLX interpreter with the batch of arguments Converts the DSLX program to IR Optimizes the converted IR Interprets the pre-optimized and optimized IR with the batch of arguments Generates the Verilog from the IR with randomly selected codegen options Simulates the generated Verilog using the batch of arguments Performs a multi-way comparison of the DSLX interpreter results, the pre-optimized IR interpreter results, post-optimized IR interpreter results, and the simulator results If an issue is observed, the fuzz driver attempts to minimize the IR that causes an issue to occur. The above actions are coordinated and run by the SampleRunner class. Many actions are performed by invoking a separate binary which isolates any crashes. When miscompares in results occur or the generated function crashes part of XLS, all artifacts generated by the fuzzer for that sample are written into a uniquely-named subdirectory under the --crash_path given in the command line. The fuzzer also writes a crasher file which is a single file for reproducing the issue. See below for instructions on debugging a failing sample. Crashers Directory The crashers directory includes a subdirectory created for each failing sample. To avoid collisions the subdirectory is named using a hash of the DSLX code. Each crasher subdirectory has the following contents: $ ls /tmp/crashers-2019-06-25/05adbd50 args.txt ir_converter_main.stderr run.sh cl.txt ir_minimizer.options.json sample.ir crasher_2020-04-23_9b05.x ir_minimizer_test.sh sample.ir.results eval_ir_main.stderr options.json sample.x exception.txt opt_main.stderr sample.x.results The directory includes the problematic DSLX sample ( sample.x ) and the input arguments ( args.txt ) as well as all artifacts generated and stderr output emitted by the various utilities invoked to test the sample. Notable files include: options.json : Options used to run the sample. sample.ir : Unoptimized IR generated from the DSLX sample. sample.opt.ir : IR after optimizations. sample.v : Generated Verilog. *.results : The results (numeric values) produced by interpreting or simulating the respective input (DSLX, IR, or Verilog). exception.txt : The exception raised when running the sample. Typically this will indicate either a result miscomparison or a tool return non-zero status (for example, the IR optimizer crashed). crasher_*.x : A single file reproducer which includes the DSLX code, arguments, and options. See below for details. Typically the exact nature of the failure can be identified by reading the file exception.txt and possibly the stderr outputs of the various tools. The fuzzer can optionally produce a minimized IR reproduction of the problem. This will be written to minimized.ir . See below for details. Single-file reproducers {#reproducers} When the fuzzer encounters an issue it will create a single-file reproducer: --- Worker 14 observed an exception, noting --- Worker 14 noted crasher #1 for sampleno 42 at /tmp/crashers/095fb405 Copying that file to the directory //xls/dslx/fuzzer/crashers will automatically create a bazel test target for it and add it to the regression suite. Tests can also be added as known failures in //xls/dslx/fuzzer/build_defs.bzl as they're being triaged / investigated like so: generate_crasher_regression_tests( srcs = glob([\"crashers/*\"]), prefix = \"xls/dslx/fuzzer\", # TODO(xls-team): 2019-06-30 Triage and fix these. failing = [ \"crashers/crasher_2019-06-29_129987.x\", \"crashers/crasher_2019-06-29_402110.x\", ], ) Known-failures are marked as manual and excluded from continuous testing. To run the regression suite: bazel test //xls/dslx/fuzzer:all To run the regression suite including known-failures, run the regression target directly: bazel test //xls/dslx/fuzzer:regression_tests To reproduce from that single-file reproducer there is a command line tool: bazel run //xls/dslx/fuzzer/run_crasher -- \\ crasher_2019-06-26_3354.x IR minimization {#minimization} By default the fuzzer attempts to generate a minimal IR reproducer for the problem identified by the DSLX sample. Starting with the unoptimized IR the fuzzer invokes ir_minimizer_main to reduce the size of the input IR. It uses various simplification strategies to minimize the number of nodes in the IR. See the usage description in the tool source code for detailed information. The minimized IR is written to a file minimized.ir in the crasher directory for the sample. Note that minimization is only possible if the problem (crash, result miscomparison, etc.) occurs after conversion from DSLX to XLS IR. Summaries To monitor progress of the fuzzer and to determine op coverage the fuzzer can optionally (with --summary_path ) write summary information to files. The summary files are Protobuf files containing the proto SampleSummaryProto defined in //xls/dslx/fuzzer/sample_summary.proto . The summary information about the IR generated from the DSLX sample such as the number and type of each IR op as well as the bit width and number of operands. The summaries can be read with the tool //xls/dslx/fuzzer/read_summary_main . See usage description in the code for more details. Debugging a failing sample {#debugging} A generated sample can fail in one of two ways: a tool crash or a result miscomparison. A tool crash occurred if one of the tools invoked by the fuzzer (e.g., opt_main which optimizes the IR) returned a non-zero status. A result miscomparison occurred if there is not perfect correspondence between the results produced by various ways in which the generated function is evaluated: Interpreted DSLX Evaluated unoptimized IR Evaluated optimized IR Simulation of the generated (System)Verilog Generally, the results produced by the interpretation of the DSLX serves are the reference results for comparisons. To identify the underlying cause of the sample failure inspect the exception.txt file in the crasher directory. The file contains the text of the exception raised in SampleRunner which clearly identifies the kind of failure (result miscomparison or tool crash) and details about which evaluation resulted in a miscompare or which tool crashed, respectively. Consult the following sections on how to debug particular kinds of failures. Debugging a tool crash The exception.txt file includes the invocation of the tool for reproducing the failure. Generally, this is a straightforward debugging process. If the failing tool is the IR optimizer binary opt_main the particular pass causing the failure should be in the backtrace. To retrieve the input to this pass, run opt_main with --ir_dump_path to dump the IR between each pass. The last IR file produced (the files are numbered sequentially) is the input to the failing pass. Result miscomparison: unoptimized IR The evaluation of the unoptimized IR is the first point at which result comparison occurs (DSLX interpretation versus unoptimized IR evaluation). A miscomparison here can indicate a bug in one of several places: DSLX interpreter DSLX to IR conversion IR interpreter or IR JIT. The error message in exception.txt indicates whether the JIT or the interpreter was used. To help narrow this down, the IR interpreter can be compared against the JIT with the eval_ir_main tool: eval_ir_main --test_llvm_jit --input_file=args.txt sample.ir This runs both the JIT and the interpreter on the unoptimized IR file ( sample.ir ) using the arguments in args.txt and compares the results. If this is successful, then likely the IR interpreter and the JIT are correct and problem lies earlier in the pipeline (DSLX interpretation or DSLX to IR conversion). Otherwise, there is definitely a bug in either the interpreter or the JIT as their results should always be equal. If a minimized IR file exists ( minimized.ir ) this may be a better starting point for isolating the failure. Result miscomparison: optimized IR This can indicate a bug in IR evaluation (interpreter or JIT) or in the optimizer. In this case, a comparison of the evaluation of the unoptimized IR against the DSLX interpreter has already succeeds so DSLX interpretation or conversion is unlikely to be the underlying cause. As with miscomparison involving the unoptimized IR, eval_ir_main can be used to compare the JIT results against the interpreter results: eval_ir_main --test_llvm_jit --input_file=args.txt sample.opt.ir If the above invocation fails there is a bug in the JIT or the interpreter. Otherwise, there may be a bug in the optimizer. The tool eval_ir_main can help isolate the problematic optimization pass by running with the options --optimize_ir and --eval_after_each_pass . With these flags, the tool runs the optimization pipeline on the given IR and evaluates the IR after each pass is run. The first pass which results in a miscompare against the unoptimized input IR is flagged. Invocation: eval_ir_main --input_file=args.txt \\ --optimize_ir \\ --eval_after_each_pass \\ sample.ir Debugging the LLVM JIT To help isolate bugs in the JIT, LLVM's optimization level can be set using the --llvm_opt_level flag: eval_ir_main --test_llvm_jit \\ --llvm_opt_level=0 \\ --input_file=args.txt sample.opt.ir If the results match (pass) with the optimization level set to zero but fail with the default optimization level of 3, there is likely a bug in the LLVM optimizer or the XLS-generated LLVM program has undefined behavior. Unoptimized and optimized LLVM IR are dumped by the JIT with vlog level of 2 or higher. The assembly at level 3 or higher. For example: eval_ir_main -v=3 --logtostderr sample.opt.ir The LLVM tool lli evaluates LLVM IR. The tool expects the IR to include a entry function main . See the uploaded file in this LLVM bug for an example LLVM IR file which includes a main function that calls a (slightly modified) XLS-generated function. The LLVM tool opt optimizes the LLVM IR and can be piped to lli like so: opt sample.ll --O2 | lli Result miscomparison: simulated Verilog This can be a bug in codegen, XLS's Verilog testbench code, or the Verilog simulator itself. Running the generated Verilog with different simulators can help isolate the problem: simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=iverilog \\ sample.v simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=${SIM_2} \\ sample.v The tool outputs the results of the evaluation to stdout so diffing their outputs is required.","title":"Fuzzer"},{"location":"fuzzer/#xls-fuzzer","text":"XLS Fuzzer Crashers Directory Single-file reproducers {#reproducers} IR minimization {#minimization} Summaries Debugging a failing sample {#debugging} Debugging a tool crash Result miscomparison: unoptimized IR Result miscomparison: optimized IR Debugging the LLVM JIT Result miscomparison: simulated Verilog To execute the XLS fuzz driver simply run a command line like the following: bazel run -c opt \\ //xls/dslx/fuzzer/run_fuzz_multiprocess \\ -- --crash_path=/tmp/crashers-$(date +'%Y-%m-%d') --seed=0 --duration=8h The XLS fuzzer generates a sequence of randomly generated DSLX functions and a set of random inputs to each function often with interesting bit patterns. Given that stimulus, the fuzz driver performs the following actions some of which may be disabled/enabled via flags (run with --help for more details): Runs the DSLX program through the DSLX interpreter with the batch of arguments Converts the DSLX program to IR Optimizes the converted IR Interprets the pre-optimized and optimized IR with the batch of arguments Generates the Verilog from the IR with randomly selected codegen options Simulates the generated Verilog using the batch of arguments Performs a multi-way comparison of the DSLX interpreter results, the pre-optimized IR interpreter results, post-optimized IR interpreter results, and the simulator results If an issue is observed, the fuzz driver attempts to minimize the IR that causes an issue to occur. The above actions are coordinated and run by the SampleRunner class. Many actions are performed by invoking a separate binary which isolates any crashes. When miscompares in results occur or the generated function crashes part of XLS, all artifacts generated by the fuzzer for that sample are written into a uniquely-named subdirectory under the --crash_path given in the command line. The fuzzer also writes a crasher file which is a single file for reproducing the issue. See below for instructions on debugging a failing sample.","title":"XLS Fuzzer"},{"location":"fuzzer/#crashers-directory","text":"The crashers directory includes a subdirectory created for each failing sample. To avoid collisions the subdirectory is named using a hash of the DSLX code. Each crasher subdirectory has the following contents: $ ls /tmp/crashers-2019-06-25/05adbd50 args.txt ir_converter_main.stderr run.sh cl.txt ir_minimizer.options.json sample.ir crasher_2020-04-23_9b05.x ir_minimizer_test.sh sample.ir.results eval_ir_main.stderr options.json sample.x exception.txt opt_main.stderr sample.x.results The directory includes the problematic DSLX sample ( sample.x ) and the input arguments ( args.txt ) as well as all artifacts generated and stderr output emitted by the various utilities invoked to test the sample. Notable files include: options.json : Options used to run the sample. sample.ir : Unoptimized IR generated from the DSLX sample. sample.opt.ir : IR after optimizations. sample.v : Generated Verilog. *.results : The results (numeric values) produced by interpreting or simulating the respective input (DSLX, IR, or Verilog). exception.txt : The exception raised when running the sample. Typically this will indicate either a result miscomparison or a tool return non-zero status (for example, the IR optimizer crashed). crasher_*.x : A single file reproducer which includes the DSLX code, arguments, and options. See below for details. Typically the exact nature of the failure can be identified by reading the file exception.txt and possibly the stderr outputs of the various tools. The fuzzer can optionally produce a minimized IR reproduction of the problem. This will be written to minimized.ir . See below for details.","title":"Crashers Directory"},{"location":"fuzzer/#single-file-reproducers-reproducers","text":"When the fuzzer encounters an issue it will create a single-file reproducer: --- Worker 14 observed an exception, noting --- Worker 14 noted crasher #1 for sampleno 42 at /tmp/crashers/095fb405 Copying that file to the directory //xls/dslx/fuzzer/crashers will automatically create a bazel test target for it and add it to the regression suite. Tests can also be added as known failures in //xls/dslx/fuzzer/build_defs.bzl as they're being triaged / investigated like so: generate_crasher_regression_tests( srcs = glob([\"crashers/*\"]), prefix = \"xls/dslx/fuzzer\", # TODO(xls-team): 2019-06-30 Triage and fix these. failing = [ \"crashers/crasher_2019-06-29_129987.x\", \"crashers/crasher_2019-06-29_402110.x\", ], ) Known-failures are marked as manual and excluded from continuous testing. To run the regression suite: bazel test //xls/dslx/fuzzer:all To run the regression suite including known-failures, run the regression target directly: bazel test //xls/dslx/fuzzer:regression_tests To reproduce from that single-file reproducer there is a command line tool: bazel run //xls/dslx/fuzzer/run_crasher -- \\ crasher_2019-06-26_3354.x","title":"Single-file reproducers {#reproducers}"},{"location":"fuzzer/#ir-minimization-minimization","text":"By default the fuzzer attempts to generate a minimal IR reproducer for the problem identified by the DSLX sample. Starting with the unoptimized IR the fuzzer invokes ir_minimizer_main to reduce the size of the input IR. It uses various simplification strategies to minimize the number of nodes in the IR. See the usage description in the tool source code for detailed information. The minimized IR is written to a file minimized.ir in the crasher directory for the sample. Note that minimization is only possible if the problem (crash, result miscomparison, etc.) occurs after conversion from DSLX to XLS IR.","title":"IR minimization {#minimization}"},{"location":"fuzzer/#summaries","text":"To monitor progress of the fuzzer and to determine op coverage the fuzzer can optionally (with --summary_path ) write summary information to files. The summary files are Protobuf files containing the proto SampleSummaryProto defined in //xls/dslx/fuzzer/sample_summary.proto . The summary information about the IR generated from the DSLX sample such as the number and type of each IR op as well as the bit width and number of operands. The summaries can be read with the tool //xls/dslx/fuzzer/read_summary_main . See usage description in the code for more details.","title":"Summaries"},{"location":"fuzzer/#debugging-a-failing-sample-debugging","text":"A generated sample can fail in one of two ways: a tool crash or a result miscomparison. A tool crash occurred if one of the tools invoked by the fuzzer (e.g., opt_main which optimizes the IR) returned a non-zero status. A result miscomparison occurred if there is not perfect correspondence between the results produced by various ways in which the generated function is evaluated: Interpreted DSLX Evaluated unoptimized IR Evaluated optimized IR Simulation of the generated (System)Verilog Generally, the results produced by the interpretation of the DSLX serves are the reference results for comparisons. To identify the underlying cause of the sample failure inspect the exception.txt file in the crasher directory. The file contains the text of the exception raised in SampleRunner which clearly identifies the kind of failure (result miscomparison or tool crash) and details about which evaluation resulted in a miscompare or which tool crashed, respectively. Consult the following sections on how to debug particular kinds of failures.","title":"Debugging a failing sample {#debugging}"},{"location":"fuzzer/#debugging-a-tool-crash","text":"The exception.txt file includes the invocation of the tool for reproducing the failure. Generally, this is a straightforward debugging process. If the failing tool is the IR optimizer binary opt_main the particular pass causing the failure should be in the backtrace. To retrieve the input to this pass, run opt_main with --ir_dump_path to dump the IR between each pass. The last IR file produced (the files are numbered sequentially) is the input to the failing pass.","title":"Debugging a tool crash"},{"location":"fuzzer/#result-miscomparison-unoptimized-ir","text":"The evaluation of the unoptimized IR is the first point at which result comparison occurs (DSLX interpretation versus unoptimized IR evaluation). A miscomparison here can indicate a bug in one of several places: DSLX interpreter DSLX to IR conversion IR interpreter or IR JIT. The error message in exception.txt indicates whether the JIT or the interpreter was used. To help narrow this down, the IR interpreter can be compared against the JIT with the eval_ir_main tool: eval_ir_main --test_llvm_jit --input_file=args.txt sample.ir This runs both the JIT and the interpreter on the unoptimized IR file ( sample.ir ) using the arguments in args.txt and compares the results. If this is successful, then likely the IR interpreter and the JIT are correct and problem lies earlier in the pipeline (DSLX interpretation or DSLX to IR conversion). Otherwise, there is definitely a bug in either the interpreter or the JIT as their results should always be equal. If a minimized IR file exists ( minimized.ir ) this may be a better starting point for isolating the failure.","title":"Result miscomparison: unoptimized IR"},{"location":"fuzzer/#result-miscomparison-optimized-ir","text":"This can indicate a bug in IR evaluation (interpreter or JIT) or in the optimizer. In this case, a comparison of the evaluation of the unoptimized IR against the DSLX interpreter has already succeeds so DSLX interpretation or conversion is unlikely to be the underlying cause. As with miscomparison involving the unoptimized IR, eval_ir_main can be used to compare the JIT results against the interpreter results: eval_ir_main --test_llvm_jit --input_file=args.txt sample.opt.ir If the above invocation fails there is a bug in the JIT or the interpreter. Otherwise, there may be a bug in the optimizer. The tool eval_ir_main can help isolate the problematic optimization pass by running with the options --optimize_ir and --eval_after_each_pass . With these flags, the tool runs the optimization pipeline on the given IR and evaluates the IR after each pass is run. The first pass which results in a miscompare against the unoptimized input IR is flagged. Invocation: eval_ir_main --input_file=args.txt \\ --optimize_ir \\ --eval_after_each_pass \\ sample.ir","title":"Result miscomparison: optimized IR"},{"location":"fuzzer/#debugging-the-llvm-jit","text":"To help isolate bugs in the JIT, LLVM's optimization level can be set using the --llvm_opt_level flag: eval_ir_main --test_llvm_jit \\ --llvm_opt_level=0 \\ --input_file=args.txt sample.opt.ir If the results match (pass) with the optimization level set to zero but fail with the default optimization level of 3, there is likely a bug in the LLVM optimizer or the XLS-generated LLVM program has undefined behavior. Unoptimized and optimized LLVM IR are dumped by the JIT with vlog level of 2 or higher. The assembly at level 3 or higher. For example: eval_ir_main -v=3 --logtostderr sample.opt.ir The LLVM tool lli evaluates LLVM IR. The tool expects the IR to include a entry function main . See the uploaded file in this LLVM bug for an example LLVM IR file which includes a main function that calls a (slightly modified) XLS-generated function. The LLVM tool opt optimizes the LLVM IR and can be piped to lli like so: opt sample.ll --O2 | lli","title":"Debugging the LLVM JIT"},{"location":"fuzzer/#result-miscomparison-simulated-verilog","text":"This can be a bug in codegen, XLS's Verilog testbench code, or the Verilog simulator itself. Running the generated Verilog with different simulators can help isolate the problem: simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=iverilog \\ sample.v simulate_module_main --signature_file=module_sig.textproto \\ --args_file=args.txt \\ --verilog_simulator=${SIM_2} \\ sample.v The tool outputs the results of the evaluation to stdout so diffing their outputs is required.","title":"Result miscomparison: simulated Verilog"},{"location":"interpreters/","text":"Interpreters XLS provides a several interpreters to assist in design validation across our functional stack, from input DSLX down to the netlist level. Interpreters DSLX IR Netlists DSLX The DSLX interpreter operates on DSLX .x files that contain both the design and samples to execute (present as 'tests'). The adler32 example demonstrates this: the design is encapsulated in the main , adler32_seq , and mod functions, and the samples are present in the test adler32_one_char (note that unit-style tests/interpretations of adler32_seq and mod could also be present). Interpreter targets are automatically generated for dslx_test() targets, so no special declarations are necessary to wrap DSLX code. To invoke these samples, execute the following: bazel build -c opt //xls/examples:adler32_dslx_test ./bazel-bin/xls/examples/adler32_dslx_test To execute directly via the interpreter, you can instead run: bazel build -c opt //xls/dslx/interpreter:interpreter_main ./bazel-bin/xls/dslx/interpreter/interpreter_main \\ xls/examples/adler32.x These two methods are equivalent. IR XLS provides two means of evaluating IR - interpretation and native host compilation (the JIT ). Both are invoked in nearly the same way, via the eval_ir_main tool. eval_ir_main supports a wide number of use cases, but the most common end-user case will be to run a sample through a design. To evaluate a sample (1.0 + 2.5) on the floating-point adder , one would run the following: bazel build -c opt //xls/tools:eval_ir_main ./bazel-bin/xls/tools/eval_ir_main \\ --input '(bits[1]: 0x0, bits[8]:0x7F, bits[23]:0x0); (bits[1]: 0x0, bits[8]:0x80, bits[23]:0x200000)' \\ xls/modules/fpadd_2x32.x By default, this runs via the JIT. To use the interpreter, add the --use_llvm_jit=false flag to the invocation. eval_ir_main supports a broad set of options and modes of execution. Refer to its [very thorough] --help documentation for full details. Netlists Finally, compiled netlists can also be interpreted against input samples via the aptly-named netlist_interpreter_main tool. This tool currently only supports single sample evaluation (as illustrated in the IR section above): bazel build -c opt //xls/tools:netlist_interpreter_main ./bazel-bin/xls/tools/netlist_interpreter_main \\ --netlist <path to netlist> --module <module to evaluate> --cell_library[_proto] <path to the module's cell library [proto]> --inputs <input sample, as above> As XLS does not currently provide an sample/example netlist (TODO(rspringer)), concrete values can't [yet] be provided here. The --cell_library flag merits extra discussion, though. During netlist compilation, a cell library is provided to indicate the individual logic cells available for the design, and these cells are referenced in the output netlist. The interpreter needs a description of these cells' behaviors/functions, so the cell library must be provided here, as well. Many cell libraries are very large (> 1GB), and can thus incur significant processing overhead at startup, so we also accept pre-processed cell libraries, as CellLibraryProto messages, that contain much-abridged cell descriptions. The function_extractor_main tool can automatically perform this extraction for Liberty -formatted cell library descriptions.","title":"Interpreters"},{"location":"interpreters/#interpreters","text":"XLS provides a several interpreters to assist in design validation across our functional stack, from input DSLX down to the netlist level. Interpreters DSLX IR Netlists","title":"Interpreters"},{"location":"interpreters/#dslx","text":"The DSLX interpreter operates on DSLX .x files that contain both the design and samples to execute (present as 'tests'). The adler32 example demonstrates this: the design is encapsulated in the main , adler32_seq , and mod functions, and the samples are present in the test adler32_one_char (note that unit-style tests/interpretations of adler32_seq and mod could also be present). Interpreter targets are automatically generated for dslx_test() targets, so no special declarations are necessary to wrap DSLX code. To invoke these samples, execute the following: bazel build -c opt //xls/examples:adler32_dslx_test ./bazel-bin/xls/examples/adler32_dslx_test To execute directly via the interpreter, you can instead run: bazel build -c opt //xls/dslx/interpreter:interpreter_main ./bazel-bin/xls/dslx/interpreter/interpreter_main \\ xls/examples/adler32.x These two methods are equivalent.","title":"DSLX"},{"location":"interpreters/#ir","text":"XLS provides two means of evaluating IR - interpretation and native host compilation (the JIT ). Both are invoked in nearly the same way, via the eval_ir_main tool. eval_ir_main supports a wide number of use cases, but the most common end-user case will be to run a sample through a design. To evaluate a sample (1.0 + 2.5) on the floating-point adder , one would run the following: bazel build -c opt //xls/tools:eval_ir_main ./bazel-bin/xls/tools/eval_ir_main \\ --input '(bits[1]: 0x0, bits[8]:0x7F, bits[23]:0x0); (bits[1]: 0x0, bits[8]:0x80, bits[23]:0x200000)' \\ xls/modules/fpadd_2x32.x By default, this runs via the JIT. To use the interpreter, add the --use_llvm_jit=false flag to the invocation. eval_ir_main supports a broad set of options and modes of execution. Refer to its [very thorough] --help documentation for full details.","title":"IR"},{"location":"interpreters/#netlists","text":"Finally, compiled netlists can also be interpreted against input samples via the aptly-named netlist_interpreter_main tool. This tool currently only supports single sample evaluation (as illustrated in the IR section above): bazel build -c opt //xls/tools:netlist_interpreter_main ./bazel-bin/xls/tools/netlist_interpreter_main \\ --netlist <path to netlist> --module <module to evaluate> --cell_library[_proto] <path to the module's cell library [proto]> --inputs <input sample, as above> As XLS does not currently provide an sample/example netlist (TODO(rspringer)), concrete values can't [yet] be provided here. The --cell_library flag merits extra discussion, though. During netlist compilation, a cell library is provided to indicate the individual logic cells available for the design, and these cells are referenced in the output netlist. The interpreter needs a description of these cells' behaviors/functions, so the cell library must be provided here, as well. Many cell libraries are very large (> 1GB), and can thus incur significant processing overhead at startup, so we also accept pre-processed cell libraries, as CellLibraryProto messages, that contain much-abridged cell descriptions. The function_extractor_main tool can automatically perform this extraction for Liberty -formatted cell library descriptions.","title":"Netlists"},{"location":"ir_jit/","text":"IR JIT Compiler IR JIT Compiler Usage Design Arg passing ArrayIndex main() generator Usage Design Output type determination XLS provides a JIT compiler for evaluating functions written in the [XLS] compiler intermediate representation (IR) at native machine speed. Usage The IR JIT is the default backend for the eval_ir_main tool, which loads IR from disk and runs with args present on either the command line or in a specified file. For programmatic usage, the JIT is available as a library with a straightforward interface: xabsl::StatusOr<Value> RunOnJit( Function* function, absl::Span<const Value> args) { XLS_ASSIGN_OR_RETURN(auto jit, LlvmIrJit::Create(function)); return jit->Run(args); } The advantages of JIT compilation (or any compilation, for that matter) only come into play when repeatedly using the compiled object, so programs should be structured to compile a function once and to reuse it many times, e.g., to test a module across many - or even exhaustively, across all possible - inputs. Design Internally, the JIT converts XLS IR to LLVM IR and uses LLVM's ORC infrastructure to convert that into native machine code. The details of compiling an LLVM IR program with ORC are mostly generic and are available online - here are discussed details specific to our usage in XLS. XLS IR is converted to LLVM IR by recursively visiting every node in a function using the DfsVisitor functions. Most nodes have relatively straightforward implementations, e.g., in Concat, we create an empty value with the combined width of the operands, and each is shifted and blitted into that value to produce the result. Some operations, though, merit more discussion. Arg passing When LLVM JIT-compiles a program, the resulting value is simply a function pointer to the requested entry point (that can be called like any function pointer). Calling such a function pointer with concrete-typed arguments, though, is difficult: one must either make heavy [ab]use of C++ templates or \"hide\" the argument types behind an opaque pointer. The latter approach is taken here. When a compiled function is invoked (via LlvmIrJit::Run() ), the typed input args are \"packed\" into an opaque byte buffer which is passed into the new function. Inside there, any references to an argument (via DfsVisitor::HandleParam() ) calculate the offset of that param in the opaque buffer and load from there appropriately (this should happen at most once per arg; LLVM and/or XLS should optimize away redundant loads). A special case is for function invocations (inside the JITted function): for these, the arguments already exist inside \"LLVM-space\", so there's no need for unpacking args, so LlvmFunction::getArg() can be used as usual. Results must be handled in a similar way - they could be of any type and will need to be packed inside XLS types before returning, so there's a corresponding argument unpacking phase at function exit. For both packing and unpacking, LLVM's DataLayout must be used to determining where input and output values will be placed, as LLVM will use those conventions when, e.g., loading values from a struct. ArrayIndex IRBuilder provides three means of extracting values from an aggregate type: CreateGEP : these use the getelementptr instruction, which requires a pointer-typed value (not the same thing as an array!). This requires holding a value in a specially-created allocation (via CreateAlloca() or in an input buffer). CreateExtractElement : returns the value at a given index in a vector -typed value. CreateExtractValue : returns the value at a given constant index in an aggregate value. Unfortunately, #2 doesn't apply, as arrays aren't LLVM vectors, and #3 doesn't apply, as an array index isn't necessarily a constant value. Uniformly managing arrays as allocas doesn't scale well (consider the case of arrays of arrays of tuples...), so for ArrayIndex nodes, we lazily create allocas for only the array of interest and load the requested index from there. main() generator The IR JIT finds more than its share of LLVM bugs, in large part due to XLS' use of fuzzing, which often generates bit widths not often found in software, e.g. a 231-bit wide integer, which won't be emitted by Clang (as it's not a native C type). To ensure that any mismatches between the JIT and the IR interpreter are correctly triaged, it's important to compare results between the JIT and LLVM-provided tools, e.g., lli or llc, the LLVM IR interpreter and compiler, respectively. Both lli and llc execute self-contained LLVM IR programs, i.e., those with an int main(int argc, char** argv) entry point - the JIT does not produce these (as they're not part of a hardware description). To avoid the need to manually edit the JIT-produced IR (which experience has shown to be very error prone), we can generate a main driver function for our samples. Usage To generate and execute a main for LLVM IR produced by the JIT, run the following: $ bazel build //xls/tools:llvm_main_generator \\ //xls/tools:run_llvm_main $ ./bazel-bin/xls/tools/llvm_main_generator \\ -entry_function <Mangled IR function name> \\ -input <Path to file containing JIT-generated LLVM IR> \\ -output <Path to write output> $ ./bazel-bin/xls/tools/run_llvm_main \\ <Output path from above> \\ <Input values as string-formatted XLS Values> For a concrete example: $ bazel-bin/xls/tools/llvm_main_generator \\ -entry_function \"sample::__sample__main\" \\ -input ~/fuzz/mismatch/sample.opt.ll \\ -output ./foo.ll $ bazel-bin/xls/tools/run_llvm_main \\ ./foo.ll \\ bits[56]:0x800_0000_0000 \\ bits[66]:0x4000_0000 \\ bits[7]:0x1 \\ bits[2]:0x3 \\ bits[12]:0x0 \\ bits[74]:0x3c5_482a_0984_e061_1a90 bits[74]:0x3ff_ffc5_482a_0984_e061 The final line is the result of running the sample. If the value mismatch occurs both in the IR JIT evaluation as well as lli evaluation, then there's likely a bug in LLVM. A main-annotated IR sample is suitable for attaching to an LLVM bug report (on http://bugs.llvm.org) as a reproducer, along with the input to generate the mismatch. Design The main generator, at a high level, uses the LLVM tools (namely IRBuilder) to create a main() function to: Examine its command-line parameters to determine their overall size and to them into the input buffer (as in Arg Passing above). Invoke the entry function (which remains unchanged from the source emitted by the JIT). Unpack the output buffer after the entry function completes and print its contents. For arg packing/unpacking, the functions provided by LlvmIrRuntime are used, but in a stateless context wrapped in an extern \"C\" space, to simplify invocation from within LLVM IR. Output type determination Inferring the computation's output type merits special discussion. While it's trivial for the main generator to detect the output type, it's very difficult to do so inside the executing main() - in the former, the type is real data, wherein the latter, it's metadata. To make this possible (without doing down an RTTI rabbit hole, or even simply requiring the user to specify it on the command line), we do the following: Determine the llvm::Type of the output. Convert that into an xls::Type , and capture that type as a String. Hardcode that string as a constant in the emitted main() function. At runtime, pass that type string into UnpackAndPrintBuffer() (one of the extern \"C\" function wrappers). Inside UnpackAndPrintBuffer() , parse that string (via xls::Parser::ParseType() ), and use the resulting xls::Type to determine the contents of the computation's output buffer.","title":"Overview"},{"location":"ir_jit/#ir-jit-compiler","text":"IR JIT Compiler Usage Design Arg passing ArrayIndex main() generator Usage Design Output type determination XLS provides a JIT compiler for evaluating functions written in the [XLS] compiler intermediate representation (IR) at native machine speed.","title":"IR JIT Compiler"},{"location":"ir_jit/#usage","text":"The IR JIT is the default backend for the eval_ir_main tool, which loads IR from disk and runs with args present on either the command line or in a specified file. For programmatic usage, the JIT is available as a library with a straightforward interface: xabsl::StatusOr<Value> RunOnJit( Function* function, absl::Span<const Value> args) { XLS_ASSIGN_OR_RETURN(auto jit, LlvmIrJit::Create(function)); return jit->Run(args); } The advantages of JIT compilation (or any compilation, for that matter) only come into play when repeatedly using the compiled object, so programs should be structured to compile a function once and to reuse it many times, e.g., to test a module across many - or even exhaustively, across all possible - inputs.","title":"Usage"},{"location":"ir_jit/#design","text":"Internally, the JIT converts XLS IR to LLVM IR and uses LLVM's ORC infrastructure to convert that into native machine code. The details of compiling an LLVM IR program with ORC are mostly generic and are available online - here are discussed details specific to our usage in XLS. XLS IR is converted to LLVM IR by recursively visiting every node in a function using the DfsVisitor functions. Most nodes have relatively straightforward implementations, e.g., in Concat, we create an empty value with the combined width of the operands, and each is shifted and blitted into that value to produce the result. Some operations, though, merit more discussion.","title":"Design"},{"location":"ir_jit/#arg-passing","text":"When LLVM JIT-compiles a program, the resulting value is simply a function pointer to the requested entry point (that can be called like any function pointer). Calling such a function pointer with concrete-typed arguments, though, is difficult: one must either make heavy [ab]use of C++ templates or \"hide\" the argument types behind an opaque pointer. The latter approach is taken here. When a compiled function is invoked (via LlvmIrJit::Run() ), the typed input args are \"packed\" into an opaque byte buffer which is passed into the new function. Inside there, any references to an argument (via DfsVisitor::HandleParam() ) calculate the offset of that param in the opaque buffer and load from there appropriately (this should happen at most once per arg; LLVM and/or XLS should optimize away redundant loads). A special case is for function invocations (inside the JITted function): for these, the arguments already exist inside \"LLVM-space\", so there's no need for unpacking args, so LlvmFunction::getArg() can be used as usual. Results must be handled in a similar way - they could be of any type and will need to be packed inside XLS types before returning, so there's a corresponding argument unpacking phase at function exit. For both packing and unpacking, LLVM's DataLayout must be used to determining where input and output values will be placed, as LLVM will use those conventions when, e.g., loading values from a struct.","title":"Arg passing"},{"location":"ir_jit/#arrayindex","text":"IRBuilder provides three means of extracting values from an aggregate type: CreateGEP : these use the getelementptr instruction, which requires a pointer-typed value (not the same thing as an array!). This requires holding a value in a specially-created allocation (via CreateAlloca() or in an input buffer). CreateExtractElement : returns the value at a given index in a vector -typed value. CreateExtractValue : returns the value at a given constant index in an aggregate value. Unfortunately, #2 doesn't apply, as arrays aren't LLVM vectors, and #3 doesn't apply, as an array index isn't necessarily a constant value. Uniformly managing arrays as allocas doesn't scale well (consider the case of arrays of arrays of tuples...), so for ArrayIndex nodes, we lazily create allocas for only the array of interest and load the requested index from there.","title":"ArrayIndex"},{"location":"ir_jit/#main-generator","text":"The IR JIT finds more than its share of LLVM bugs, in large part due to XLS' use of fuzzing, which often generates bit widths not often found in software, e.g. a 231-bit wide integer, which won't be emitted by Clang (as it's not a native C type). To ensure that any mismatches between the JIT and the IR interpreter are correctly triaged, it's important to compare results between the JIT and LLVM-provided tools, e.g., lli or llc, the LLVM IR interpreter and compiler, respectively. Both lli and llc execute self-contained LLVM IR programs, i.e., those with an int main(int argc, char** argv) entry point - the JIT does not produce these (as they're not part of a hardware description). To avoid the need to manually edit the JIT-produced IR (which experience has shown to be very error prone), we can generate a main driver function for our samples.","title":"main() generator"},{"location":"ir_jit/#usage_1","text":"To generate and execute a main for LLVM IR produced by the JIT, run the following: $ bazel build //xls/tools:llvm_main_generator \\ //xls/tools:run_llvm_main $ ./bazel-bin/xls/tools/llvm_main_generator \\ -entry_function <Mangled IR function name> \\ -input <Path to file containing JIT-generated LLVM IR> \\ -output <Path to write output> $ ./bazel-bin/xls/tools/run_llvm_main \\ <Output path from above> \\ <Input values as string-formatted XLS Values> For a concrete example: $ bazel-bin/xls/tools/llvm_main_generator \\ -entry_function \"sample::__sample__main\" \\ -input ~/fuzz/mismatch/sample.opt.ll \\ -output ./foo.ll $ bazel-bin/xls/tools/run_llvm_main \\ ./foo.ll \\ bits[56]:0x800_0000_0000 \\ bits[66]:0x4000_0000 \\ bits[7]:0x1 \\ bits[2]:0x3 \\ bits[12]:0x0 \\ bits[74]:0x3c5_482a_0984_e061_1a90 bits[74]:0x3ff_ffc5_482a_0984_e061 The final line is the result of running the sample. If the value mismatch occurs both in the IR JIT evaluation as well as lli evaluation, then there's likely a bug in LLVM. A main-annotated IR sample is suitable for attaching to an LLVM bug report (on http://bugs.llvm.org) as a reproducer, along with the input to generate the mismatch.","title":"Usage"},{"location":"ir_jit/#design_1","text":"The main generator, at a high level, uses the LLVM tools (namely IRBuilder) to create a main() function to: Examine its command-line parameters to determine their overall size and to them into the input buffer (as in Arg Passing above). Invoke the entry function (which remains unchanged from the source emitted by the JIT). Unpack the output buffer after the entry function completes and print its contents. For arg packing/unpacking, the functions provided by LlvmIrRuntime are used, but in a stateless context wrapped in an extern \"C\" space, to simplify invocation from within LLVM IR.","title":"Design"},{"location":"ir_jit/#output-type-determination","text":"Inferring the computation's output type merits special discussion. While it's trivial for the main generator to detect the output type, it's very difficult to do so inside the executing main() - in the former, the type is real data, wherein the latter, it's metadata. To make this possible (without doing down an RTTI rabbit hole, or even simply requiring the user to specify it on the command line), we do the following: Determine the llvm::Type of the output. Convert that into an xls::Type , and capture that type as a String. Hardcode that string as a constant in the emitted main() function. At runtime, pass that type string into UnpackAndPrintBuffer() (one of the extern \"C\" function wrappers). Inside UnpackAndPrintBuffer() , parse that string (via xls::Parser::ParseType() ), and use the resulting xls::Type to determine the contents of the computation's output buffer.","title":"Output type determination"},{"location":"ir_semantics/","text":"XLS: IR semantics XLS: IR semantics Data types Bits Array Tuple Token Operations Unary bitwise operations Variadic bitwise operations Arithmetic unary operations Arithmetic binary operations Comparison operations Shift operations Extension operations zero_ext sign_ext Channel operations after_all Miscellaneous operations array array_index array_update bit_slice concat counted_for decode encode invoke map one_hot one_hot_sel param reverse sel tuple tuple_index The XLS IR is a pure dataflow-oriented IR that has the static-single-assignment property, but is specialized for generating circuitry. Notably, it includes high level parallel patterns. The aim is to create effective circuit designs through a \"lifted\" understanding of the high-level operations and their semantics, instead of trying to reverse all relevant properties via dependence analysis, which often cannot take advantage of high level knowledge that the designer holds in their mind at design time. This document describes the semantics of the XLS intermediate representation (IR) including data types, operations, and textual representation. Data types Bits A vector of bits with a fixed width. Type syntax: bits[N] where N is the number of bits. Value syntax: A literal decimal number. Example: 42 . A binary number prefixed with 0b . Example: 0b10101 A hexadecimal number: 0x . Example: 0xdeadbeef The representation may optionally include the bit width in which case the type is prefixed before the literal: bits[N]:$literal . Example: bits[8]:0xab . Array A one-dimensional array of elements of the same type with a fixed number of elements. An array can contain bits, arrays, or tuples as elements. May be empty (in which case the type of the array element cannot be automatically deduced). Type syntax: $type[N] : an array containing N elements of type $type . Examples: Two-element array of 8-bit bits type: bits[8][2] Three-element array of tuple type: (bits[32], bits[2])[3] Value syntax: [$value_1, ... , $value_N] where $value_n is the value of the n -th element. Examples: Array of bits elements with explicit bit count: [bits[8]:10, bits[8]:30] Three-element array consisting of two-element arrays of bits elements: [[1, 2], [3, 4], [5, 6]]] Tuple An ordered set of fixed size containing elements with potentially different types. tuples can contain bits, arrays, or tuples as elements. May be empty. Type syntax: ($type_{0}, ..., $type_{N-1}) where N is the number of elements and where $type_n is the type of the n -th element. Value syntax: ($value_{0}, ..., $value_{N-1}) where $value_n is the value of the n -th element. Examples: Tuple containing two bits elements: (0b100, 0b101) A nested tuple containing various element types: ((1, 2), 42, [5, 6]) Token A type used to enforce ordering between channel operations. The token type has no value and all tokens are identical. A token is purely symbolic / semantic and has no correlate in hardware. Type syntax: token Operations Operations share a common syntax and have both positional and keyword arguments \u00e0 la Python. Positional arguments are ordered and must appear first in the argument list. Positional arguments are exclusively the identifiers of the operands. Keyword arguments are unordered and must appear after the positional arguments. Keyword arguments can include arbitrary value types. result = operation(pos_arg_0, ..., pos_arg_N, keyword_0=value0, ..., keyword_M=valueM, ...) Common keyword arguments Keyword Type Required Default Description pos SourceLocation no The source location : : : : : associated with this : : : : : : operation. The syntax : : : : : : is a triplet of : : : : : : comma-separated : : : : : : integer values\\: : : : : : : Fileno,Lineno,Colno : Unary bitwise operations Performs a bit-wise operation on a single bits-typed operand. Syntax result = identity(operand) result = not(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics identity Op::kIdentity result = operand not Op::kNot result = ~operand Variadic bitwise operations Performs a bit-wise operation on one-or-more identically-typed bits operands. If only a single argument is provided the operation is a no-op. Syntax result = and(operand_{0}, ..., operand_{N-1}) result = or(operand_{0}, ..., operand_{N-1}) result = xor(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} bits[N] result bits[N] Operations Operation Opcode Semantics and Op::kAnd result = lhs & rhs & ... or Op::kOr result = lhs \\| rhs \\| ... xor Op::kXor result = lhs ^ rhs ^ ... Arithmetic unary operations Performs an arithmetic operation on a single bits-typed operand. Syntax result = neg(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics neg Op::kNeg result = -operand Arithmetic binary operations Performs an arithmetic operation on a pair of bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Syntax result = add(lhs, rhs) result = smul(lhs, rhs) result = umul(lhs, rhs) result = sdiv(lhs, rhs) result = sub(lhs, rhs) result = udiv(lhs, rhs) Types Currently signed and unsigned multiply support arbitrary width operands and result. For all other arithmetic operations the operands and the result are the same width. The expectation is that all arithmetic operations will eventually support arbitrary widths. Operations Operation Opcode Semantics add Op::kAnd result = lhs + rhs sdiv Op::kSDiv result = $signed(lhs) / $signed(rhs) * smul Op::kSMul result = $signed(lhs) * $signed(rhs) sub Op::kSub result = lhs - rhs udiv Op::kUDiv result = lhs / rhs * umul Op::kUMul result = lhs * rhs * Synthesizing division can lead to failing synthesis and/or problems with timing closure. It is usually best not to rely on this Verilog operator in practice, but instead explicitly instantiate a divider of choice. Comparison operations Performs a comparison on a pair of identically-typed bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Produces a result of bits[1] type. Syntax result = eq(lhs, rhs) result = ne(lhs, rhs) result = sge(lhs, rhs) result = sgt(lhs, rhs) result = sle(lhs, rhs) result = slt(lhs, rhs) result = uge(lhs, rhs) result = ugt(lhs, rhs) result = ule(lhs, rhs) result = ult(lhs, rhs) Types Value Type lhs bits[N] rhs bits[N] result bits[1] Operations Operation Opcode Semantics eq Op::kEq result = lhs == rhs ne Op::kNe result = lhs != rhs sge Op::kSGe result = lhs >= rhs sgt Op::kSGt result = lhs > rhs sle Op::kSLe result = lhs <= rhs slt Op::kSLt result = lhs < rhs uge Op::kUGe result = lhs >= rhs ugt Op::kUGt result = lhs > rhs ule Op::kULe result = lhs <= rhs ult Op::kULt result = lhs < rhs Shift operations Performs an shift operation on an input operand where the shift amount is specified by a second operand. Syntax result = shll(operand, amount) result = shra(operand, amount) result = shrl(operand, amount) Types The shifted operand and the result of the shift are the same width. Widths of the shift amount may be arbitrary. Operations Operation Opcode Semantics shll Op::kShll result = lhs << rhs * shra Op::kShra result = lhs >>> rhs (arithmetic shift right) ** shrl Op::kShra result = lhs >> rhs * * Logically shifting greater than or equal to the number of bits in the lhs produces a result of zero. ** Arithmetic right shifting greater than or equal to the number of bits in the lhs produces a result equal to all of the bits set to the sign of the lhs . Extension operations Extends a bit value to a new (larger) target bit-length. Syntax result = zero_ext(x, new_bit_count=42) result = sign_ext(x, new_bit_count=42) Types Value Type arg bits[N] new_bit_count int64 result bits[new_bit_count] Note: new_bit_count should be >= N or an error may be raised. zero_ext Zero-extends a value: turns its bit-length into the new target bit-length by filling zeroes in the most significant bits. sign_ext Sign-extends a value: turns its bit-length into the new target bit-length by filling in the most significant bits (MSbs) with the following policy: ones in the MSbs if the MSb of the original value was set, or zeros in the MSbs if the MSb of the original value was unset. Channel operations after_all Used to construct partial orderings among channel operations. result = after_all(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} token result token after_all can consume an arbitrary number of token operands including zero. Miscellaneous operations array Constructs a array of its operands. result = array(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T result T[N] Array can take an arbitrary number of operands including zero (which produces an empty array). array_index Returns a single element from an array. Syntax result = array_index(data, index) Types Value Type lhs Array of elements of type T rhs bits[M] result T Returns the element at the index given by operand index from the array data . TODO: Define out of bounds semantics for array_index. array_update Returns a modified copy of an array. Syntax result = array_update(array, index, value) Types Value Type array Array of elements of type T index bits[M] * value T result Array of elements of type T * M is arbitrary. Returns a copy of the input array with the element at the index replaced with the given value. If index is out of bounds, the returned array is identical to the input array. bit_slice Slices a a contiguous range of bits from a bits-typed operand. Syntax result = bit_slice(operand, start=<start>, width=<width>) Types Value Type operand bits[N] result bits[<width>] Keyword arguments Keyword Type Required Default Description start int64 yes The starting bit : : : : : of the slice. : : : : : : start is is : : : : : : zero-indexed : : : : : : where zero is the : : : : : : least-significant : : : : : : bit of the : : : : : : operand. : width int64 yes The width of the : : : : : slice. : The bit-width of operand must be greater than or equal to <start> plus <width> . concat Concatenates and arbitrary number of bits-typed operands. result = concat(operand{0}, ..., operand{n-1}) Types Value Type operand_{i} bits[N_{i}] result bits[Sum(N_{i})] This is equivalent to the verilog concat operator: result = {arg0, ..., argN} counted_for Invokes a fixed-trip count loop. Syntax result = counted_for(init, trip_count=<trip_count>, stride=<stride>, body=<body>, invariant_args=<inv_args>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description trip_count int64 yes Trip count of the loop : : : : : (number of times that the : : : : : : loop body will be : : : : : : executed) : stride int64 no 1 Stride of the induction : : : : : variable : invariant_args array of yes Names of the invariant : : operands : : : operands as the loop body : body string yes Name of the function to : : : : : use as the loop body : counted_for invokes the function body trip_count times, passing loop-carried data that starts with value init . The first argument passed to body is the induction variable -- presently, the induction variable always starts at zero and increments by stride after every trip. The second argument passed to body is the loop-carry data. The return type of body must be the same as the type of the init loop carry data. The value returned from the last trip is the result of the counted_for expression. All subsequent arguments passed to body are passed from invariant_args ; e.g. if there are two members in invariant_args those values are passed as the third and fourth arguments. Therefore body should have a signature that matches the following: body(i, loop_carry_data[, invariant_arg0, invariant_arg1, ...]) Note that we currently inspect the body function to see what type of induction variable ( i above) it accepts in order to pass an i value of that type. decode Implements a binary decoder. result = decode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be less than or equal to 2** N where N is the operand width. Keyword arguments Keyword Type Required Default Description width int64 yes Width of the result decode converts the binary-encoded operand value into a one-hot result. For an operand value of n interpreted as an unsigned number the n -th result bit and only the n -th result bit is set. The width of the decode operation may be less than the maximum value expressible by the input (2** N - 1). If the encoded operand value is larger than the number of bits of the result the result is zero. encode Implements a binary encoder. result = encode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be equal to $$\\lceil \\log_{2} N \\rceil$$. encode converts the one-hot operand value into a binary-encoded value of the \"hot\" bit of the input. If the n -th bit and only the n -th bit of the operand is set the result is equal the value n as an unsigned number. If multiple bits of the input are set the result is equal to the logical or of the results produced by the input bits individually. For example, if bit 3 and bit 5 of an encode input are set the result is equal to 3 | 5 = 7. If no bits of the input are set the result is zero. invoke Invokes a function. Syntax result = invoke(operand_{0}, ... , operand_{N-1}, to_apply=<to_apply>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to use as : : : : : the loop body : TODO: finish map Applies a function to the elements of an array and returns the result as an array. Syntax result = map(operand, to_apply=<to_apply>) Types Value Type operand array[T] result array[U] Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to apply : : : : : to each element of the operand : TODO: finish one_hot Produces a bits value with exactly one bit set. The index of the set bit depends upon the input value. Syntax result = one_hot(input, lsb_prio=true) result = one_hot(input, lsb_prio=false) Types Value Type input bits[N] result bits[N+1] Keyword arguments Keyword Type Required Default Description lsb_prio bool yes Whether the least significant : : : : : bit (LSb) has priority. : For lsb_prio=true : result bit i for 0 <= i < N is set in result iff bit i is set in the input and all lower bits j for j < i are not set in the input. For lsb_prio=false : result bit i for N-1 >= i >= 0 is set in result iff bit i is set in the input and all higher (more significant) bits j for j > i are not set in the input. For both lsb_prio=true and lsb_prio=false , result bit N (the most significant bit in the output) is only set if no bits in the input are set. Examples: one_hot(0b0011, lsb_prio=true) => 0b00001 -- note that an extra MSb has been appended to the output to potentially represent the \"all zeros\" case. one_hot(0b0111, lsb_prio=false) => 0b00100 . one_hot(0b00, lsb_prio=false) => 0b100 . one_hot(0b00, lsb_prio=true) => 0b100 -- note the output for one_hot is the same for the all-zeros case regardless of whether lsb_prio is true or false. This operation is useful for constructing match or switch operation semantics where a condition is matched against an ordered set of cases and the first match is chosen. It is also useful for one-hot canonicalizing, e.g. as a prelude to counting leading/trailing zeros. one_hot_sel Selects between operands based on a one-hot selector. Syntax result = one_hot_sel(selector, cases=[case_{0}, ... , case_{N-1}]) Types Value Type selector bits[N] case_{i} T result T The result is the logical OR of all cases case_{i} for which the corresponding bit i is set in the selector. When selector is one-hot this performs a select operation. param TODO: finish reverse Reverses the order of bits of its operand. result = reverse(operand) Types Value Type operand bits[N] result bits[N] sel Selects between operands based on a selector value. Syntax result = sel(selector, cases=[case_{0}, ... , case_{N-1}], default=<default>) Types Value Type selector bits[M] case_{i} T default T result T tuple Constructs a tuple of its operands. result = tuple(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T_{i} result (T_{0}, ... , T_{N-1}) Tuple can take and arbitrary number of operands including zero (which produces an empty tuple). tuple_index Returns a single element from a tuple-typed operand. Syntax result = tuple_index(operand, index=<index>) Types Value Type operand (T_{0}, ... , T_{N-1}) result T_{<index>} Keyword arguments Keyword Type Required Default Description index int64 yes Index of tuple element to produce","title":"Semantics"},{"location":"ir_semantics/#xls-ir-semantics","text":"XLS: IR semantics Data types Bits Array Tuple Token Operations Unary bitwise operations Variadic bitwise operations Arithmetic unary operations Arithmetic binary operations Comparison operations Shift operations Extension operations zero_ext sign_ext Channel operations after_all Miscellaneous operations array array_index array_update bit_slice concat counted_for decode encode invoke map one_hot one_hot_sel param reverse sel tuple tuple_index The XLS IR is a pure dataflow-oriented IR that has the static-single-assignment property, but is specialized for generating circuitry. Notably, it includes high level parallel patterns. The aim is to create effective circuit designs through a \"lifted\" understanding of the high-level operations and their semantics, instead of trying to reverse all relevant properties via dependence analysis, which often cannot take advantage of high level knowledge that the designer holds in their mind at design time. This document describes the semantics of the XLS intermediate representation (IR) including data types, operations, and textual representation.","title":"XLS: IR semantics"},{"location":"ir_semantics/#data-types","text":"","title":"Data types"},{"location":"ir_semantics/#bits","text":"A vector of bits with a fixed width. Type syntax: bits[N] where N is the number of bits. Value syntax: A literal decimal number. Example: 42 . A binary number prefixed with 0b . Example: 0b10101 A hexadecimal number: 0x . Example: 0xdeadbeef The representation may optionally include the bit width in which case the type is prefixed before the literal: bits[N]:$literal . Example: bits[8]:0xab .","title":"Bits"},{"location":"ir_semantics/#array","text":"A one-dimensional array of elements of the same type with a fixed number of elements. An array can contain bits, arrays, or tuples as elements. May be empty (in which case the type of the array element cannot be automatically deduced). Type syntax: $type[N] : an array containing N elements of type $type . Examples: Two-element array of 8-bit bits type: bits[8][2] Three-element array of tuple type: (bits[32], bits[2])[3] Value syntax: [$value_1, ... , $value_N] where $value_n is the value of the n -th element. Examples: Array of bits elements with explicit bit count: [bits[8]:10, bits[8]:30] Three-element array consisting of two-element arrays of bits elements: [[1, 2], [3, 4], [5, 6]]]","title":"Array"},{"location":"ir_semantics/#tuple","text":"An ordered set of fixed size containing elements with potentially different types. tuples can contain bits, arrays, or tuples as elements. May be empty. Type syntax: ($type_{0}, ..., $type_{N-1}) where N is the number of elements and where $type_n is the type of the n -th element. Value syntax: ($value_{0}, ..., $value_{N-1}) where $value_n is the value of the n -th element. Examples: Tuple containing two bits elements: (0b100, 0b101) A nested tuple containing various element types: ((1, 2), 42, [5, 6])","title":"Tuple"},{"location":"ir_semantics/#token","text":"A type used to enforce ordering between channel operations. The token type has no value and all tokens are identical. A token is purely symbolic / semantic and has no correlate in hardware. Type syntax: token","title":"Token"},{"location":"ir_semantics/#operations","text":"Operations share a common syntax and have both positional and keyword arguments \u00e0 la Python. Positional arguments are ordered and must appear first in the argument list. Positional arguments are exclusively the identifiers of the operands. Keyword arguments are unordered and must appear after the positional arguments. Keyword arguments can include arbitrary value types. result = operation(pos_arg_0, ..., pos_arg_N, keyword_0=value0, ..., keyword_M=valueM, ...) Common keyword arguments Keyword Type Required Default Description pos SourceLocation no The source location : : : : : associated with this : : : : : : operation. The syntax : : : : : : is a triplet of : : : : : : comma-separated : : : : : : integer values\\: : : : : : : Fileno,Lineno,Colno :","title":"Operations"},{"location":"ir_semantics/#unary-bitwise-operations","text":"Performs a bit-wise operation on a single bits-typed operand. Syntax result = identity(operand) result = not(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics identity Op::kIdentity result = operand not Op::kNot result = ~operand","title":"Unary bitwise operations"},{"location":"ir_semantics/#variadic-bitwise-operations","text":"Performs a bit-wise operation on one-or-more identically-typed bits operands. If only a single argument is provided the operation is a no-op. Syntax result = and(operand_{0}, ..., operand_{N-1}) result = or(operand_{0}, ..., operand_{N-1}) result = xor(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} bits[N] result bits[N] Operations Operation Opcode Semantics and Op::kAnd result = lhs & rhs & ... or Op::kOr result = lhs \\| rhs \\| ... xor Op::kXor result = lhs ^ rhs ^ ...","title":"Variadic bitwise operations"},{"location":"ir_semantics/#arithmetic-unary-operations","text":"Performs an arithmetic operation on a single bits-typed operand. Syntax result = neg(operand) Types Value Type operand bits[N] result bits[N] Operations Operation Opcode Semantics neg Op::kNeg result = -operand","title":"Arithmetic unary operations"},{"location":"ir_semantics/#arithmetic-binary-operations","text":"Performs an arithmetic operation on a pair of bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Syntax result = add(lhs, rhs) result = smul(lhs, rhs) result = umul(lhs, rhs) result = sdiv(lhs, rhs) result = sub(lhs, rhs) result = udiv(lhs, rhs) Types Currently signed and unsigned multiply support arbitrary width operands and result. For all other arithmetic operations the operands and the result are the same width. The expectation is that all arithmetic operations will eventually support arbitrary widths. Operations Operation Opcode Semantics add Op::kAnd result = lhs + rhs sdiv Op::kSDiv result = $signed(lhs) / $signed(rhs) * smul Op::kSMul result = $signed(lhs) * $signed(rhs) sub Op::kSub result = lhs - rhs udiv Op::kUDiv result = lhs / rhs * umul Op::kUMul result = lhs * rhs * Synthesizing division can lead to failing synthesis and/or problems with timing closure. It is usually best not to rely on this Verilog operator in practice, but instead explicitly instantiate a divider of choice.","title":"Arithmetic binary operations"},{"location":"ir_semantics/#comparison-operations","text":"Performs a comparison on a pair of identically-typed bits operands. Unsigned operations are prefixed with a 'u', and signed operations are prefixed with a 's'. Produces a result of bits[1] type. Syntax result = eq(lhs, rhs) result = ne(lhs, rhs) result = sge(lhs, rhs) result = sgt(lhs, rhs) result = sle(lhs, rhs) result = slt(lhs, rhs) result = uge(lhs, rhs) result = ugt(lhs, rhs) result = ule(lhs, rhs) result = ult(lhs, rhs) Types Value Type lhs bits[N] rhs bits[N] result bits[1] Operations Operation Opcode Semantics eq Op::kEq result = lhs == rhs ne Op::kNe result = lhs != rhs sge Op::kSGe result = lhs >= rhs sgt Op::kSGt result = lhs > rhs sle Op::kSLe result = lhs <= rhs slt Op::kSLt result = lhs < rhs uge Op::kUGe result = lhs >= rhs ugt Op::kUGt result = lhs > rhs ule Op::kULe result = lhs <= rhs ult Op::kULt result = lhs < rhs","title":"Comparison operations"},{"location":"ir_semantics/#shift-operations","text":"Performs an shift operation on an input operand where the shift amount is specified by a second operand. Syntax result = shll(operand, amount) result = shra(operand, amount) result = shrl(operand, amount) Types The shifted operand and the result of the shift are the same width. Widths of the shift amount may be arbitrary. Operations Operation Opcode Semantics shll Op::kShll result = lhs << rhs * shra Op::kShra result = lhs >>> rhs (arithmetic shift right) ** shrl Op::kShra result = lhs >> rhs * * Logically shifting greater than or equal to the number of bits in the lhs produces a result of zero. ** Arithmetic right shifting greater than or equal to the number of bits in the lhs produces a result equal to all of the bits set to the sign of the lhs .","title":"Shift operations"},{"location":"ir_semantics/#extension-operations","text":"Extends a bit value to a new (larger) target bit-length. Syntax result = zero_ext(x, new_bit_count=42) result = sign_ext(x, new_bit_count=42) Types Value Type arg bits[N] new_bit_count int64 result bits[new_bit_count] Note: new_bit_count should be >= N or an error may be raised.","title":"Extension operations"},{"location":"ir_semantics/#zero_ext","text":"Zero-extends a value: turns its bit-length into the new target bit-length by filling zeroes in the most significant bits.","title":"zero_ext"},{"location":"ir_semantics/#sign_ext","text":"Sign-extends a value: turns its bit-length into the new target bit-length by filling in the most significant bits (MSbs) with the following policy: ones in the MSbs if the MSb of the original value was set, or zeros in the MSbs if the MSb of the original value was unset.","title":"sign_ext"},{"location":"ir_semantics/#channel-operations","text":"","title":"Channel operations"},{"location":"ir_semantics/#after_all","text":"Used to construct partial orderings among channel operations. result = after_all(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} token result token after_all can consume an arbitrary number of token operands including zero.","title":"after_all"},{"location":"ir_semantics/#miscellaneous-operations","text":"","title":"Miscellaneous operations"},{"location":"ir_semantics/#array_1","text":"Constructs a array of its operands. result = array(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T result T[N] Array can take an arbitrary number of operands including zero (which produces an empty array).","title":"array"},{"location":"ir_semantics/#array_index","text":"Returns a single element from an array. Syntax result = array_index(data, index) Types Value Type lhs Array of elements of type T rhs bits[M] result T Returns the element at the index given by operand index from the array data . TODO: Define out of bounds semantics for array_index.","title":"array_index"},{"location":"ir_semantics/#array_update","text":"Returns a modified copy of an array. Syntax result = array_update(array, index, value) Types Value Type array Array of elements of type T index bits[M] * value T result Array of elements of type T * M is arbitrary. Returns a copy of the input array with the element at the index replaced with the given value. If index is out of bounds, the returned array is identical to the input array.","title":"array_update"},{"location":"ir_semantics/#bit_slice","text":"Slices a a contiguous range of bits from a bits-typed operand. Syntax result = bit_slice(operand, start=<start>, width=<width>) Types Value Type operand bits[N] result bits[<width>] Keyword arguments Keyword Type Required Default Description start int64 yes The starting bit : : : : : of the slice. : : : : : : start is is : : : : : : zero-indexed : : : : : : where zero is the : : : : : : least-significant : : : : : : bit of the : : : : : : operand. : width int64 yes The width of the : : : : : slice. : The bit-width of operand must be greater than or equal to <start> plus <width> .","title":"bit_slice"},{"location":"ir_semantics/#concat","text":"Concatenates and arbitrary number of bits-typed operands. result = concat(operand{0}, ..., operand{n-1}) Types Value Type operand_{i} bits[N_{i}] result bits[Sum(N_{i})] This is equivalent to the verilog concat operator: result = {arg0, ..., argN}","title":"concat"},{"location":"ir_semantics/#counted_for","text":"Invokes a fixed-trip count loop. Syntax result = counted_for(init, trip_count=<trip_count>, stride=<stride>, body=<body>, invariant_args=<inv_args>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description trip_count int64 yes Trip count of the loop : : : : : (number of times that the : : : : : : loop body will be : : : : : : executed) : stride int64 no 1 Stride of the induction : : : : : variable : invariant_args array of yes Names of the invariant : : operands : : : operands as the loop body : body string yes Name of the function to : : : : : use as the loop body : counted_for invokes the function body trip_count times, passing loop-carried data that starts with value init . The first argument passed to body is the induction variable -- presently, the induction variable always starts at zero and increments by stride after every trip. The second argument passed to body is the loop-carry data. The return type of body must be the same as the type of the init loop carry data. The value returned from the last trip is the result of the counted_for expression. All subsequent arguments passed to body are passed from invariant_args ; e.g. if there are two members in invariant_args those values are passed as the third and fourth arguments. Therefore body should have a signature that matches the following: body(i, loop_carry_data[, invariant_arg0, invariant_arg1, ...]) Note that we currently inspect the body function to see what type of induction variable ( i above) it accepts in order to pass an i value of that type.","title":"counted_for"},{"location":"ir_semantics/#decode","text":"Implements a binary decoder. result = decode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be less than or equal to 2** N where N is the operand width. Keyword arguments Keyword Type Required Default Description width int64 yes Width of the result decode converts the binary-encoded operand value into a one-hot result. For an operand value of n interpreted as an unsigned number the n -th result bit and only the n -th result bit is set. The width of the decode operation may be less than the maximum value expressible by the input (2** N - 1). If the encoded operand value is larger than the number of bits of the result the result is zero.","title":"decode"},{"location":"ir_semantics/#encode","text":"Implements a binary encoder. result = encode(operand, width=<width>) Types Value Type operand bits[N] result bits[M] The result width M must be equal to $$\\lceil \\log_{2} N \\rceil$$. encode converts the one-hot operand value into a binary-encoded value of the \"hot\" bit of the input. If the n -th bit and only the n -th bit of the operand is set the result is equal the value n as an unsigned number. If multiple bits of the input are set the result is equal to the logical or of the results produced by the input bits individually. For example, if bit 3 and bit 5 of an encode input are set the result is equal to 3 | 5 = 7. If no bits of the input are set the result is zero.","title":"encode"},{"location":"ir_semantics/#invoke","text":"Invokes a function. Syntax result = invoke(operand_{0}, ... , operand_{N-1}, to_apply=<to_apply>) Types Value Type init T result T Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to use as : : : : : the loop body : TODO: finish","title":"invoke"},{"location":"ir_semantics/#map","text":"Applies a function to the elements of an array and returns the result as an array. Syntax result = map(operand, to_apply=<to_apply>) Types Value Type operand array[T] result array[U] Keyword arguments Keyword Type Required Default Description to_apply string yes Name of the function to apply : : : : : to each element of the operand : TODO: finish","title":"map"},{"location":"ir_semantics/#one_hot","text":"Produces a bits value with exactly one bit set. The index of the set bit depends upon the input value. Syntax result = one_hot(input, lsb_prio=true) result = one_hot(input, lsb_prio=false) Types Value Type input bits[N] result bits[N+1] Keyword arguments Keyword Type Required Default Description lsb_prio bool yes Whether the least significant : : : : : bit (LSb) has priority. : For lsb_prio=true : result bit i for 0 <= i < N is set in result iff bit i is set in the input and all lower bits j for j < i are not set in the input. For lsb_prio=false : result bit i for N-1 >= i >= 0 is set in result iff bit i is set in the input and all higher (more significant) bits j for j > i are not set in the input. For both lsb_prio=true and lsb_prio=false , result bit N (the most significant bit in the output) is only set if no bits in the input are set. Examples: one_hot(0b0011, lsb_prio=true) => 0b00001 -- note that an extra MSb has been appended to the output to potentially represent the \"all zeros\" case. one_hot(0b0111, lsb_prio=false) => 0b00100 . one_hot(0b00, lsb_prio=false) => 0b100 . one_hot(0b00, lsb_prio=true) => 0b100 -- note the output for one_hot is the same for the all-zeros case regardless of whether lsb_prio is true or false. This operation is useful for constructing match or switch operation semantics where a condition is matched against an ordered set of cases and the first match is chosen. It is also useful for one-hot canonicalizing, e.g. as a prelude to counting leading/trailing zeros.","title":"one_hot"},{"location":"ir_semantics/#one_hot_sel","text":"Selects between operands based on a one-hot selector. Syntax result = one_hot_sel(selector, cases=[case_{0}, ... , case_{N-1}]) Types Value Type selector bits[N] case_{i} T result T The result is the logical OR of all cases case_{i} for which the corresponding bit i is set in the selector. When selector is one-hot this performs a select operation.","title":"one_hot_sel"},{"location":"ir_semantics/#param","text":"TODO: finish","title":"param"},{"location":"ir_semantics/#reverse","text":"Reverses the order of bits of its operand. result = reverse(operand) Types Value Type operand bits[N] result bits[N]","title":"reverse"},{"location":"ir_semantics/#sel","text":"Selects between operands based on a selector value. Syntax result = sel(selector, cases=[case_{0}, ... , case_{N-1}], default=<default>) Types Value Type selector bits[M] case_{i} T default T result T","title":"sel"},{"location":"ir_semantics/#tuple_1","text":"Constructs a tuple of its operands. result = tuple(operand_{0}, ..., operand_{N-1}) Types Value Type operand_{i} T_{i} result (T_{0}, ... , T_{N-1}) Tuple can take and arbitrary number of operands including zero (which produces an empty tuple).","title":"tuple"},{"location":"ir_semantics/#tuple_index","text":"Returns a single element from a tuple-typed operand. Syntax result = tuple_index(operand, index=<index>) Types Value Type operand (T_{0}, ... , T_{N-1}) result T_{<index>} Keyword arguments Keyword Type Required Default Description index int64 yes Index of tuple element to produce","title":"tuple_index"},{"location":"ir_visualization/","text":"IR Visualization The XLS IR visualization web app presents the IR in text and graphical form side-by-side and enables interactive exploration of the IR. Running the web app To build and launch the IR visualization web app run: bazel run -c opt xls/visualization/ir_viz:app -- --delay_model=unit Then visit http://localhost:5000 in a browser. Screenshot The screenshot below shows a zoomed-in portion of the IR graph for the fp_adder benchmark. The highlighted path in blue is the timing critical path the through the graph. Usage Text IR The left hand side of the UI shows the IR in text form in an editable text box. The IR may be entered or loaded in several ways: Upload from a file on the local file system via the Upload button. Enter directly by typing in the text box or cut and pasting. Load a pre-compiled benchmark via the Benchmarks button. The IR is from the benchmark after optimizations. The text IR is parsed as you type. The result of the parse ( OK or an error) appears in an alert at the bottom of the text box. On successful parsing all identifiers in the IR will be shown in bold. IR graph The right had side of the UI shows the IR in graphical form. Clicking on the View Graph button renders the text IR on the left hand side as a graph. The View Graph button is enabled only if the IR is parsed successfully. The graph view may be manipulated as follows: Zoom The mouse scroll wheel zooms the view of the IR graph. Pan Clicking and holding the left mouse button down in the graph panel (while not on a graph element) and moving the mouse pans the graph. Moving nodes Nodes in the graph are moved by clicking and holding on the node and moving the mouse. Focusing on nodes Clicking on a node in the graph while holding down the control key scrolls the respective definition of the node in the text IR into view in the text box. Similarly, control clicking on an identifier in the text IR zooms and centers the graph view on the respective node. Node colors Every node in the graph is assigned a color on a spectrum from white ( #FFFFFF ) to red ( #FF0000 ) depending on the modeled latency of the operation. The nodes with the longest latency in the graph are assigned red. Nodes with zero latency are assigned white. Hovering on IR elements Hovering on nodes and edges in the graph highlights the corresponding element in the text IR and vice versa. In the text IR, the definition and all uses of the IR value are highlighted when a node is highlighted. When a graph edge is highlighted, the definition and corresponding use are highlighted in the text IR. Information about a highlighted node (identifier in text IR) is displayed in a box above the IR graph. This information includes: The definition of the IR value in text form. Estimate of the delay in picoseconds of the corresponding operation. The delay estimation methodology is described here . Any known bits of the value as determined by the query engine (https://github.com/google/xls/tree/main/xls/passes/query_engine.h). Selecting nodes Nodes in the graph may be in a selected or deselected state. Clicking on a node in the graph or identifier in the text IR toggles the selection state. A selected node (identifier in IR text) is shown with a blue border. Nodes and edges which are neighbors of selected nodes (the selection frontier) are shown in orange. Clicking on an empty area of the graph deselects all nodes. Showing only selected nodes The toggle Show only selected nodes controls whether to show the entire graph or only the selected node and those elements in the selection frontier. Showing only selected nodes can be used to display only a subgraph of interest. For large graphs which are slow to render in their entirety, this mechanism can be used to interactively explore parts of the graph. When showing only selected nodes, the graph maybe be expanded by selecting additional nodes to add to the graph. The graph is re-rendered to include the newly selected node. Similarly, nodes may be removed from the graph by deselecting nodes. Selecting the critical path The button Critical Path selects exactly those nodes which are on the critical path as determined by XLS's timing model. This may be used with the Show only selected nodes toggle to show a graph containing only critical path elements and neighbors. In the screen shot above, the selected critical path is shown in blue.","title":"Visualizer"},{"location":"ir_visualization/#ir-visualization","text":"The XLS IR visualization web app presents the IR in text and graphical form side-by-side and enables interactive exploration of the IR.","title":"IR Visualization"},{"location":"ir_visualization/#running-the-web-app","text":"To build and launch the IR visualization web app run: bazel run -c opt xls/visualization/ir_viz:app -- --delay_model=unit Then visit http://localhost:5000 in a browser.","title":"Running the web app"},{"location":"ir_visualization/#screenshot","text":"The screenshot below shows a zoomed-in portion of the IR graph for the fp_adder benchmark. The highlighted path in blue is the timing critical path the through the graph.","title":"Screenshot"},{"location":"ir_visualization/#usage","text":"","title":"Usage"},{"location":"ir_visualization/#text-ir","text":"The left hand side of the UI shows the IR in text form in an editable text box. The IR may be entered or loaded in several ways: Upload from a file on the local file system via the Upload button. Enter directly by typing in the text box or cut and pasting. Load a pre-compiled benchmark via the Benchmarks button. The IR is from the benchmark after optimizations. The text IR is parsed as you type. The result of the parse ( OK or an error) appears in an alert at the bottom of the text box. On successful parsing all identifiers in the IR will be shown in bold.","title":"Text IR"},{"location":"ir_visualization/#ir-graph","text":"The right had side of the UI shows the IR in graphical form. Clicking on the View Graph button renders the text IR on the left hand side as a graph. The View Graph button is enabled only if the IR is parsed successfully. The graph view may be manipulated as follows: Zoom The mouse scroll wheel zooms the view of the IR graph. Pan Clicking and holding the left mouse button down in the graph panel (while not on a graph element) and moving the mouse pans the graph. Moving nodes Nodes in the graph are moved by clicking and holding on the node and moving the mouse. Focusing on nodes Clicking on a node in the graph while holding down the control key scrolls the respective definition of the node in the text IR into view in the text box. Similarly, control clicking on an identifier in the text IR zooms and centers the graph view on the respective node.","title":"IR graph"},{"location":"ir_visualization/#node-colors","text":"Every node in the graph is assigned a color on a spectrum from white ( #FFFFFF ) to red ( #FF0000 ) depending on the modeled latency of the operation. The nodes with the longest latency in the graph are assigned red. Nodes with zero latency are assigned white.","title":"Node colors"},{"location":"ir_visualization/#hovering-on-ir-elements","text":"Hovering on nodes and edges in the graph highlights the corresponding element in the text IR and vice versa. In the text IR, the definition and all uses of the IR value are highlighted when a node is highlighted. When a graph edge is highlighted, the definition and corresponding use are highlighted in the text IR. Information about a highlighted node (identifier in text IR) is displayed in a box above the IR graph. This information includes: The definition of the IR value in text form. Estimate of the delay in picoseconds of the corresponding operation. The delay estimation methodology is described here . Any known bits of the value as determined by the query engine (https://github.com/google/xls/tree/main/xls/passes/query_engine.h).","title":"Hovering on IR elements"},{"location":"ir_visualization/#selecting-nodes","text":"Nodes in the graph may be in a selected or deselected state. Clicking on a node in the graph or identifier in the text IR toggles the selection state. A selected node (identifier in IR text) is shown with a blue border. Nodes and edges which are neighbors of selected nodes (the selection frontier) are shown in orange. Clicking on an empty area of the graph deselects all nodes.","title":"Selecting nodes"},{"location":"ir_visualization/#showing-only-selected-nodes","text":"The toggle Show only selected nodes controls whether to show the entire graph or only the selected node and those elements in the selection frontier. Showing only selected nodes can be used to display only a subgraph of interest. For large graphs which are slow to render in their entirety, this mechanism can be used to interactively explore parts of the graph. When showing only selected nodes, the graph maybe be expanded by selecting additional nodes to add to the graph. The graph is re-rendered to include the newly selected node. Similarly, nodes may be removed from the graph by deselecting nodes.","title":"Showing only selected nodes"},{"location":"ir_visualization/#selecting-the-critical-path","text":"The button Critical Path selects exactly those nodes which are on the critical path as determined by XLS's timing model. This may be used with the Show only selected nodes toggle to show a graph containing only critical path elements and neighbors. In the screen shot above, the selected critical path is shown in blue.","title":"Selecting the critical path"},{"location":"optimizations/","text":"XLS Optimizations XLS Optimizations Narrowing Optimizations Select operations Arithmetic and shift operations Comparison operations Strength Reductions Arithmetic Comparison Strength Reductions \"Simple\" Boolean Simplification Bit-slice optimizations Slicing sign-extended values Concat optimizations Hoisting a reverse above a concat Hoisting a bitwise operation above a concat Merging consecutive bit-slices Select optimizations Converting chains of Selects to into a single OneHotSelect Binary Decision Diagram based optimizations BDD common subexpression elimination OneHot MSB elimination Traditional compiler optimizations Common Subexpression Elimination (CSE) Constant Folding Dead Code Elimination (DCE) Reassociation Narrowing Optimizations The XLS compiler performs bitwise flow analysis , and so can deduce that certain bits in the output of operations are always-zero or always-one. As a result, after running this bit-level tracking flow analysis, some of the bits on the output of an operation may be \"known\". With known bits in the output of an operation, we can often narrow the operation to only produce the unknown bits (those bits that are not known static constants), which reduces the \"cost\" of the operation (amount of delay through the operation) by reducing its bitwidth. Select operations An example of this is select narrowing, as shown in the following -- beforehand we have three operands, but from our bitwise analysis we know that there are two constant bits in the MSb as well as two constant bits in the LSb being propagated from all of our input operands. {style=\"display:block;margin:auto;width:50%\"} Recognizing that property, we squeeze the one hot select operation -- in the \"after\" diagram below observe we've narrowed the operation by slicing the known-constant bits out of the one hot select operation, making it cheaper in terms of delay, and propagated the slices up to the input operands -- these slices being presented at the output of the operands may in turn let them narrow their operation and become cheaper, and this can continue transitively): {style=\"display:block;margin:auto;width:50%\"} Arithmetic and shift operations Most arithmetic ops support mixed bit widths where the operand widths may not be the same width as each other or the result. This provides opportunities for narrowing. Specfically (for multiplies, adds and subtracts): If the operands are wider than the result, the operand can be truncated to the result width. If the operation result is wider than the full-precision width of the operation, the operation result can be narrowed to the full-precision width then sign- or zero-extended (depending upon the sign of the operation) to the original width to produce the desired result. The full-precision width of an add or subtract is one more than the width of the widest operand, and the full-precision width of a multiply is the sum of the operand widths. If the most-significant bit of an operand are zeros (for unsigned operations) or the same as the sign-bit value (for signed operations), the operand can be narrowed to remove these known bits. As a special case, adds can be narrowed if the least-significant bits of an is all zeros. The add operation is narrowed to exclude this range of least-significant bits. The least-signifant bits of the result are simply the least-significant bits of the non-zero operand: Similarly, if the most-significant bits of the shift-amount of a shift operation are zero the shift amount can be narrowed. Comparison operations Leading and trailing bits can be stripped from the operands of comparison operations if these bits do not affect the result of the comparison. For unsigned comparisons, leading and trailing bits which are identical between the two operands can be stripped which narrows the comparison and reduces the cost of the operation. Signed comparison are more complicated to handle because the sign bit (most-signicant bit) affects the interpretation of the value of the remaining bits. Stripping leading bits must preserve the sign bit of the operand. Strength Reductions Arithmetic Comparison Strength Reductions When arithmetic comparisons occur with respect to constant values, comparisons which are be arithmetic in the general case may be strength reduced to more boolean-analyzeable patterns; for example, comparison with mask constants: u4:0bwxyz > u4:0b0011 Can be strength reduced -- for the left hand side to be greater one of the wx bits must be set, so we can simply replace this with an or-reduction of wx . Similarly, a trailing-bit and-reduce is possible for less-than comparisons. NOTE These examples highlight a set of optimizations that are not applicable (profitable) on traditional CPUs: the ability to use bit slice values below what we'd traditionally think of as a fundamental comparison \"instruction\", which would nominally take a single cycle. \"Simple\" Boolean Simplification NOTE This optimization is a prelude to a more general optimization we expect to come in the near future that is based on Binary Decision Diagrams. It is documented largely as a historical note of an early / simple optimization approach. With the addition of the one_hot_select and its corresponding optimizations, a larger amount of boolean logic appears in XLS' optimized graphs (e.g. or-ing together bits in the selector to eliminate duplicate one_hot_select operands). Un-simplified boolean operations compound their delay on the critical path; with the process independent constant $$\\tau$$ a single bit or might be $$6\\tau$$, which is just to say that having lots of dependent or operations can meaningfully add up in the delay estimation for the critical path. If we can simplify several layers of boolean operations into one operation (say, perhaps with inputs optionally inverted) we could save a meaningful number of tau versus a series of dependent boolean operations. For a simple approach to boolean simplification: The number of parameters to the boolean function is limited to three inputs The truth table is computed for the aggregate boolean function by flowing input bit vectors through all the boolean operation nodes. The result bit vectors on the output frontier are matched the resulting truth table from the flow against one of our standard operations (perhaps with the input operands inverted). The algorithm starts by giving x and y their vectors (columns) from the truth table, enumerating all possible bit combinations for those operands. For example, consider two operands and a (bitwise) boolean function, the following is the truth table: X Y | X+~Y ----+------ 0 0 | 1 0 1 | 0 1 0 | 1 1 1 | 1 Each column in this table is a representation of the possibilities for a node in the graph to take on, as a vector. After giving the vector [0, 0, 1, 1] to the first input node (which is arbitrarily called X) and the vector [0, 1, 0, 1] to the second input node (which is arbitrarily called Y), and flowing those bit vectors through a network of boolean operations, if you wind up with a vector [1, 0, 1, 1] at the end, it is sound to replace that whole network with the expression X+~Y . Similarly, if the algorithm arrived at the vector [1, 1, 1, 1] at the end of the network, you could replace the result with a literal 1 , because it has been proven for all input operand possiblities the result is always 1 in every bit. Effectively, this method works by brute force enumerating all the possibilities for input bits and operating on all of those possibilities at the same time. In the end, the algorithm arrives at a composite boolean function that can be pattern matched against XLS's set of \"simple boolean functions\". In the following example there are two nodes on the \"input frontier\" for the boolean operations ( sub and add , which we \"rename\" to x and y for the purposes of analysis). {style=\"display:block;margin:auto;width:75%\"} As shown in the picture, the algorithm starts flowing the bit vector, which represents all possible input values for x and y . You can see that the not which produces $$\\bar{x}$$ (marked with a red star) simply inverts all the entries in the vector and corresponds to the $$\\bar{x}$$ column in the truth table. Similarly the and operation joins the two vector with the binary & operation, and finally we end up with the blue-starred bit vector on the \"output frontier\", feeding the dependent one_hot_select (marked as ohs in the diagram). When we resolve that final result bit vector with the blue star against our table of known function substitutions, we see that the final result can be replaced with a node that is simply or(x, y) , saving two unnecessary levels of logic, and reducing the critical path delay in this example from something like $$13\\tau$$ to something like $$6\\tau$$. This basic procedure is then extended to permit three variables on the input frontier to the boolean expression nodes, and the \"known function\" table is extended to include all of our supported logical operators (i.e. nand , nor , xor , and , or ) with bit vectors for all combinations of inputs being present, and when present, either asserted, or their inversions (e.g. we can find $$nand(\\bar{X}, Y)$$ even though X is inverted). Bit-slice optimizations Bit-slice operations narrow values by selecting a contiguous subset of bits from their operand. Bit-slices are zero-cost operations as no computation is performed. However, optimization of bit-slices can be beneficial as bit-slices can interfere with optimizations and hoisting bit-slices can narrow other operations reducing their computation cost. Slicing sign-extended values A bit-slice of a sign-extended value is a widening operation followed by a narrowing operation and can be optimized. The details of the transformation depends upon relative position of the slice and the sign bit of the original value. Let sssssssXXXXXXXX be the sign-extended value where s is the sign bit and X represents the bits of the original value. There are three possible cases: Slice is entirely within the sign-extend operand. Transformation: replace the bit-slice of the sign-extended value with a bit-slice of the original value. Slice spans the sign bit of the sign-extend operand. Transformation: slice the most significant bits from the original value and sign-extend the result. Slice is entirely within the sign-extended bits. Transformation: slice the sign bit from the original value and sign-extend it. To avoid introducing additional sign-extension operations cases (2) and (3) should only be performed if the bit-slice is the only user of the sign-extension. Concat optimizations Concat (short for concatenation) operations join their operands into a single word. Like BitSlice s, Concat operations have no cost since since they simply create a new label to refer to a set of bits, performing no actual computation. However, Concat optimizations can still provide benefit by reducing the number of IR nodes (increases human readability) or by refactoring the IR in a way that allows other optimizations to be applied. Several Concat optimizations involve hoisting an operation on one or more Cocnat s to above the Concat such that the operation is applied on the Concat operands directly. This may provide opportunities for optimization by bringing operations which actually perform logic closer to other operations performing logic. Hoisting a reverse above a concat A Reverse operation reverses the order of the bits of the input operand. If a Concat is reversed and the Concat has no other consumers except for reduction operations (which are not sensitive to bit order), we hoist the Reverse above the Concat . In the modified IR, the Concat input operands are Reverse 'd and then concatenated in reverse order, e.g. : Reverse(Concat(a, b, c)) => Concat(Reverse(c), Reverse(b), Reverse(a)) Hoisting a bitwise operation above a concat If the output of multiple Concat operations are combined with a bitwise operation, the bitwise operation is hoisted above the Concat s. In the modified IR, we have a single Concat whose operands are bitwise'd BitSlice s of the original Concat s, e.g. : Or(Concat(A, B), Concat(C, D)), where A,B,C, and D are 1-bit values => Concat(Or(Concat(A, B)[1], Concat(C, D)[1]), Or(Concat(A, B)[0], Concat(C, D)[0])) In the case that an added BitSlice exactly aligns with an original Concat operand, other optimizations (bit slice simplification, constant folding, dead code elimination) will replace the BitSlice with the operand, e.g. for the above example: => Concat(Or(A, C), Or(B, D)) Merging consecutive bit-slices If consecutive Concat operands are consecutive BitSlice s, we create a new, merged BitSlice spanning the range of the consecutive BitSlice s. Then, we create a new Concat that includes this BitSlice as an operand, e.g. Concat(A[3:2], A[1:0], B) => Concat(A[3:0], B) This optimization is sometimes helpful and sometimes harmful, e.g. in the case that there are other consumers of the original BitSlice s, we may only end up adding more IR nodes since the original BitSlice s will not be removed by DCE once they are replaced in the Concat . Some adjustments might be able to help with this issue. An initial attempt at this limited the application of this optimization to cases where the given Concat is the only consumer of the consecutive BitSlice s. This limited the more harmful applications of this optimization, but also reduced instances in which the optimization was beneficially applied (e.g. the same consecutive BitSlice s could be merged in multiple Concat s). Select optimizations XLS supports two types of select operations: Select (opcode Op::kSel ) is a traditional multiplexer. An n -bit binary-encoded selector chooses among 2** n inputs. OneHotSelect (opcode Op::kOneHotSel ) has one bit in the selector for each input. The output of the operation is equal to the logical-or reduction of the inputs corresponding to the set bits of the selector. Generally, a OneHotSelect is lower latency and area than a Select as a Select is effectively a decode operation followed by a OneHotSelect . Converting chains of Select s to into a single OneHotSelect A linear chain of binary Select operations may be produced by the front end to select amongst a number of different values. This is equivalent to nested ternary operators in C++. A chain of Select s has high latency, but latency may be reduced by converting the Select chain into a single OneHotSelect . This may only be performed if the single-bit selectors of the Select instructions are one-hot (at most one selector is set at one time). In this case, the single-bit Select selectors are concatenated together to produce the selector for the one hot. This effectively turns a serial operation into a lower latency parallel one. If the selectors of the original Select instructions can be all zero (but still at most one selector is asserted) the transformation is slightly modified. An additional selector which is the logical NOR of all of the original Select selector bits is appended to the OneHotSelect selector and the respective case is the value selected when all selector bits are zero ( case_0 in the diagram). Binary Decision Diagram based optimizations A binary decision diagram (BDD) is a data structure that can represent arbitrary boolean expressions. Properties of the BDD enable easy determination of relationships between different expressions (equality, implication, etc.) which makes them useful for optimization and analysis BDD common subexpression elimination Determining whether two expression are equivalent is trivial using BDDs. This can be used to identify operations in the graph which produce identical results. BDD CSE is an optimization pass which commons these equivalent operations. OneHot MSB elimination A OneHot instruction returns a bitmask with exactly one bit equal to 1. If the input is not all-0 bits, this is the first 1 bit encountered in the input going from least to most significant bit or vice-versa depending on the priority specified. If the input is all-0 bits, the most significant bit of the OneHot output is set to 1. The semantics of OneHot are described in detail here . If the MSB of a OneHot does not affect the functionality of a program, we replace the MSB with a 0-bit, e.g. OneHot(A) such that the MSB has no effect \u21d2 Concat(0, OneHot(A)[all bits except MSB])) This can open up opportunities for further optimization. To determine if a OneHot \u2019s MSB has any effect on a function, we iterate over the OneHot \u2019s post-dominators. We use the BDD to test if setting the OneHot \u2019s MSB to 0 or to 1 (other bits are 0 in both cases) implies the same value for the post-dominator node. If so, we know the value of the MSB cannot possibly affect the function output, so the MSB can safely be replaced with a 0 bit. Note: This approach assumes that IR nodes do not have any side effects. When IR nodes with side effects are introduced (i.e. channels) the analysis for this optimization will have to be adjusted slightly to account for this. Traditional compiler optimizations Many optimizations from traditional compilers targeting CPUs also apply to the optimization of hardware. Common objectives of traditional compiler optimizations include exposing parallelism, reducing latency, and eliminating instructions. Often these translate directly into the primary objectives of hardware optimization of reducing delay and area. Common Subexpression Elimination (CSE) TODO: Finish. Constant Folding TODO: Finish. Dead Code Elimination (DCE) TODO: Finish. Reassociation Reassociation in XLS uses the associative and commutative property of arithmetic operations (such as adds and multiplies) to rearrange expressions of identical operations to minimize delay and area. Delay is reduced by transforming chains of operations into balanced trees which reduces the critical-path delay. For example, given the following expression: This can be reassociated into the following balanced tree: The transformation has reduced the critical path through the expression from three adds down to two adds. Reassociation can also create opportunities for constant folding. If an expression contains multiple literal values (constants) the expressions can be reassociated to gather literals into the same subepxression which can then be folded. Generally this requires the operation to be commutative as well as associative. For example, given the following expression: This can be reassociated into: The right-most add of the two literals can be folded reducing the number of adds in the expression to two.","title":"Optimizations"},{"location":"optimizations/#xls-optimizations","text":"XLS Optimizations Narrowing Optimizations Select operations Arithmetic and shift operations Comparison operations Strength Reductions Arithmetic Comparison Strength Reductions \"Simple\" Boolean Simplification Bit-slice optimizations Slicing sign-extended values Concat optimizations Hoisting a reverse above a concat Hoisting a bitwise operation above a concat Merging consecutive bit-slices Select optimizations Converting chains of Selects to into a single OneHotSelect Binary Decision Diagram based optimizations BDD common subexpression elimination OneHot MSB elimination Traditional compiler optimizations Common Subexpression Elimination (CSE) Constant Folding Dead Code Elimination (DCE) Reassociation","title":"XLS Optimizations"},{"location":"optimizations/#narrowing-optimizations","text":"The XLS compiler performs bitwise flow analysis , and so can deduce that certain bits in the output of operations are always-zero or always-one. As a result, after running this bit-level tracking flow analysis, some of the bits on the output of an operation may be \"known\". With known bits in the output of an operation, we can often narrow the operation to only produce the unknown bits (those bits that are not known static constants), which reduces the \"cost\" of the operation (amount of delay through the operation) by reducing its bitwidth.","title":"Narrowing Optimizations"},{"location":"optimizations/#select-operations","text":"An example of this is select narrowing, as shown in the following -- beforehand we have three operands, but from our bitwise analysis we know that there are two constant bits in the MSb as well as two constant bits in the LSb being propagated from all of our input operands. {style=\"display:block;margin:auto;width:50%\"} Recognizing that property, we squeeze the one hot select operation -- in the \"after\" diagram below observe we've narrowed the operation by slicing the known-constant bits out of the one hot select operation, making it cheaper in terms of delay, and propagated the slices up to the input operands -- these slices being presented at the output of the operands may in turn let them narrow their operation and become cheaper, and this can continue transitively): {style=\"display:block;margin:auto;width:50%\"}","title":"Select operations"},{"location":"optimizations/#arithmetic-and-shift-operations","text":"Most arithmetic ops support mixed bit widths where the operand widths may not be the same width as each other or the result. This provides opportunities for narrowing. Specfically (for multiplies, adds and subtracts): If the operands are wider than the result, the operand can be truncated to the result width. If the operation result is wider than the full-precision width of the operation, the operation result can be narrowed to the full-precision width then sign- or zero-extended (depending upon the sign of the operation) to the original width to produce the desired result. The full-precision width of an add or subtract is one more than the width of the widest operand, and the full-precision width of a multiply is the sum of the operand widths. If the most-significant bit of an operand are zeros (for unsigned operations) or the same as the sign-bit value (for signed operations), the operand can be narrowed to remove these known bits. As a special case, adds can be narrowed if the least-significant bits of an is all zeros. The add operation is narrowed to exclude this range of least-significant bits. The least-signifant bits of the result are simply the least-significant bits of the non-zero operand: Similarly, if the most-significant bits of the shift-amount of a shift operation are zero the shift amount can be narrowed.","title":"Arithmetic and shift operations"},{"location":"optimizations/#comparison-operations","text":"Leading and trailing bits can be stripped from the operands of comparison operations if these bits do not affect the result of the comparison. For unsigned comparisons, leading and trailing bits which are identical between the two operands can be stripped which narrows the comparison and reduces the cost of the operation. Signed comparison are more complicated to handle because the sign bit (most-signicant bit) affects the interpretation of the value of the remaining bits. Stripping leading bits must preserve the sign bit of the operand.","title":"Comparison operations"},{"location":"optimizations/#strength-reductions","text":"","title":"Strength Reductions"},{"location":"optimizations/#arithmetic-comparison-strength-reductions","text":"When arithmetic comparisons occur with respect to constant values, comparisons which are be arithmetic in the general case may be strength reduced to more boolean-analyzeable patterns; for example, comparison with mask constants: u4:0bwxyz > u4:0b0011 Can be strength reduced -- for the left hand side to be greater one of the wx bits must be set, so we can simply replace this with an or-reduction of wx . Similarly, a trailing-bit and-reduce is possible for less-than comparisons. NOTE These examples highlight a set of optimizations that are not applicable (profitable) on traditional CPUs: the ability to use bit slice values below what we'd traditionally think of as a fundamental comparison \"instruction\", which would nominally take a single cycle.","title":"Arithmetic Comparison Strength Reductions"},{"location":"optimizations/#simple-boolean-simplification","text":"NOTE This optimization is a prelude to a more general optimization we expect to come in the near future that is based on Binary Decision Diagrams. It is documented largely as a historical note of an early / simple optimization approach. With the addition of the one_hot_select and its corresponding optimizations, a larger amount of boolean logic appears in XLS' optimized graphs (e.g. or-ing together bits in the selector to eliminate duplicate one_hot_select operands). Un-simplified boolean operations compound their delay on the critical path; with the process independent constant $$\\tau$$ a single bit or might be $$6\\tau$$, which is just to say that having lots of dependent or operations can meaningfully add up in the delay estimation for the critical path. If we can simplify several layers of boolean operations into one operation (say, perhaps with inputs optionally inverted) we could save a meaningful number of tau versus a series of dependent boolean operations. For a simple approach to boolean simplification: The number of parameters to the boolean function is limited to three inputs The truth table is computed for the aggregate boolean function by flowing input bit vectors through all the boolean operation nodes. The result bit vectors on the output frontier are matched the resulting truth table from the flow against one of our standard operations (perhaps with the input operands inverted). The algorithm starts by giving x and y their vectors (columns) from the truth table, enumerating all possible bit combinations for those operands. For example, consider two operands and a (bitwise) boolean function, the following is the truth table: X Y | X+~Y ----+------ 0 0 | 1 0 1 | 0 1 0 | 1 1 1 | 1 Each column in this table is a representation of the possibilities for a node in the graph to take on, as a vector. After giving the vector [0, 0, 1, 1] to the first input node (which is arbitrarily called X) and the vector [0, 1, 0, 1] to the second input node (which is arbitrarily called Y), and flowing those bit vectors through a network of boolean operations, if you wind up with a vector [1, 0, 1, 1] at the end, it is sound to replace that whole network with the expression X+~Y . Similarly, if the algorithm arrived at the vector [1, 1, 1, 1] at the end of the network, you could replace the result with a literal 1 , because it has been proven for all input operand possiblities the result is always 1 in every bit. Effectively, this method works by brute force enumerating all the possibilities for input bits and operating on all of those possibilities at the same time. In the end, the algorithm arrives at a composite boolean function that can be pattern matched against XLS's set of \"simple boolean functions\". In the following example there are two nodes on the \"input frontier\" for the boolean operations ( sub and add , which we \"rename\" to x and y for the purposes of analysis). {style=\"display:block;margin:auto;width:75%\"} As shown in the picture, the algorithm starts flowing the bit vector, which represents all possible input values for x and y . You can see that the not which produces $$\\bar{x}$$ (marked with a red star) simply inverts all the entries in the vector and corresponds to the $$\\bar{x}$$ column in the truth table. Similarly the and operation joins the two vector with the binary & operation, and finally we end up with the blue-starred bit vector on the \"output frontier\", feeding the dependent one_hot_select (marked as ohs in the diagram). When we resolve that final result bit vector with the blue star against our table of known function substitutions, we see that the final result can be replaced with a node that is simply or(x, y) , saving two unnecessary levels of logic, and reducing the critical path delay in this example from something like $$13\\tau$$ to something like $$6\\tau$$. This basic procedure is then extended to permit three variables on the input frontier to the boolean expression nodes, and the \"known function\" table is extended to include all of our supported logical operators (i.e. nand , nor , xor , and , or ) with bit vectors for all combinations of inputs being present, and when present, either asserted, or their inversions (e.g. we can find $$nand(\\bar{X}, Y)$$ even though X is inverted).","title":"\"Simple\" Boolean Simplification"},{"location":"optimizations/#bit-slice-optimizations","text":"Bit-slice operations narrow values by selecting a contiguous subset of bits from their operand. Bit-slices are zero-cost operations as no computation is performed. However, optimization of bit-slices can be beneficial as bit-slices can interfere with optimizations and hoisting bit-slices can narrow other operations reducing their computation cost.","title":"Bit-slice optimizations"},{"location":"optimizations/#slicing-sign-extended-values","text":"A bit-slice of a sign-extended value is a widening operation followed by a narrowing operation and can be optimized. The details of the transformation depends upon relative position of the slice and the sign bit of the original value. Let sssssssXXXXXXXX be the sign-extended value where s is the sign bit and X represents the bits of the original value. There are three possible cases: Slice is entirely within the sign-extend operand. Transformation: replace the bit-slice of the sign-extended value with a bit-slice of the original value. Slice spans the sign bit of the sign-extend operand. Transformation: slice the most significant bits from the original value and sign-extend the result. Slice is entirely within the sign-extended bits. Transformation: slice the sign bit from the original value and sign-extend it. To avoid introducing additional sign-extension operations cases (2) and (3) should only be performed if the bit-slice is the only user of the sign-extension.","title":"Slicing sign-extended values"},{"location":"optimizations/#concat-optimizations","text":"Concat (short for concatenation) operations join their operands into a single word. Like BitSlice s, Concat operations have no cost since since they simply create a new label to refer to a set of bits, performing no actual computation. However, Concat optimizations can still provide benefit by reducing the number of IR nodes (increases human readability) or by refactoring the IR in a way that allows other optimizations to be applied. Several Concat optimizations involve hoisting an operation on one or more Cocnat s to above the Concat such that the operation is applied on the Concat operands directly. This may provide opportunities for optimization by bringing operations which actually perform logic closer to other operations performing logic.","title":"Concat optimizations"},{"location":"optimizations/#hoisting-a-reverse-above-a-concat","text":"A Reverse operation reverses the order of the bits of the input operand. If a Concat is reversed and the Concat has no other consumers except for reduction operations (which are not sensitive to bit order), we hoist the Reverse above the Concat . In the modified IR, the Concat input operands are Reverse 'd and then concatenated in reverse order, e.g. : Reverse(Concat(a, b, c)) => Concat(Reverse(c), Reverse(b), Reverse(a))","title":"Hoisting a reverse above a concat"},{"location":"optimizations/#hoisting-a-bitwise-operation-above-a-concat","text":"If the output of multiple Concat operations are combined with a bitwise operation, the bitwise operation is hoisted above the Concat s. In the modified IR, we have a single Concat whose operands are bitwise'd BitSlice s of the original Concat s, e.g. : Or(Concat(A, B), Concat(C, D)), where A,B,C, and D are 1-bit values => Concat(Or(Concat(A, B)[1], Concat(C, D)[1]), Or(Concat(A, B)[0], Concat(C, D)[0])) In the case that an added BitSlice exactly aligns with an original Concat operand, other optimizations (bit slice simplification, constant folding, dead code elimination) will replace the BitSlice with the operand, e.g. for the above example: => Concat(Or(A, C), Or(B, D))","title":"Hoisting a bitwise operation above a concat"},{"location":"optimizations/#merging-consecutive-bit-slices","text":"If consecutive Concat operands are consecutive BitSlice s, we create a new, merged BitSlice spanning the range of the consecutive BitSlice s. Then, we create a new Concat that includes this BitSlice as an operand, e.g. Concat(A[3:2], A[1:0], B) => Concat(A[3:0], B) This optimization is sometimes helpful and sometimes harmful, e.g. in the case that there are other consumers of the original BitSlice s, we may only end up adding more IR nodes since the original BitSlice s will not be removed by DCE once they are replaced in the Concat . Some adjustments might be able to help with this issue. An initial attempt at this limited the application of this optimization to cases where the given Concat is the only consumer of the consecutive BitSlice s. This limited the more harmful applications of this optimization, but also reduced instances in which the optimization was beneficially applied (e.g. the same consecutive BitSlice s could be merged in multiple Concat s).","title":"Merging consecutive bit-slices"},{"location":"optimizations/#select-optimizations","text":"XLS supports two types of select operations: Select (opcode Op::kSel ) is a traditional multiplexer. An n -bit binary-encoded selector chooses among 2** n inputs. OneHotSelect (opcode Op::kOneHotSel ) has one bit in the selector for each input. The output of the operation is equal to the logical-or reduction of the inputs corresponding to the set bits of the selector. Generally, a OneHotSelect is lower latency and area than a Select as a Select is effectively a decode operation followed by a OneHotSelect .","title":"Select optimizations"},{"location":"optimizations/#converting-chains-of-selects-to-into-a-single-onehotselect","text":"A linear chain of binary Select operations may be produced by the front end to select amongst a number of different values. This is equivalent to nested ternary operators in C++. A chain of Select s has high latency, but latency may be reduced by converting the Select chain into a single OneHotSelect . This may only be performed if the single-bit selectors of the Select instructions are one-hot (at most one selector is set at one time). In this case, the single-bit Select selectors are concatenated together to produce the selector for the one hot. This effectively turns a serial operation into a lower latency parallel one. If the selectors of the original Select instructions can be all zero (but still at most one selector is asserted) the transformation is slightly modified. An additional selector which is the logical NOR of all of the original Select selector bits is appended to the OneHotSelect selector and the respective case is the value selected when all selector bits are zero ( case_0 in the diagram).","title":"Converting chains of Selects to into a single OneHotSelect"},{"location":"optimizations/#binary-decision-diagram-based-optimizations","text":"A binary decision diagram (BDD) is a data structure that can represent arbitrary boolean expressions. Properties of the BDD enable easy determination of relationships between different expressions (equality, implication, etc.) which makes them useful for optimization and analysis","title":"Binary Decision Diagram based optimizations"},{"location":"optimizations/#bdd-common-subexpression-elimination","text":"Determining whether two expression are equivalent is trivial using BDDs. This can be used to identify operations in the graph which produce identical results. BDD CSE is an optimization pass which commons these equivalent operations.","title":"BDD common subexpression elimination"},{"location":"optimizations/#onehot-msb-elimination","text":"A OneHot instruction returns a bitmask with exactly one bit equal to 1. If the input is not all-0 bits, this is the first 1 bit encountered in the input going from least to most significant bit or vice-versa depending on the priority specified. If the input is all-0 bits, the most significant bit of the OneHot output is set to 1. The semantics of OneHot are described in detail here . If the MSB of a OneHot does not affect the functionality of a program, we replace the MSB with a 0-bit, e.g. OneHot(A) such that the MSB has no effect \u21d2 Concat(0, OneHot(A)[all bits except MSB])) This can open up opportunities for further optimization. To determine if a OneHot \u2019s MSB has any effect on a function, we iterate over the OneHot \u2019s post-dominators. We use the BDD to test if setting the OneHot \u2019s MSB to 0 or to 1 (other bits are 0 in both cases) implies the same value for the post-dominator node. If so, we know the value of the MSB cannot possibly affect the function output, so the MSB can safely be replaced with a 0 bit. Note: This approach assumes that IR nodes do not have any side effects. When IR nodes with side effects are introduced (i.e. channels) the analysis for this optimization will have to be adjusted slightly to account for this.","title":"OneHot MSB elimination"},{"location":"optimizations/#traditional-compiler-optimizations","text":"Many optimizations from traditional compilers targeting CPUs also apply to the optimization of hardware. Common objectives of traditional compiler optimizations include exposing parallelism, reducing latency, and eliminating instructions. Often these translate directly into the primary objectives of hardware optimization of reducing delay and area.","title":"Traditional compiler optimizations"},{"location":"optimizations/#common-subexpression-elimination-cse","text":"TODO: Finish.","title":"Common Subexpression Elimination (CSE)"},{"location":"optimizations/#constant-folding","text":"TODO: Finish.","title":"Constant Folding"},{"location":"optimizations/#dead-code-elimination-dce","text":"TODO: Finish.","title":"Dead Code Elimination (DCE)"},{"location":"optimizations/#reassociation","text":"Reassociation in XLS uses the associative and commutative property of arithmetic operations (such as adds and multiplies) to rearrange expressions of identical operations to minimize delay and area. Delay is reduced by transforming chains of operations into balanced trees which reduces the critical-path delay. For example, given the following expression: This can be reassociated into the following balanced tree: The transformation has reduced the critical path through the expression from three adds down to two adds. Reassociation can also create opportunities for constant folding. If an expression contains multiple literal values (constants) the expressions can be reassociated to gather literals into the same subepxression which can then be folded. Generally this requires the operation to be commutative as well as associative. For example, given the following expression: This can be reassociated into: The right-most add of the two literals can be folded reducing the number of adds in the expression to two.","title":"Reassociation"},{"location":"scheduling/","text":"XLS Scheduling XLS Scheduling Pipeline Scheduling Scheduling to minimize pipeline registers Rematerialization Pipeline Scheduling Pipeline scheduling divides the nodes of an XLS function into a sequence of stages constituting a feed-forward pipeline. Sequential stages are separated by registers enabling pipeline parallelism. The schedule must satify dependency constraints between XLS nodes as well as timing constraints imposed by the target clock frequency. Pipeline scheduling has multiple competing optimization objectives: minimize number of stages (minimize pipeline latency), minimize maximum delay of any stage (maximize clock frequency), and minimize the number of pipeline registers. Scheduling to minimize pipeline registers Scheduling to minimize pipeline registers can be formulated as a graph min-cut problem where the graph cut divides the nodes of the graph into separate pipeline stages and the cost of the cut is the number of bits in the pipeline register between the stages. This formulation of the pipeline scheduling problem is attractive because the graph min-cut problem can be solved in polynomial time. In general, the XLS IR graph cannot be used directly by the min-cut algorithm because of additional constraints imposed by pipeline scheduling and features of the IR graph. As a motivating example, consider the following graph of an XLS function to be scheduled into a two-stage pipeline: Node x is a parameter to the function, and node F is the return value. The width of the edges correlates with the (labeled) bit width of the respective operation. The bit width of the edges are edge weights in the min-cut algorithm. The drawing above shows the initial and final pipeline registers which flops the input output on the interface boundary. In this example, a two-stage pipeline is desired so one additional pipeline register is required. The scheduling problem is to partition the nodes A through F into the two pipeline stages while minimizing register count, or alternatively formulated, identify a cut through the graph of minimum cost. Below is one possible cut resulting in a pipeline schedule where nodes A and B are in stage one and nodes C through F are in stage two. In the pipeline generated from the example cut above pipeline registers are required for nodes A and B of bit widths 2 and 32 respectively for a total of 34 flops. However this value is inconsistent with the cost of the cut in the IR graph which is equal to the sum of weights of the cut edges: 2 + 32 + 32 = 66. To reconcile the cost function, transformations are applied to the graph about nodes with fan-out. Specifically, an artificial node N' is added for each node N with fan-out and an edge is added from each immediate successor of N to N' . The weight of each edge fanning out from N and fanning into N' is set to the original weight of the edges of N divided by the fan-out factor. In the example, the edge weight is set to 32 / 2 = 16. Below is the transformed graph. As shown, the cost of the cut (sum of edge weights) now equals the number of flops in the pipeline register (34). For scheduling the example graph, assume the target clock period be three time units and all nodes have unit delay. In this case, not all cuts will produce a schedule which satisfies the timing constraint. Specifically, if B is scheduled in the second stage, or F is scheduled in the first stage the pipeline cannot meet the timing constraint. To ensure the min-cut results in a schedule which satisfies timing an artificial source and sink node is added to the graph. Edges with infinite weight are added from the source to nodes which must be scheduled in the first stage (or earlier in the case of parameter nodes) to satisfy timing constraints. Similar edges are added from nodes which must be scheduled in the second stage to the sink node as shown below: One additional transformation (not shown) is required to ensure a correct pipeline. Generally, a partitioning created by a min-cut allows edges going in both directions between the partitions. However, pipeline stages do not allow circular dependencies. To enforce directionality to the edges of the cut, an edge of infinite weight is added parallel to and in the opposite direction of every edge in the graph. After transforming the graph, a min-cut is found by applying a max flow algorithm (Ford-Fulkerson) from the artificial source node to the artificial sink node. In the running example, the min cut is edges C -> F and E -> F of cost 12. All nodes except F are placed in the first stage of the pipeline. Generally, a pipeline can have more than two stages so a single cut is insufficient to determine a schedule. In this case a sequence of cuts is performed, one for each boundary between pipeline stages. Each min-cut partitions the nodes into two parts: the set of nodes scheduled before the respective stage boundary, and the set of node scheduled after. This imposes additional constraints on later min-cut computations. These constraints are imposed by extending infinite weight edges between thes nodes and the source or sink node in the graph. The order in which the sequence of cuts is performed (e.g., cut the boundary between stage 0 and stage 1, then between 1 and 2, then between 2 and 3, and so on) can affect the total number of pipeline flops so, in general, multiple orders are attemped and the result with the fewest pipeline flops is kept. Rematerialization TODO(meheff): Finish.","title":"Overview"},{"location":"scheduling/#xls-scheduling","text":"XLS Scheduling Pipeline Scheduling Scheduling to minimize pipeline registers Rematerialization","title":"XLS Scheduling"},{"location":"scheduling/#pipeline-scheduling","text":"Pipeline scheduling divides the nodes of an XLS function into a sequence of stages constituting a feed-forward pipeline. Sequential stages are separated by registers enabling pipeline parallelism. The schedule must satify dependency constraints between XLS nodes as well as timing constraints imposed by the target clock frequency. Pipeline scheduling has multiple competing optimization objectives: minimize number of stages (minimize pipeline latency), minimize maximum delay of any stage (maximize clock frequency), and minimize the number of pipeline registers.","title":"Pipeline Scheduling"},{"location":"scheduling/#scheduling-to-minimize-pipeline-registers","text":"Scheduling to minimize pipeline registers can be formulated as a graph min-cut problem where the graph cut divides the nodes of the graph into separate pipeline stages and the cost of the cut is the number of bits in the pipeline register between the stages. This formulation of the pipeline scheduling problem is attractive because the graph min-cut problem can be solved in polynomial time. In general, the XLS IR graph cannot be used directly by the min-cut algorithm because of additional constraints imposed by pipeline scheduling and features of the IR graph. As a motivating example, consider the following graph of an XLS function to be scheduled into a two-stage pipeline: Node x is a parameter to the function, and node F is the return value. The width of the edges correlates with the (labeled) bit width of the respective operation. The bit width of the edges are edge weights in the min-cut algorithm. The drawing above shows the initial and final pipeline registers which flops the input output on the interface boundary. In this example, a two-stage pipeline is desired so one additional pipeline register is required. The scheduling problem is to partition the nodes A through F into the two pipeline stages while minimizing register count, or alternatively formulated, identify a cut through the graph of minimum cost. Below is one possible cut resulting in a pipeline schedule where nodes A and B are in stage one and nodes C through F are in stage two. In the pipeline generated from the example cut above pipeline registers are required for nodes A and B of bit widths 2 and 32 respectively for a total of 34 flops. However this value is inconsistent with the cost of the cut in the IR graph which is equal to the sum of weights of the cut edges: 2 + 32 + 32 = 66. To reconcile the cost function, transformations are applied to the graph about nodes with fan-out. Specifically, an artificial node N' is added for each node N with fan-out and an edge is added from each immediate successor of N to N' . The weight of each edge fanning out from N and fanning into N' is set to the original weight of the edges of N divided by the fan-out factor. In the example, the edge weight is set to 32 / 2 = 16. Below is the transformed graph. As shown, the cost of the cut (sum of edge weights) now equals the number of flops in the pipeline register (34). For scheduling the example graph, assume the target clock period be three time units and all nodes have unit delay. In this case, not all cuts will produce a schedule which satisfies the timing constraint. Specifically, if B is scheduled in the second stage, or F is scheduled in the first stage the pipeline cannot meet the timing constraint. To ensure the min-cut results in a schedule which satisfies timing an artificial source and sink node is added to the graph. Edges with infinite weight are added from the source to nodes which must be scheduled in the first stage (or earlier in the case of parameter nodes) to satisfy timing constraints. Similar edges are added from nodes which must be scheduled in the second stage to the sink node as shown below: One additional transformation (not shown) is required to ensure a correct pipeline. Generally, a partitioning created by a min-cut allows edges going in both directions between the partitions. However, pipeline stages do not allow circular dependencies. To enforce directionality to the edges of the cut, an edge of infinite weight is added parallel to and in the opposite direction of every edge in the graph. After transforming the graph, a min-cut is found by applying a max flow algorithm (Ford-Fulkerson) from the artificial source node to the artificial sink node. In the running example, the min cut is edges C -> F and E -> F of cost 12. All nodes except F are placed in the first stage of the pipeline. Generally, a pipeline can have more than two stages so a single cut is insufficient to determine a schedule. In this case a sequence of cuts is performed, one for each boundary between pipeline stages. Each min-cut partitions the nodes into two parts: the set of nodes scheduled before the respective stage boundary, and the set of node scheduled after. This imposes additional constraints on later min-cut computations. These constraints are imposed by extending infinite weight edges between thes nodes and the source or sink node in the graph. The order in which the sequence of cuts is performed (e.g., cut the boundary between stage 0 and stage 1, then between 1 and 2, then between 2 and 3, and so on) can affect the total number of pipeline flops so, in general, multiple orders are attemped and the result with the fewest pipeline flops is kept.","title":"Scheduling to minimize pipeline registers"},{"location":"scheduling/#rematerialization","text":"TODO(meheff): Finish.","title":"Rematerialization"},{"location":"solvers/","text":"XLS Solvers Programs that are represented as optimized XLS IR are converted into circuits based on boolean logic, and so it is also possible to feed those as logical operations to a theorem prover. We have implemented that conversion with the Z3 theorem prover using its \"bit vector\" type support. As a result, you can conceptually ask Z3 to prove any predicate that can be expressed as XLS, over all possible parameter inputs. See the tools documentation for usage information on related command line tools. Applications This facility is expected to be useful to augment random testing. While profiling the values in an XLS IR function that is given random stimulus, we may observe bits that result from nodes that appear to be constant (but are not created via a \"literal\" or a \"concat\" of a literal). Example: Say the value resulting from and.1234 in the graph appears to be constant zero with all the stimulus provided via a fuzzer thus far -- the solver provides a facility whereby we can ask \"is there a counterexample to and.1234 always being zero?\" and the solver will either say \"no, it is always zero\", or it will yield a counterexample, or will not terminate within the allocated deadline. Assuming we can prove useful properties in a reasonable amount of time, we can use this proof capability to help find interesting example inputs that provide unique stimulus. Correctness WRT reference: 32-bit Floating-Point Adder The full input space for a 32-bit adder is a whopping 64 bits - far more than is possible to exhaustively test for correctness. Proving correctness via Z3, however, is relatively straightforward: at a high level, one simply compares the output from the DSLX (translated into Z3) to the same operation performed solely in Z3. In detail, the steps are: Translate the DSLX implementation into Z3 via Z3Translator::CreateAndTranslate() . Create a Z3 implementation of the same addition. This is nearly trivial, as Z3 helpfully has built-in support for floating-point values and theories. Take the result nodes from each \"branch\" above and create a new node subtracting the two. This is the absolute error. Note: Usually, one is interested in relative error when working with FP values, but here, our target is absolute equivalence, so absolute error sufficies (and is simpler). Create a Z3 node comparing that error to the maximum bound (here 0.0f). Feed that error node into a Z3 solver, asking it to prove that the error could be greater than that bound. If the solver can not satisfy that criterion, then that means the error is never greater than that bound, i.e., that the implementations are equivalent (with our 0.0f bound). IR Transform validity It's usually not possible (or is merely extremely difficult) to write tests to prove that an optimization/transform is safe across all input IR. By comparing the optimized vs. unoptimized IR in a similar manner as the correctness proof above, we can symbolically prove safety. The only difference between this and the correctness proof is that both the optimized and unoptimized IR need to be fed into the same Z3Translator (the second via Z3Translator::AddFunction() ) and the result nodes each are used in the error comparison. IR to netlist Logical Equivalence Checking (LEC) After a user design has been lowered to IR, it is optimized (see the previous section), then Verilog is generated for that optimized IR. That Verilog is then compiled by an external tool, which, if successful, will output a \"netlist\" - a set of standard cells (think AND, OR, NOT, flops, etc.) and wires connecting them that realizes the design. Between the IR level and that netlist, many, many transformations are applied to the design. Before processing the netlist further - and certainly before sending the final design to fabrication - it's a very good idea to ensure that the netlist describes the correct logic! Demonstrating initial design correctness is up to the user, via unit tests or integration tests at the DSLX level. At all stages below that, though, ensuring logical equivalence between forms is XLS' responsibility. To prove equivalence between the IR and netlist forms of a design, XLS uses formal verification via solvers - currently only Z3, above. Performing IR-to-netlist LEC is very similar to the checking above - the source IR is one half of the comparison. Here, the second half is the netlist translated into IR, which only requires a small amount of extra work. Consider the snippet below: FOO p1_and_1 ( .A(p0_i0), .B(p0_i1), .Z(p1_and_1_comb) ); BAR p1_and_2 ( .A(p0_i2), .B(p0_i3), .Z(p1_and_2_comb) ); These lines describe, in order: One cell, called AND , that takes two inputs, .A and .B, provided by the wires p0_i0 and p0_i1 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_1_comb. One cell, called OR , that takes two inputs, .A and .B, provided by the wires p0_i2 and p0_i2 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_2_comb. Note that the values computed by the cells wasn't mentioned - that's because FOO and BAR are defined in the \"cell library\", the list of standard cells used to generate the netlist. Thus, to be able to model these gates in a solver, we need to take that cell library as input to the LEC tool. The netlist describes how cells are laid out, and the cell library indicates what cells actually do . With both of these in hand, preparing the netlist half of a LEC is a [relatively] straightforward matter of parsing a netlist and cell library and converting those together into a description of logic. See z3_netlist_translator.cc for full details. Utilities tools/lec_main.cc : Driver function for performing IR-to-netlist LEC. solvers/python/z3_lec.cc : Wrapper to perform IR-to-netlist LEC from Python. Current Limitations Time-to-result Under the hood, Z3 (and many other tools in this space) is an (SMT solver)[https://en.wikipedia.org/wiki/Satisfiability_modulo_theories]. At a high level, think of an SMT solver as a SAT solver that has special handling for certain classes of data (bit vectors, floating-point numbers). Many sufficiently complicated problems will reduce to raw SAT solving (especially those involving netlists, which have to implement complex logic at the gate level. Consider what that means for a multiply, for example!). Since SAT scales exponentially with the size of its inputs, execution time can quickly grow past a point of utility for complex operations, notably multiplication. Fortunately, for most designs (without such complex ops), proving equivalence of a single pipeline stage can complete in a small amount of time (O(minutes)). Predicate coverage Hypothetically, any XLS function that computes a predicate (bool) can be fed to Z3 for satisfiability testing. Currently a more limited set of predicates are exposed that can be easily expressed on the command line; however, it should be possible to provide: an XLS IR file a set of nodes in the entry function a DSLX function that computes a predicate on those nodes Which would allow the user to compute arbitrary properties of nodes in the function with the concise DSL syntax. Subroutines Z3 doesn't intrinsically have support for subroutines, or as they're called in Z3, \"macros\", instead requiring that all function calls be inlined . There is an extension that adds support for recursive function decls and defs, but in our experience, it doesn't behave the way we'd expect. Consider the following example: package p fn mapper(value: bits[32]) -> bits[32] { ret value } fn main() -> bits[1] { literal_0: bits[32] = literal(value=0) literal_1: bits[1] = literal(value=1) elem_0: bits[32] = invoke(literal_0, to_apply=mapper) eq_12: bits[1] = eq(literal_0, elem_0) ret and_13: bits[1] = and(eq_12, literal_1) } Here, it's trivial for a human reader to see that the results are the same; the output should be equal to 1. Z3, however, reports that this is not necessarily the case, suggesting that literal_0 and elem_0 would not be equal in the case where the input to mapper was 1...which is clearly never the case here. To address this, we require that all subroutines (including those used in maps and counted fors) be inlined before consumption by Z3.","title":"Formal"},{"location":"solvers/#xls-solvers","text":"Programs that are represented as optimized XLS IR are converted into circuits based on boolean logic, and so it is also possible to feed those as logical operations to a theorem prover. We have implemented that conversion with the Z3 theorem prover using its \"bit vector\" type support. As a result, you can conceptually ask Z3 to prove any predicate that can be expressed as XLS, over all possible parameter inputs. See the tools documentation for usage information on related command line tools.","title":"XLS Solvers"},{"location":"solvers/#applications","text":"This facility is expected to be useful to augment random testing. While profiling the values in an XLS IR function that is given random stimulus, we may observe bits that result from nodes that appear to be constant (but are not created via a \"literal\" or a \"concat\" of a literal). Example: Say the value resulting from and.1234 in the graph appears to be constant zero with all the stimulus provided via a fuzzer thus far -- the solver provides a facility whereby we can ask \"is there a counterexample to and.1234 always being zero?\" and the solver will either say \"no, it is always zero\", or it will yield a counterexample, or will not terminate within the allocated deadline. Assuming we can prove useful properties in a reasonable amount of time, we can use this proof capability to help find interesting example inputs that provide unique stimulus.","title":"Applications"},{"location":"solvers/#correctness-wrt-reference-32-bit-floating-point-adder","text":"The full input space for a 32-bit adder is a whopping 64 bits - far more than is possible to exhaustively test for correctness. Proving correctness via Z3, however, is relatively straightforward: at a high level, one simply compares the output from the DSLX (translated into Z3) to the same operation performed solely in Z3. In detail, the steps are: Translate the DSLX implementation into Z3 via Z3Translator::CreateAndTranslate() . Create a Z3 implementation of the same addition. This is nearly trivial, as Z3 helpfully has built-in support for floating-point values and theories. Take the result nodes from each \"branch\" above and create a new node subtracting the two. This is the absolute error. Note: Usually, one is interested in relative error when working with FP values, but here, our target is absolute equivalence, so absolute error sufficies (and is simpler). Create a Z3 node comparing that error to the maximum bound (here 0.0f). Feed that error node into a Z3 solver, asking it to prove that the error could be greater than that bound. If the solver can not satisfy that criterion, then that means the error is never greater than that bound, i.e., that the implementations are equivalent (with our 0.0f bound).","title":"Correctness WRT reference: 32-bit Floating-Point Adder"},{"location":"solvers/#ir-transform-validity","text":"It's usually not possible (or is merely extremely difficult) to write tests to prove that an optimization/transform is safe across all input IR. By comparing the optimized vs. unoptimized IR in a similar manner as the correctness proof above, we can symbolically prove safety. The only difference between this and the correctness proof is that both the optimized and unoptimized IR need to be fed into the same Z3Translator (the second via Z3Translator::AddFunction() ) and the result nodes each are used in the error comparison.","title":"IR Transform validity"},{"location":"solvers/#ir-to-netlist-logical-equivalence-checking-lec","text":"After a user design has been lowered to IR, it is optimized (see the previous section), then Verilog is generated for that optimized IR. That Verilog is then compiled by an external tool, which, if successful, will output a \"netlist\" - a set of standard cells (think AND, OR, NOT, flops, etc.) and wires connecting them that realizes the design. Between the IR level and that netlist, many, many transformations are applied to the design. Before processing the netlist further - and certainly before sending the final design to fabrication - it's a very good idea to ensure that the netlist describes the correct logic! Demonstrating initial design correctness is up to the user, via unit tests or integration tests at the DSLX level. At all stages below that, though, ensuring logical equivalence between forms is XLS' responsibility. To prove equivalence between the IR and netlist forms of a design, XLS uses formal verification via solvers - currently only Z3, above. Performing IR-to-netlist LEC is very similar to the checking above - the source IR is one half of the comparison. Here, the second half is the netlist translated into IR, which only requires a small amount of extra work. Consider the snippet below: FOO p1_and_1 ( .A(p0_i0), .B(p0_i1), .Z(p1_and_1_comb) ); BAR p1_and_2 ( .A(p0_i2), .B(p0_i3), .Z(p1_and_2_comb) ); These lines describe, in order: One cell, called AND , that takes two inputs, .A and .B, provided by the wires p0_i0 and p0_i1 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_1_comb. One cell, called OR , that takes two inputs, .A and .B, provided by the wires p0_i2 and p0_i2 , respectively, and one output, .Z, which will be assigned to the wire `p1_and_2_comb. Note that the values computed by the cells wasn't mentioned - that's because FOO and BAR are defined in the \"cell library\", the list of standard cells used to generate the netlist. Thus, to be able to model these gates in a solver, we need to take that cell library as input to the LEC tool. The netlist describes how cells are laid out, and the cell library indicates what cells actually do . With both of these in hand, preparing the netlist half of a LEC is a [relatively] straightforward matter of parsing a netlist and cell library and converting those together into a description of logic. See z3_netlist_translator.cc for full details.","title":"IR to netlist Logical Equivalence Checking (LEC)"},{"location":"solvers/#utilities","text":"tools/lec_main.cc : Driver function for performing IR-to-netlist LEC. solvers/python/z3_lec.cc : Wrapper to perform IR-to-netlist LEC from Python.","title":"Utilities"},{"location":"solvers/#current-limitations","text":"","title":"Current Limitations"},{"location":"solvers/#time-to-result","text":"Under the hood, Z3 (and many other tools in this space) is an (SMT solver)[https://en.wikipedia.org/wiki/Satisfiability_modulo_theories]. At a high level, think of an SMT solver as a SAT solver that has special handling for certain classes of data (bit vectors, floating-point numbers). Many sufficiently complicated problems will reduce to raw SAT solving (especially those involving netlists, which have to implement complex logic at the gate level. Consider what that means for a multiply, for example!). Since SAT scales exponentially with the size of its inputs, execution time can quickly grow past a point of utility for complex operations, notably multiplication. Fortunately, for most designs (without such complex ops), proving equivalence of a single pipeline stage can complete in a small amount of time (O(minutes)).","title":"Time-to-result"},{"location":"solvers/#predicate-coverage","text":"Hypothetically, any XLS function that computes a predicate (bool) can be fed to Z3 for satisfiability testing. Currently a more limited set of predicates are exposed that can be easily expressed on the command line; however, it should be possible to provide: an XLS IR file a set of nodes in the entry function a DSLX function that computes a predicate on those nodes Which would allow the user to compute arbitrary properties of nodes in the function with the concise DSL syntax.","title":"Predicate coverage"},{"location":"solvers/#subroutines","text":"Z3 doesn't intrinsically have support for subroutines, or as they're called in Z3, \"macros\", instead requiring that all function calls be inlined . There is an extension that adds support for recursive function decls and defs, but in our experience, it doesn't behave the way we'd expect. Consider the following example: package p fn mapper(value: bits[32]) -> bits[32] { ret value } fn main() -> bits[1] { literal_0: bits[32] = literal(value=0) literal_1: bits[1] = literal(value=1) elem_0: bits[32] = invoke(literal_0, to_apply=mapper) eq_12: bits[1] = eq(literal_0, elem_0) ret and_13: bits[1] = and(eq_12, literal_1) } Here, it's trivial for a human reader to see that the results are the same; the output should be equal to 1. Z3, however, reports that this is not necessarily the case, suggesting that literal_0 and elem_0 would not be equal in the case where the input to mapper was 1...which is clearly never the case here. To address this, we require that all subroutines (including those used in maps and counted fors) be inlined before consumption by Z3.","title":"Subroutines"},{"location":"tools/","text":"XLS Tools An index of XLS developer tools. bdd_stats Constructs a binary decision diagram (BDD) using a given XLS function and prints various statistics about the BDD. BDD construction can be very slow in pathological cases and this utility is useful for identifying the underlying causes. Accepts arbitrary IR as input or a benchmark specified by name. benchmark_main Prints numerous metrics and other information about an XLS IR file including: total delay, critical path, codegen information, optimization time, etc. This tool may be run against arbitrary IR not just the fixed set of XLS benchmarks. The output of this tool is scraped by run_benchmarks to construct a table comparing metrics against a mint CL across the benchmark suite. codegen_main Lowers an XLS IR file into Verilog. Options include emitting a feedforward pipeline or a purely combinational block. Emits both a Verilog file and a module signature which includes metadata about the block. The tool does not run any XLS passes so unoptimized IR may fail if the IR contains constructs not expected by the backend. eval_ir_main Evaluates an XLS IR file with user-specified or random inputs. Includes features for evaluating the IR before and after optimizations which makes this tool very useful for identifying optimization bugs. ir_minimizer_main Tool for reducing IR to a minimal test case based on an external test. ## check_ir_equivalence Verifies that two IR files (for example, optimized and unoptimized IR from the same source) are logically equivalent. opt_main Runs XLS IR through the optimization pipeline. simulate_module_main Runs an Verilog block emitted by XLS through a Verilog simulator. Requires both the Verilog text and the module signature which includes metadata about the block. solver Uses a SMT solver (i.e. Z3) to prove properties of an XLS IR program from the command line. Currently the set of \"predicates\" that the solver supports from the command line are limited, but in theory it is capable of solving for arbitrary IR-function-specified predicates. This can be used to uncover opportunities for optimization that were missed, or to prove equivalence of transformed representations with their original version. cell_library_extract_formula Parses a cell library \".lib\" file and extracts boolean formulas from it that determine the functionality of cells. This is useful for LEC of the XLS IR against the post-sythesis netlist.","title":"Listing"},{"location":"tools/#xls-tools","text":"An index of XLS developer tools.","title":"XLS Tools"},{"location":"tools/#bdd_stats","text":"Constructs a binary decision diagram (BDD) using a given XLS function and prints various statistics about the BDD. BDD construction can be very slow in pathological cases and this utility is useful for identifying the underlying causes. Accepts arbitrary IR as input or a benchmark specified by name.","title":"bdd_stats"},{"location":"tools/#benchmark_main","text":"Prints numerous metrics and other information about an XLS IR file including: total delay, critical path, codegen information, optimization time, etc. This tool may be run against arbitrary IR not just the fixed set of XLS benchmarks. The output of this tool is scraped by run_benchmarks to construct a table comparing metrics against a mint CL across the benchmark suite.","title":"benchmark_main"},{"location":"tools/#codegen_main","text":"Lowers an XLS IR file into Verilog. Options include emitting a feedforward pipeline or a purely combinational block. Emits both a Verilog file and a module signature which includes metadata about the block. The tool does not run any XLS passes so unoptimized IR may fail if the IR contains constructs not expected by the backend.","title":"codegen_main"},{"location":"tools/#eval_ir_main","text":"Evaluates an XLS IR file with user-specified or random inputs. Includes features for evaluating the IR before and after optimizations which makes this tool very useful for identifying optimization bugs.","title":"eval_ir_main"},{"location":"tools/#ir_minimizer_main","text":"Tool for reducing IR to a minimal test case based on an external test. ## check_ir_equivalence Verifies that two IR files (for example, optimized and unoptimized IR from the same source) are logically equivalent.","title":"ir_minimizer_main"},{"location":"tools/#opt_main","text":"Runs XLS IR through the optimization pipeline.","title":"opt_main"},{"location":"tools/#simulate_module_main","text":"Runs an Verilog block emitted by XLS through a Verilog simulator. Requires both the Verilog text and the module signature which includes metadata about the block.","title":"simulate_module_main"},{"location":"tools/#solver","text":"Uses a SMT solver (i.e. Z3) to prove properties of an XLS IR program from the command line. Currently the set of \"predicates\" that the solver supports from the command line are limited, but in theory it is capable of solving for arbitrary IR-function-specified predicates. This can be used to uncover opportunities for optimization that were missed, or to prove equivalence of transformed representations with their original version.","title":"solver"},{"location":"tools/#cell_library_extract_formula","text":"Parses a cell library \".lib\" file and extracts boolean formulas from it that determine the functionality of cells. This is useful for LEC of the XLS IR against the post-sythesis netlist.","title":"cell_library_extract_formula"},{"location":"tools_quick_start/","text":"XLS Tools Quick Start This document is a quick start guide through the use of the individual XLS tools, from DSL input to RTL generation. Note: This guide assumes you have set up your system so it can build the XLS tools via Bazel . There is currently no binary tools distribution so building from source is required. Create a file /tmp/simple_add.x with the following contents: fn add(x: u32, y: u32) -> u32 { x + y + u32:0 // Something to optimize. } test add { assert_eq(add(u32:2, u32:3), u32:5) } This contains a function, and a unit test of that function. Interpreting the DSL file Now, run it through the DSL interpreter -- the DSL interpreter is useful for interactive development and debugging. $ bazel run -c opt //xls/dslx/interpreter:interpreter_main -- /tmp/simple_add.x [ RUN ] add [ OK ] add The DSL interpreter is the execution engine running the test shown. In lieu of using bazel run for the subsequent commands, this document will assume bazel build -c opt //xls/... has been completed so the binaries in ./bazel-bin can be used directly: $ ./bazel-bin/xls/dslx/interpreter/interpreter_main /tmp/simple_add.x [ RUN ] add [ OK ] add DSL to IR conversion To convert the DSL file to IR, run the following command: $ ./bazel-bin/xls/dslx/ir_converter_main /tmp/simple_add.x > /tmp/simple_add.ir IR optimization To optimize the IR, use the opt_main tool: $ ./bazel-bin/xls/tools/opt_main /tmp/simple_add.ir > /tmp/simple_add.opt.ir Check the output of diff -U8 /tmp/simple_add*.ir to see that the optimizer eliminated the useless add-with-zero. Verilog RTL generation To generate RTL from the optimized IR, use the codegen tool: $ ./bazel-bin/xls/tools/codegen_main --pipeline_stages=1 /tmp/simple_add.opt.ir > /tmp/simple_add.v IR visualizer To get a graphical view of the IR files, use the IR visualization tool: $ ./bazel-bin/xls/ir/visualization/app --delay_model=unit --ir_path=/tmp/simple_add.ir This starts a server on the local host port 5000 by default, so you can access it from your machine as http://localhost:5000 in a web browser.","title":"Quick Start"},{"location":"tools_quick_start/#xls-tools-quick-start","text":"This document is a quick start guide through the use of the individual XLS tools, from DSL input to RTL generation. Note: This guide assumes you have set up your system so it can build the XLS tools via Bazel . There is currently no binary tools distribution so building from source is required. Create a file /tmp/simple_add.x with the following contents: fn add(x: u32, y: u32) -> u32 { x + y + u32:0 // Something to optimize. } test add { assert_eq(add(u32:2, u32:3), u32:5) } This contains a function, and a unit test of that function.","title":"XLS Tools Quick Start"},{"location":"tools_quick_start/#interpreting-the-dsl-file","text":"Now, run it through the DSL interpreter -- the DSL interpreter is useful for interactive development and debugging. $ bazel run -c opt //xls/dslx/interpreter:interpreter_main -- /tmp/simple_add.x [ RUN ] add [ OK ] add The DSL interpreter is the execution engine running the test shown. In lieu of using bazel run for the subsequent commands, this document will assume bazel build -c opt //xls/... has been completed so the binaries in ./bazel-bin can be used directly: $ ./bazel-bin/xls/dslx/interpreter/interpreter_main /tmp/simple_add.x [ RUN ] add [ OK ] add","title":"Interpreting the DSL file"},{"location":"tools_quick_start/#dsl-to-ir-conversion","text":"To convert the DSL file to IR, run the following command: $ ./bazel-bin/xls/dslx/ir_converter_main /tmp/simple_add.x > /tmp/simple_add.ir","title":"DSL to IR conversion"},{"location":"tools_quick_start/#ir-optimization","text":"To optimize the IR, use the opt_main tool: $ ./bazel-bin/xls/tools/opt_main /tmp/simple_add.ir > /tmp/simple_add.opt.ir Check the output of diff -U8 /tmp/simple_add*.ir to see that the optimizer eliminated the useless add-with-zero.","title":"IR optimization"},{"location":"tools_quick_start/#verilog-rtl-generation","text":"To generate RTL from the optimized IR, use the codegen tool: $ ./bazel-bin/xls/tools/codegen_main --pipeline_stages=1 /tmp/simple_add.opt.ir > /tmp/simple_add.v","title":"Verilog RTL generation"},{"location":"tools_quick_start/#ir-visualizer","text":"To get a graphical view of the IR files, use the IR visualization tool: $ ./bazel-bin/xls/ir/visualization/app --delay_model=unit --ir_path=/tmp/simple_add.ir This starts a server on the local host port 5000 by default, so you can access it from your machine as http://localhost:5000 in a web browser.","title":"IR visualizer"},{"location":"xls_style/","text":"XLS Style Guide The Google style guides recommend enforcing local consistency where stylistic choices are not pre-defined. This file notes some of the choices we make locally in the XLS project, with the relevant Google style guides ( C++ , Python ) as their bases. C++ Align the pointer or reference modifier token with the type; e.g. Foo& foo = ... instead of Foo &foo = ... , and Foo* foo = ... instead of Foo *foo= ... . Use /*parameter_name=*/value style comments if you choose to annotate arguments in a function invocation. clang-tidy recognizes this form, and provides a Tricorder notification if parameter_name is mismatched against the parameter name of the callee. Prefer int64 over int to avoid any possibility of overflow. Always use Status or StatusOr for any error that a user could encounter. Other than user-facing errors, use Status only in exceptional situations. For example, Status is good to signal that a required file does not exist but not for signaling that constant folding did not constant fold an expression. Internal errors for conditions that should never be false can use CHECK , but may also use Status or StatusOr . Prefer CHECK to DCHECK , except that DCHECK can be used to verify conditions that it would be too expensive to verify in production, but that are fast enough to include outside of production. Functions Short or easily-explained argument lists (as defined by the developer) can be explained inline with the rest of the function comment. For more complex argument lists, the following pattern should be used: // <Function description> // Args: // arg1: <arg1 description> // arg2: <arg2 description> // ...","title":"Style Guide"},{"location":"xls_style/#xls-style-guide","text":"The Google style guides recommend enforcing local consistency where stylistic choices are not pre-defined. This file notes some of the choices we make locally in the XLS project, with the relevant Google style guides ( C++ , Python ) as their bases.","title":"XLS Style Guide"},{"location":"xls_style/#c","text":"Align the pointer or reference modifier token with the type; e.g. Foo& foo = ... instead of Foo &foo = ... , and Foo* foo = ... instead of Foo *foo= ... . Use /*parameter_name=*/value style comments if you choose to annotate arguments in a function invocation. clang-tidy recognizes this form, and provides a Tricorder notification if parameter_name is mismatched against the parameter name of the callee. Prefer int64 over int to avoid any possibility of overflow. Always use Status or StatusOr for any error that a user could encounter. Other than user-facing errors, use Status only in exceptional situations. For example, Status is good to signal that a required file does not exist but not for signaling that constant folding did not constant fold an expression. Internal errors for conditions that should never be false can use CHECK , but may also use Status or StatusOr . Prefer CHECK to DCHECK , except that DCHECK can be used to verify conditions that it would be too expensive to verify in production, but that are fast enough to include outside of production.","title":"C++"},{"location":"xls_style/#functions","text":"Short or easily-explained argument lists (as defined by the developer) can be explained inline with the rest of the function comment. For more complex argument lists, the following pattern should be used: // <Function description> // Args: // arg1: <arg1 description> // arg2: <arg2 description> // ...","title":"Functions"}]}